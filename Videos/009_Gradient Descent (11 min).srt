
1
00:00:00,000 --> 00:00:04,093
We've previously defined the cost function
J. In this video I want to tell you about

2
00:00:04,093 --> 00:00:09,063
an algorithm called gradient descent for
minimizing the cost function J. It turns

3
00:00:09,063 --> 00:00:14,027
out gradient descent is a more general
algorithm and is used not only in linear

4
00:00:14,027 --> 00:00:18,091
regression. It's actually used all over
the place in machine learning. And later

5
00:00:18,091 --> 00:00:23,079
in the class we'll use gradient descent to
minimize other functions as well, not just

6
00:00:23,079 --> 00:00:27,084
the cost function J, for linear regression.
So in this video, I'm going to

7
00:00:27,084 --> 00:00:32,055
talk about gradient descent for minimizing
some arbitrary function J. And then in

8
00:00:32,055 --> 00:00:37,040
later videos, we'll take those algorithm
and apply it specifically to the cost

9
00:00:37,040 --> 00:00:43,033
function J that we had to find for linear
regression. So here's the problem

10
00:00:43,033 --> 00:00:48,011
setup. We're going to see that we have
some function J of (theta0, theta1).

11
00:00:48,011 --> 00:00:52,077
Maybe it's a cost function from linear
regression. Maybe it's some other function

12
00:00:52,077 --> 00:00:56,080
we want to minimize. And we want
to come up with an algorithm for

13
00:00:56,080 --> 00:01:01,017
minimizing that as a function of J of
(theta0, theta1). Just as an aside,

14
00:01:01,017 --> 00:01:05,079
it turns out that gradient descent
actually applies to more general

15
00:01:05,079 --> 00:01:10,099
functions. So imagine if you have
a function that's a function of

16
00:01:10,099 --> 00:01:16,019
J of (theta0, theta1, theta2, up to
some theta n). And you want to

17
00:01:16,040 --> 00:01:21,079
minimize over (theta0 up to theta n)
of this J of (theta0 up to theta n).

18
00:01:21,079 --> 00:01:26,057
It turns out gradient descent
is an algorithm for solving

19
00:01:26,057 --> 00:01:31,036
this more general problem, but for the
sake of brevity, for the sake of

20
00:01:31,036 --> 00:01:35,093
your succinctness of notation, I'm just
going to present only two parameters

21
00:01:36,011 --> 00:01:41,009
throughout the rest of this video. Here's
the idea for gradient descent. What we're

22
00:01:41,009 --> 00:01:45,088
going to do is we're going to start off
with some initial guesses for theta0 and

23
00:01:45,088 --> 00:01:50,078
theta1. Doesn't really matter what they
are, but a common choice would be if we

24
00:01:50,078 --> 00:01:55,045
set theta0 to 0, and
set theta1 to 0. Just initialize

25
00:01:55,045 --> 00:02:00,032
them to 0. What we're going to do in
gradient descent is we'll keep changing

26
00:02:00,032 --> 00:02:05,025
theta0 and theta1 a little bit to
try to reduce J of (theta0, theta1)

27
00:02:05,025 --> 00:02:10,057
until hopefully we wind up at a minimum or
maybe a local minimum. So, let's see

28
00:02:10,079 --> 00:02:16,010
see pictures of what gradient descent
does. Let's say I try to minimize this

29
00:02:16,010 --> 00:02:20,084
function. So notice the axes. This is,
(theta0, theta1) on the horizontal

30
00:02:20,084 --> 00:02:25,077
axes, and J is a vertical axis. And so the
height of the surface shows J, and we

31
00:02:25,077 --> 00:02:30,058
want to minimize this function. So, we're
going to start off with (theta0, theta1)

32
00:02:30,058 --> 00:02:35,037
at some point. So imagine picking some
value for (theta0, theta1), and that

33
00:02:35,037 --> 00:02:39,093
corresponds to starting at some point on
the surface of this function. Okay? So

34
00:02:39,093 --> 00:02:44,020
whatever value of (theta0, theta1)
gives you some point here. I did

35
00:02:44,020 --> 00:02:48,081
initialize them to 0, but
sometimes you initialize it to other

36
00:02:48,081 --> 00:02:53,094
values as well. Now. I want us to imagine
that this figure shows a hill. Imagine

37
00:02:53,094 --> 00:02:59,017
this is like a landscape of some grassy
park with two hills like so.

38
00:02:59,017 --> 00:03:04,061
And I want you to imagine that you are
physically standing at that point on the

39
00:03:04,061 --> 00:03:09,099
hill on this little red hill in your park.
In gradient descent, what we're

40
00:03:09,099 --> 00:03:15,077
going to do is spin 360 degrees around and
just look all around us and ask, "If I were

41
00:03:15,077 --> 00:03:20,042
to take a little baby step in some
direction, and I want to go downhill as

42
00:03:20,042 --> 00:03:25,031
quickly as possible, what direction do I
take that little baby step in if I want to

43
00:03:25,031 --> 00:03:29,068
go down, if I sort of want to physically
walk down this hill as rapidly as

44
00:03:29,068 --> 00:03:34,046
possible?" Turns out that if we're standing
at that point on the hill, you look all

45
00:03:34,046 --> 00:03:39,018
around, you find that the best direction
to take a little step downhill

46
00:03:39,018 --> 00:03:44,003
is roughly that direction. Okay. And now
you're at this new point on your hill.

47
00:03:44,003 --> 00:03:49,043
You're going to, again, look all around, and then
say, "What direction should I step in order

48
00:03:49,043 --> 00:03:54,069
to take a little baby step downhill?" And
if you do that and take another step, you

49
00:03:54,069 --> 00:03:59,069
take a step in that direction, and then
you keep going. You know, from this new

50
00:03:59,069 --> 00:04:04,083
point, you look around, decide what
direction will take you downhill most

51
00:04:04,083 --> 00:04:09,077
quickly, take another step, another step,
and so on, until you converge to this,

52
00:04:09,096 --> 00:04:15,005
local minimum down here. Further descent
has an interesting property. This first

53
00:04:15,005 --> 00:04:19,068
time we ran gradient descent, we were
starting at this point over here, right?

54
00:04:19,068 --> 00:04:24,018
Started at that point over here. Now
imagine, we initialize gradient

55
00:04:24,018 --> 00:04:29,023
descent just a couple steps to the right.
Imagine we initialized gradient descent with

56
00:04:29,023 --> 00:04:34,015
that point on the upper right. If you were
to repeat this process, and stop at the

57
00:04:34,015 --> 00:04:39,020
point, and look all around. Take a little
step in the direction of steepest descent.

58
00:04:39,020 --> 00:04:44,077
You would do that. Then look around, take
another step, and so on. And if you start

59
00:04:44,077 --> 00:04:50,056
it just a couple steps to the right, the
gradient descent will have taken you to

60
00:04:50,056 --> 00:04:56,023
this second local optimum over on the
right. So if you had started at this first

61
00:04:56,023 --> 00:05:01,060
point, you would have wound up at this
local optimum. But if you started just a

62
00:05:01,060 --> 00:05:06,076
little bit, a slightly different location,
you would have wound up at a very

63
00:05:06,076 --> 00:05:12,019
different local optimum. And this is a
property of gradient descent that we'll

64
00:05:12,019 --> 00:05:17,042
say a little bit more about later. So,
that's the intuition in pictures. Let's

65
00:05:17,042 --> 00:05:22,092
look at the map. This is the definition of
the gradient descent algorithm. We're

66
00:05:22,092 --> 00:05:28,024
going to just repeatedly do this. On to
convergence. We're going to update my

67
00:05:28,024 --> 00:05:33,054
parameter theta j by, you know, taking
theta j and subtracting from it alpha

68
00:05:33,054 --> 00:05:39,012
times this term over here. So, let's
see. There are a lot of details in this

69
00:05:39,012 --> 00:05:45,002
equation, so let me unpack some of it.
First, this notation here, colon equals.

70
00:05:45,002 --> 00:05:51,064
We're going to use := to denote
assignment, so it's the assignment

71
00:05:51,064 --> 00:05:57,079
operator. So concretely, if I
write A: =B, what this means in

72
00:05:57,079 --> 00:06:02,087
a computer, this means take the
value in B and use it to overwrite

73
00:06:02,087 --> 00:06:08,051
whatever the value of A. So this means we
will set A to be equal to the value of B.

74
00:06:08,051 --> 00:06:13,067
Okay, it's assignment. I can
also do A:=A+1. This means

75
00:06:13,067 --> 00:06:18,096
take A and increase its value by one.
Whereas in contrast, if I use the equals

76
00:06:18,096 --> 00:06:26,006
sign and I write A=B, then this is a
truth assertion. So if I write A=B,

77
00:06:26,006 --> 00:06:31,000
then I'm asserting that the value of A
equals to the value of B. So the left hand

78
00:06:31,000 --> 00:06:36,033
side, that's a computer operation, where
you set the value of A to be a value. The

79
00:06:36,033 --> 00:06:41,039
right hand side, this is asserting, I'm
just making a claim that the values of A

80
00:06:41,039 --> 00:06:46,027
and B are the same. And so, whereas I can
write A: =A+1, that means increment A by

81
00:06:46,027 --> 00:06:50,076
1. Hopefully, I won't ever write A=A+1.
Because that's just wrong.

82
00:06:50,076 --> 00:06:55,070
A and A+1 can never be equal to
the same values. So that's the first

83
00:06:55,070 --> 00:07:05,073
part of the definition. This alpha
here is a number that is called the

84
00:07:05,073 --> 00:07:12,036
learning rate. And what alpha does is, it
basically controls how big a step we take

85
00:07:12,036 --> 00:07:17,011
downhill with gradient descent. So if
alpha is very large, then that corresponds

86
00:07:17,011 --> 00:07:21,092
to a very aggressive gradient descent
procedure, where we're trying to take huge

87
00:07:21,092 --> 00:07:26,032
steps downhill. And if alpha is very
small, then we're taking little, little

88
00:07:26,032 --> 00:07:31,019
baby steps downhill. And, I'll come
back and say more about this later.

89
00:07:31,019 --> 00:07:35,066
About how to set alpha and so on.
And finally, this term here. That's the

90
00:07:35,066 --> 00:07:40,058
derivative term, I don't want to talk
about it right now, but I will derive this

91
00:07:40,058 --> 00:07:45,056
derivative term and tell you exactly what
this is based on. And some of you

92
00:07:45,056 --> 00:07:50,054
will be more familiar with calculus than
others, but even if you aren't familiar

93
00:07:50,054 --> 00:07:55,046
with calculus don't worry about it, I'll
tell you what you need to know about this

94
00:07:55,046 --> 00:08:00,057
term here. Now there's one more subtlety
about gradient descent which is, in

95
00:08:00,057 --> 00:08:05,083
gradient descent, we're going to
update theta0 and theta1. So

96
00:08:05,083 --> 00:08:10,069
this update takes place where j=0, and
j=1. So you're going to update j, theta0,

97
00:08:10,069 --> 00:08:15,095
and update theta1. And the subtlety of
how you implement gradient descent is,

98
00:08:15,095 --> 00:08:21,056
for this expression, for this
update equation, you want to

99
00:08:21,056 --> 00:08:31,038
simultaneously update theta0 and
theta1. What I mean by that is

100
00:08:31,038 --> 00:08:36,043
that in this equation,
we're going to update

101
00:08:36,043 --> 00:08:40,097
theta0:=theta0 - something, and update
theta1:=theta1 - something.

102
00:08:40,097 --> 00:08:45,083
And the way to implement this is,
you should compute the right hand

103
00:08:45,083 --> 00:08:52,067
side. Compute that thing for both theta0
and theta1, and then simultaneously at

104
00:08:52,067 --> 00:08:57,046
the same time update theta0 and
theta1. So let me say what I mean

105
00:08:57,046 --> 00:09:02,002
by that. This is a correct implementation
of gradient descent meaning simultaneous

106
00:09:02,002 --> 00:09:06,046
updates. I'm going to set temp0 equals
that, set temp1 equals that. So basically

107
00:09:06,046 --> 00:09:11,042
compute the right hand sides. And then having
computed the right hand sides and stored

108
00:09:11,042 --> 00:09:15,092
them together in temp0 and temp1,
I'm going to update theta0 and theta1

109
00:09:15,092 --> 00:09:20,024
simultaneously, because that's the
correct implementation. In contrast,

110
00:09:20,024 --> 00:09:25,053
here's an incorrect implementation that
does not do a simultaneous update. So in

111
00:09:25,053 --> 00:09:31,066
this incorrect implementation, we compute
temp0, and then we update theta0

112
00:09:31,066 --> 00:09:36,064
and then we compute temp1. Then we
update temp1. And the difference between

113
00:09:36,064 --> 00:09:41,087
the right hand side and the left hand side
implementations is that if we look down

114
00:09:41,087 --> 00:09:46,079
here, you look at this step, if by this
time you've already updated theta0

115
00:09:46,079 --> 00:09:51,089
then you would be using the new
value of theta0 to compute this

116
00:09:51,089 --> 00:09:57,034
derivative term and so this gives you a
different value of temp1 than the left

117
00:09:57,034 --> 00:10:01,056
hand side, because you've now
plugged in the new value of theta0

118
00:10:01,056 --> 00:10:05,085
into this equation. And so this on right
hand side is not a correct implementation

119
00:10:05,085 --> 00:10:09,091
of gradient descent. So I don't
want to say why you need to do the

120
00:10:09,091 --> 00:10:14,061
simultaneous updates, it turns
out that the way gradient descent

121
00:10:14,061 --> 00:10:18,073
is usually implemented, we'll say more
about it later, it actually turns out to

122
00:10:18,073 --> 00:10:22,049
be more natural to implement the
simultaneous update. And when people talk

123
00:10:22,049 --> 00:10:26,066
about gradient descent, they always mean
simultaneous update. If you implement the

124
00:10:26,066 --> 00:10:30,062
non-simultaneous update, it turns out
it will probably work anyway, but this

125
00:10:30,062 --> 00:10:34,074
algorithm on the right is not what people
people refer to as gradient descent and

126
00:10:34,074 --> 00:10:38,035
this is some other algorithm with
different properties. And for various

127
00:10:38,035 --> 00:10:42,022
reasons, this can behave in
slightly stranger ways. And

128
00:10:42,022 --> 00:10:46,062
what you should do is to really
implement the simultaneous update of

129
00:10:46,062 --> 00:10:52,031
gradient descent. So, that's the outline of the
gradient descent algorithm. In the next video,

130
00:10:52,031 --> 00:10:56,099
we're going to go into the details of the
derivative term, which I wrote out but

131
00:10:56,099 --> 00:11:01,079
didn't really define. And if you've taken
a calculus class before and if you're

132
00:11:01,079 --> 00:11:06,036
familiar with partial derivatives and
derivatives, it turns out that's exactly

133
00:11:06,036 --> 00:11:11,042
what that derivative term is. But in case
you aren't familiar with calculus, don't

134
00:11:11,042 --> 00:11:15,067
worry about it. The next video will give
you all the intuitions and will tell you

135
00:11:15,067 --> 00:11:19,088
everything you need to know to compute
that derivative term, even if you haven't

136
00:11:19,088 --> 00:11:24,029
seen calculus, or even if you haven't seen
partial derivatives before. And with

137
00:11:24,029 --> 00:11:28,028
that, with the next video, hopefully,
we'll be able to give all the intuitions

138
00:11:28,028 --> 00:11:30,017
you need to apply gradient descent.
