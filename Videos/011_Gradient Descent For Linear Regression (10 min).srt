
1
00:00:00,045 --> 00:00:02,020
In previous videos, we talked

2
00:00:02,023 --> 00:00:04,058
about the gradient descent algorithm

3
00:00:04,058 --> 00:00:06,000
and talked about the linear

4
00:00:06,000 --> 00:00:09,007
regression model and the squared error cost function.

5
00:00:09,007 --> 00:00:10,082
In this video, we're going to

6
00:00:10,082 --> 00:00:12,069
put together gradient descent with

7
00:00:12,069 --> 00:00:14,067
our cost function, and that

8
00:00:14,067 --> 00:00:16,065
will give us an algorithm for

9
00:00:16,065 --> 00:00:19,043
linear regression for fitting a straight line to our data.

10
00:00:21,000 --> 00:00:22,074
So, this is

11
00:00:22,074 --> 00:00:25,007
what we worked out in the previous videos.

12
00:00:25,007 --> 00:00:27,009
That's our gradient descent algorithm, which

13
00:00:27,009 --> 00:00:29,019
should be familiar, and you

14
00:00:29,019 --> 00:00:31,019
see the linear linear regression model

15
00:00:31,019 --> 00:00:36,046
with our linear hypothesis and our squared error cost function.

16
00:00:36,046 --> 00:00:38,061
What we're going to do is apply

17
00:00:38,061 --> 00:00:42,028
gradient descent to minimize

18
00:00:44,051 --> 00:00:47,053
our squared error cost function.

19
00:00:47,089 --> 00:00:49,038
Now, in order to apply

20
00:00:49,038 --> 00:00:50,089
gradient descent, in order

21
00:00:50,089 --> 00:00:52,069
to write this piece of

22
00:00:52,069 --> 00:00:54,019
code, the key term

23
00:00:54,019 --> 00:00:58,038
we need is this derivative term over here.

24
00:00:59,069 --> 00:01:00,079
So, we need to figure out

25
00:01:00,079 --> 00:01:02,082
what is this partial derivative term,

26
00:01:02,082 --> 00:01:04,047
and plug in the

27
00:01:04,047 --> 00:01:06,024
definition of the cost

28
00:01:06,024 --> 00:01:08,041
function J, this turns

29
00:01:08,041 --> 00:01:11,007
out to be this "inaudible"

30
00:01:12,061 --> 00:01:15,015
equals sum 1 through M of

31
00:01:15,015 --> 00:01:18,085
this squared error

32
00:01:20,045 --> 00:01:22,002
cost function term, and all

33
00:01:22,002 --> 00:01:23,079
I did here was I just

34
00:01:23,079 --> 00:01:25,053
you know plugged in the definition of

35
00:01:25,053 --> 00:01:28,027
the cost function there, and simplifying

36
00:01:28,027 --> 00:01:30,056
little bit more, this turns

37
00:01:30,056 --> 00:01:34,013
out to be equal to, this

38
00:01:34,013 --> 00:01:36,098
"inaudible" equals sum 1 through M

39
00:01:36,098 --> 00:01:40,060
of tetha zero plus tetha one, XI

40
00:01:41,016 --> 00:01:43,042
minus YI squared.

41
00:01:43,042 --> 00:01:44,077
And all I did there was took

42
00:01:44,077 --> 00:01:46,098
the definition for my hypothesis

43
00:01:46,098 --> 00:01:49,037
and plug that in there.

44
00:01:49,062 --> 00:01:51,066
And it turns out we need

45
00:01:51,066 --> 00:01:52,052
to figure out what is

46
00:01:52,052 --> 00:01:54,001
the partial derivative of two

47
00:01:54,001 --> 00:01:55,028
cases for J equals

48
00:01:55,028 --> 00:01:57,000
0 and for J equals 1 want

49
00:01:57,000 --> 00:01:58,054
to figure out what is this

50
00:01:58,054 --> 00:02:00,076
partial derivative for both the

51
00:02:00,076 --> 00:02:04,011
theta(0) case and the theta(1) case.

52
00:02:04,011 --> 00:02:07,001
And I'm just going to write out the answers.

53
00:02:07,001 --> 00:02:10,099
It turns out this first term simplifies

54
00:02:10,099 --> 00:02:14,021
to 1/M, sum over

55
00:02:14,021 --> 00:02:16,044
my training set of just

56
00:02:16,044 --> 00:02:21,014
that, X(i)-  Y(i).

57
00:02:21,014 --> 00:02:23,095
And for this term, partial derivative

58
00:02:23,095 --> 00:02:26,018
with respect to theta(1), it turns

59
00:02:26,018 --> 00:02:34,095
out I get this term: -Y(i)<i>X(i).</i>

60
00:02:35,003 --> 00:02:36,018
Okay.

61
00:02:36,040 --> 00:02:38,068
And computing these partial

62
00:02:38,068 --> 00:02:40,076
derivatives, so going from

63
00:02:40,076 --> 00:02:43,040
this equation to either

64
00:02:43,040 --> 00:02:46,041
of these equations down there, computing

65
00:02:46,041 --> 00:02:51,008
those partial derivative terms requires some multivariate calculus.

66
00:02:51,008 --> 00:02:53,011
If you know calculus, feel free

67
00:02:53,011 --> 00:02:54,082
to work through the derivations yourself

68
00:02:54,082 --> 00:02:57,005
and check take the derivatives

69
00:02:57,005 --> 00:02:59,085
you actually get the answers that I got.

70
00:02:59,085 --> 00:03:00,085
But if you are less

71
00:03:00,085 --> 00:03:02,061
familiar with calculus you don't

72
00:03:02,061 --> 00:03:04,023
worry about it, and it

73
00:03:04,023 --> 00:03:06,025
is fine to just take these equations

74
00:03:06,025 --> 00:03:08,002
worked out, and you

75
00:03:08,002 --> 00:03:09,046
won't need to know calculus or

76
00:03:09,046 --> 00:03:10,033
anything like that in order to

77
00:03:10,033 --> 00:03:14,055
do the homework, so to implement gradient descent you'd get that to work.

78
00:03:14,055 --> 00:03:16,051
But so, after these definitions,

79
00:03:16,051 --> 00:03:18,015
or after what we've worked

80
00:03:18,015 --> 00:03:19,099
out to be the derivatives, which

81
00:03:19,099 --> 00:03:21,031
is really just the slope of

82
00:03:21,031 --> 00:03:22,088
the cost function J.  We

83
00:03:22,088 --> 00:03:24,072
can now plug them back into

84
00:03:24,072 --> 00:03:27,015
our gradient descent algorithm.

85
00:03:27,015 --> 00:03:28,079
So here's gradient descent, or

86
00:03:28,079 --> 00:03:30,030
the regression, which is going

87
00:03:30,030 --> 00:03:32,097
to repeat until convergence, theta 0

88
00:03:32,097 --> 00:03:35,055
and theta one get updated as,

89
00:03:35,055 --> 00:03:37,016
you know, the same minus alpha

90
00:03:37,016 --> 00:03:39,059
times the derivative term.

91
00:03:39,059 --> 00:03:43,020
So, this term here.

92
00:03:43,020 --> 00:03:46,085
So, here's our linear regression algorithm.

93
00:03:46,085 --> 00:03:52,069
This first term here that

94
00:03:52,069 --> 00:03:54,049
term is, of course, just

95
00:03:54,049 --> 00:03:56,007
a partial derivative of respective

96
00:03:56,007 --> 00:03:59,099
theta zero, that we worked on in the previous slide.

97
00:03:59,099 --> 00:04:03,045
And this second term here,

98
00:04:03,045 --> 00:04:04,019
that term is just

99
00:04:04,019 --> 00:04:05,094
a partial derivative with respect to

100
00:04:05,094 --> 00:04:11,018
theta one that we worked out on the previous line.

101
00:04:11,018 --> 00:04:13,058
And just as a quick reminder,

102
00:04:13,058 --> 00:04:15,048
you must, when implementing gradient descent,

103
00:04:15,048 --> 00:04:17,006
there's actually there's detail that, you

104
00:04:17,006 --> 00:04:19,024
know, you should be implementing

105
00:04:19,024 --> 00:04:24,045
it so the update theta zero and theta one simultaneously.

106
00:04:24,045 --> 00:04:28,026
So, let's see how gradient descent works.

107
00:04:28,026 --> 00:04:29,044
One of the issues we solved

108
00:04:29,044 --> 00:04:32,083
gradient descent is that it can be susceptible to local optima.

109
00:04:32,083 --> 00:04:34,043
So, when I first explained gradient

110
00:04:34,044 --> 00:04:36,013
descent, I showed you this picture

111
00:04:36,013 --> 00:04:37,072
of it, you know, going downhill

112
00:04:37,072 --> 00:04:38,078
on the surface and we

113
00:04:38,078 --> 00:04:40,011
saw how, depending on where

114
00:04:40,011 --> 00:04:42,087
you're initializing, you can end up with different local optima.

115
00:04:42,087 --> 00:04:44,084
You know, you can end up here or here.

116
00:04:44,084 --> 00:04:46,095
But, it turns out that

117
00:04:46,095 --> 00:04:49,002
the cost function for gradient

118
00:04:49,002 --> 00:04:50,079
of cost function for linear regression

119
00:04:50,079 --> 00:04:52,075
is always going to be

120
00:04:52,075 --> 00:04:55,030
a bow-shaped function like this.

121
00:04:55,030 --> 00:04:57,057
The technical term for this

122
00:04:57,057 --> 00:05:01,016
is that this is called a convex function.

123
00:05:02,082 --> 00:05:04,092
And I'm not going

124
00:05:04,092 --> 00:05:06,056
to give the formal definition for what

125
00:05:06,056 --> 00:05:09,055
is a convex function, c-o-n-v-e-x, but

126
00:05:09,055 --> 00:05:11,031
informally a convex function

127
00:05:11,031 --> 00:05:14,079
means a bow-shaped function, you know, kind of like a bow shaped.

128
00:05:14,079 --> 00:05:18,000
And so, this function doesn't

129
00:05:18,000 --> 00:05:19,073
have any local optima, except

130
00:05:19,073 --> 00:05:22,043
for the one global optimum.

131
00:05:22,043 --> 00:05:24,026
And does gradient descent on

132
00:05:24,026 --> 00:05:25,058
this type of cost function which

133
00:05:25,058 --> 00:05:27,069
you get whenever you're using linear

134
00:05:27,069 --> 00:05:29,020
regression, it will always convert

135
00:05:29,020 --> 00:05:33,062
to the global optimum, because there are no other local optima other than global optimum.

136
00:05:33,062 --> 00:05:37,027
So now, let's see this algorithm in action.

137
00:05:38,002 --> 00:05:40,008
As usual, here are plots of

138
00:05:40,008 --> 00:05:42,018
the hypothesis function and of

139
00:05:42,018 --> 00:05:45,002
my cost function J.

140
00:05:45,076 --> 00:05:47,001
And so, let's see how

141
00:05:47,001 --> 00:05:49,068
to initialize my parameters at this value.

142
00:05:49,068 --> 00:05:51,065
You know, let's say, usually you

143
00:05:51,065 --> 00:05:53,059
initialize your parameters at zero

144
00:05:53,059 --> 00:05:56,028
for zero, theta zero and zero.

145
00:05:56,028 --> 00:05:58,033
For illustration in this

146
00:05:58,033 --> 00:05:59,094
specific presentation, I have

147
00:05:59,094 --> 00:06:02,061
initialised theta zero at

148
00:06:02,061 --> 00:06:06,083
about 900, and theta one at about minus 0.1, okay?

149
00:06:06,083 --> 00:06:09,079
And so, this corresponds to H

150
00:06:09,079 --> 00:06:12,002
over X, equals, you know,

151
00:06:12,002 --> 00:06:15,085
minus 900 minus 0.1 x

152
00:06:15,085 --> 00:06:19,034
is this line, so out here on the cost function.

153
00:06:19,034 --> 00:06:20,049
Now if we take one

154
00:06:20,049 --> 00:06:22,016
step of gradient descent, we end

155
00:06:22,016 --> 00:06:24,029
up going from this point

156
00:06:24,029 --> 00:06:27,006
out here, a little

157
00:06:27,006 --> 00:06:29,056
bit to the down left

158
00:06:29,056 --> 00:06:31,073
to that second point over there.

159
00:06:31,073 --> 00:06:35,027
And, you notice that my line changed a little bit.

160
00:06:35,027 --> 00:06:36,054
And, as I take another step

161
00:06:36,054 --> 00:06:41,042
at gradient descent, my line on the left will change.

162
00:06:41,042 --> 00:06:42,037
Right.

163
00:06:42,037 --> 00:06:43,080
And I have also

164
00:06:43,080 --> 00:06:47,054
moved to a new point on my cost function.

165
00:06:47,054 --> 00:06:48,074
And as I think further step

166
00:06:48,074 --> 00:06:50,069
is gradient descent, I'm going

167
00:06:50,069 --> 00:06:53,005
down in cost, right, so

168
00:06:53,005 --> 00:06:55,007
my parameter is following

169
00:06:55,007 --> 00:06:58,006
this trajectory, and if

170
00:06:58,006 --> 00:07:00,036
you look on the left, this corresponds

171
00:07:00,036 --> 00:07:04,002
to hypotheses that seem

172
00:07:04,002 --> 00:07:04,091
to be getting to be

173
00:07:04,091 --> 00:07:06,042
better and better fits for the

174
00:07:06,042 --> 00:07:09,098
data until eventually,

175
00:07:09,098 --> 00:07:12,066
I have now wound up at the global minimum.

176
00:07:12,066 --> 00:07:16,018
And this global minimum corresponds to

177
00:07:16,018 --> 00:07:20,050
this hypothesis, which gives me a good fit to the data.

178
00:07:20,092 --> 00:07:23,060
And so that's gradient

179
00:07:23,060 --> 00:07:24,096
descent, and we've just run

180
00:07:24,096 --> 00:07:27,030
it and gotten a good

181
00:07:27,030 --> 00:07:31,035
fit to my data set of housing prices.

182
00:07:31,035 --> 00:07:34,010
And you can now use it to predict.

183
00:07:34,010 --> 00:07:35,028
You know, if your friend has a

184
00:07:35,028 --> 00:07:36,045
house with a

185
00:07:36,045 --> 00:07:39,011
size 1250 square feet, you

186
00:07:39,011 --> 00:07:40,068
can now read off the value

187
00:07:40,068 --> 00:07:42,009
and tell them that, I don't

188
00:07:42,009 --> 00:07:43,018
know, maybe they can get

189
00:07:43,018 --> 00:07:47,015
$350,000 for their house.

190
00:07:48,060 --> 00:07:49,098
Finally, just to give

191
00:07:49,098 --> 00:07:51,093
this another name, it turns out

192
00:07:51,093 --> 00:07:52,099
that the algorithm that we

193
00:07:52,099 --> 00:07:55,003
just went over is sometimes

194
00:07:55,003 --> 00:07:57,007
called batch gradient descent.

195
00:07:57,007 --> 00:07:58,078
And it turns out in machine

196
00:07:58,078 --> 00:08:00,020
learning, I feel like us machine

197
00:08:00,020 --> 00:08:02,005
learning people, we're not always

198
00:08:02,005 --> 00:08:04,038
created has given me some algorithms.

199
00:08:04,038 --> 00:08:06,063
But the term batch gradient descent

200
00:08:06,063 --> 00:08:08,021
means that refers to the

201
00:08:08,021 --> 00:08:09,055
fact that, in every step

202
00:08:09,055 --> 00:08:11,016
of gradient descent we're looking

203
00:08:11,016 --> 00:08:13,064
at all of the training examples.

204
00:08:13,064 --> 00:08:15,078
So, in gradient descent, you

205
00:08:15,078 --> 00:08:18,087
know, when computing derivatives, we're computing

206
00:08:18,087 --> 00:08:21,030
these sums, this sum of.

207
00:08:21,030 --> 00:08:22,021
So, in every separate

208
00:08:22,021 --> 00:08:23,051
gradient descent, we end up

209
00:08:23,051 --> 00:08:25,027
computing something like this, that

210
00:08:25,027 --> 00:08:28,043
sums over our M training examples.

211
00:08:28,043 --> 00:08:29,083
And so the term batch gradient

212
00:08:29,083 --> 00:08:31,019
descent refers to the fact

213
00:08:31,019 --> 00:08:33,019
when looking at the entire batch

214
00:08:33,019 --> 00:08:34,055
of training examples, and again,

215
00:08:34,055 --> 00:08:35,074
this is really, really not

216
00:08:35,074 --> 00:08:36,091
a great name, but this is

217
00:08:36,091 --> 00:08:39,044
what Machine Learning people call it.

218
00:08:39,044 --> 00:08:40,095
And it turns out there are

219
00:08:40,095 --> 00:08:42,066
sometimes other versions of

220
00:08:42,066 --> 00:08:43,091
gradient descent that are not

221
00:08:43,091 --> 00:08:45,096
batch versions but instead do

222
00:08:45,096 --> 00:08:47,075
not look at the entire traning set

223
00:08:47,075 --> 00:08:49,072
but look at small subsets

224
00:08:49,072 --> 00:08:51,052
of the training sets at the time,

225
00:08:51,052 --> 00:08:54,087
and we'll talk about those versions later in this course as well.

226
00:08:54,087 --> 00:08:56,019
But for now, using the algorithm

227
00:08:56,019 --> 00:08:57,040
you just learned, now we're

228
00:08:57,040 --> 00:08:58,075
using batch gradient descent, you

229
00:08:58,075 --> 00:09:01,015
now know how to implement

230
00:09:01,015 --> 00:09:04,014
gradient descent, or linear regression.

231
00:09:05,085 --> 00:09:08,067
So that's linear regression with gradient descent.

232
00:09:09,034 --> 00:09:11,074
If you've seen advanced linear algebra

233
00:09:11,074 --> 00:09:12,067
before so some you may

234
00:09:12,067 --> 00:09:14,020
have taken a class with advanced

235
00:09:14,020 --> 00:09:16,027
linear algebra, you might

236
00:09:16,029 --> 00:09:17,067
know that there exists a solution

237
00:09:17,067 --> 00:09:19,075
for numerically solving for the

238
00:09:19,075 --> 00:09:20,091
minimum of the cost function

239
00:09:20,091 --> 00:09:22,056
J, without needing to

240
00:09:22,056 --> 00:09:25,060
use and iterative algorithm like gradient descent.

241
00:09:25,060 --> 00:09:27,015
Later in this course we will

242
00:09:27,015 --> 00:09:28,009
talk about that method as

243
00:09:28,009 --> 00:09:29,075
well that just solves for the

244
00:09:29,075 --> 00:09:31,007
minimum cost function J without

245
00:09:31,007 --> 00:09:33,079
needing this multiple steps of gradient descent.

246
00:09:33,079 --> 00:09:37,065
That other method is called normal equations methods.

247
00:09:37,065 --> 00:09:39,016
And, but in case you

248
00:09:39,016 --> 00:09:40,014
have heard of that method, it turns

249
00:09:40,014 --> 00:09:41,062
out gradient descent will

250
00:09:41,062 --> 00:09:43,077
scale better to larger data

251
00:09:43,077 --> 00:09:45,000
sets than that normal equals

252
00:09:45,000 --> 00:09:47,031
method and, now that

253
00:09:47,031 --> 00:09:48,075
we know about gradient descent, we'll

254
00:09:48,075 --> 00:09:49,092
be able to use it in

255
00:09:49,092 --> 00:09:51,038
lots of different contexts, and we'll

256
00:09:51,038 --> 00:09:54,084
use it in lots of different Machine Learning problems as well.

257
00:09:54,084 --> 00:09:57,013
So, congrats on learning

258
00:09:57,013 --> 00:10:00,031
about your first Machine Learning algorithm.

259
00:10:00,031 --> 00:10:02,050
We'll later have exercises in

260
00:10:02,050 --> 00:10:03,065
which we'll ask you to

261
00:10:03,065 --> 00:10:05,006
implement gradient descent and

262
00:10:05,006 --> 00:10:07,007
hopefully see these algorithms work for yourselves.

263
00:10:07,007 --> 00:10:08,095
But before that I first

264
00:10:08,095 --> 00:10:10,058
want to tell you in

265
00:10:10,058 --> 00:10:11,091
the next set of videos, the

266
00:10:11,091 --> 00:10:13,022
first want to tell you about

267
00:10:13,022 --> 00:10:15,072
a generalization of the gradient descent

268
00:10:15,072 --> 00:10:16,093
algorithm that will make

269
00:10:16,093 --> 00:10:18,040
it much more powerful and I

270
00:10:18,040 --> 99:59:59,000
guess I will tell you about that in the next video.
