
1
00:00:00,040 --> 00:00:01,057
In this video, I wanna give

2
00:00:01,072 --> 00:00:04,041
you more practical tips for getting gradient descent to work.

3
00:00:05,003 --> 00:00:06,024
The ideas in this video will

4
00:00:06,046 --> 00:00:08,025
center around the learning rate alpha.

5
00:00:09,092 --> 00:00:11,024
Concretely, here's the gradient

6
00:00:11,064 --> 00:00:13,040
descent update rule and what

7
00:00:13,065 --> 00:00:14,036
I want to do in this video

8
00:00:14,086 --> 00:00:16,062
is tell you about what

9
00:00:16,078 --> 00:00:18,046
I think of as debugging and some

10
00:00:18,060 --> 00:00:19,064
tips for making sure that

11
00:00:19,085 --> 00:00:21,017
Gradient Descent is working correctly

12
00:00:22,039 --> 00:00:23,037
and second, I want to tell you

13
00:00:23,058 --> 00:00:25,051
how to choose the rates

14
00:00:25,089 --> 00:00:26,071
out for, but this is how

15
00:00:27,007 --> 00:00:28,053
I go about choosing it.

16
00:00:29,021 --> 00:00:30,041
Here's something that I often do

17
00:00:30,064 --> 00:00:32,071
to make sure gradient descent is working correctly.

18
00:00:34,011 --> 00:00:35,056
The job of gradient descent is

19
00:00:35,082 --> 00:00:37,003
to find a value of

20
00:00:37,010 --> 00:00:38,050
theta for you that, you

21
00:00:38,063 --> 00:00:40,082
know, hopefully minimizes the cost function j of theta.

22
00:00:42,067 --> 00:00:43,086
What I often do is therefore

23
00:00:44,029 --> 00:00:45,089
pluck the cost function j

24
00:00:46,010 --> 00:00:48,089
of theta as gradient descent runs.

25
00:00:49,075 --> 00:00:51,009
So, the x-axis here is

26
00:00:51,031 --> 00:00:52,032
the number of iteration of gradient

27
00:00:52,085 --> 00:00:53,096
descent and as gradient descent

28
00:00:54,025 --> 00:00:55,079
runs, you'll hopefully get a

29
00:00:55,096 --> 00:00:58,025
plot that maybe looks like this.

30
00:00:59,067 --> 00:01:00,088
Notice that the x-axis is

31
00:01:01,017 --> 00:01:02,092
a number of iterations previously

32
00:01:03,057 --> 00:01:04,076
we were looking at plots of

33
00:01:05,006 --> 00:01:06,065
J of theta where the

34
00:01:07,004 --> 00:01:08,009
X-axis, where the horizontal axis,

35
00:01:08,095 --> 00:01:12,026
was the parameter vector theta but this is not where this is.

36
00:01:13,007 --> 00:01:14,073
Concretely, what this point

37
00:01:15,009 --> 00:01:17,073
is is I'm going

38
00:01:17,090 --> 00:01:19,050
to rank gradient descent for hundred iterations.

39
00:01:20,057 --> 00:01:22,006
And whatever value I get

40
00:01:22,062 --> 00:01:23,090
for theta after a hundred

41
00:01:24,010 --> 00:01:25,042
of the rations and get,

42
00:01:25,060 --> 00:01:26,076
you know, some value of theta

43
00:01:27,015 --> 00:01:28,095
after a hundred iterations and I'm

44
00:01:29,009 --> 00:01:30,035
going to evaluate the cost

45
00:01:30,067 --> 00:01:32,057
function J of theta for

46
00:01:32,084 --> 00:01:33,078
the value of theta I get

47
00:01:34,012 --> 00:01:36,001
after a hundred iterations and this

48
00:01:36,021 --> 00:01:37,060
vertical height is the

49
00:01:37,068 --> 00:01:39,073
value of J of theta for

50
00:01:39,090 --> 00:01:40,075
the value of theta I got

51
00:01:41,010 --> 00:01:42,015
after a hundred other ratios of

52
00:01:42,021 --> 00:01:43,084
gradient descent and this

53
00:01:44,004 --> 00:01:45,070
point here, that corresponds

54
00:01:46,051 --> 00:01:48,012
to the value of J of

55
00:01:48,023 --> 00:01:49,071
theta for the theta

56
00:01:50,006 --> 00:01:51,081
that I get after I've

57
00:01:52,004 --> 00:01:53,068
run grade and descent for two hundred iterations.

58
00:01:55,023 --> 00:01:56,018
So what this plot is showing,

59
00:01:56,071 --> 00:01:58,009
is it's showing the value of

60
00:01:58,020 --> 00:02:01,020
your cost function after iteration of grade and descent.

61
00:02:02,001 --> 00:02:03,012
And, if grade and descent is

62
00:02:03,034 --> 00:02:04,098
working properly, then J

63
00:02:05,018 --> 00:02:06,093
of theta should decrease.

64
00:02:10,006 --> 00:02:10,065
after every iteration.

65
00:02:17,081 --> 00:02:19,028
And one useful thing

66
00:02:19,053 --> 00:02:20,037
that this sort of plot can

67
00:02:20,050 --> 00:02:21,075
tell you also is that

68
00:02:22,050 --> 00:02:23,087
if you look at the specific figure

69
00:02:24,015 --> 00:02:25,040
that I've drawn, it looks like

70
00:02:26,003 --> 00:02:27,034
by the time you've gotten out

71
00:02:27,058 --> 00:02:28,078
to three hundred iterations,

72
00:02:29,072 --> 00:02:31,000
between three and four hundred

73
00:02:31,031 --> 00:02:32,080
iterations, in this segment, it

74
00:02:32,090 --> 00:02:35,069
looks like J of theta hasn't gone down much more.

75
00:02:35,081 --> 00:02:36,072
So by the time you get

76
00:02:36,096 --> 00:02:38,059
to four hundred iterations, it looks

77
00:02:38,081 --> 00:02:40,080
like this curve has flattened out here.

78
00:02:41,055 --> 00:02:43,018
And so, way out

79
00:02:43,034 --> 00:02:44,040
here at four hundred iterations, it

80
00:02:44,050 --> 00:02:45,056
looks like grade and descend has

81
00:02:45,084 --> 00:02:47,075
more or less converged because your

82
00:02:47,087 --> 00:02:49,056
cost function isn't going down much more.

83
00:02:50,049 --> 00:02:51,043
So looking at this figure can

84
00:02:51,059 --> 00:02:52,099
also help you judge

85
00:02:53,041 --> 00:02:55,012
whether or not gradient descent has converged.

86
00:02:57,055 --> 00:02:58,050
By the way, the number of

87
00:02:58,084 --> 00:03:00,041
iterations that gradient descent takes

88
00:03:00,078 --> 00:03:01,075
to converge for a physical

89
00:03:01,090 --> 00:03:03,081
application can vary a lot.

90
00:03:04,019 --> 00:03:05,062
So maybe for one application gradient

91
00:03:06,012 --> 00:03:07,056
descent may converge after just

92
00:03:07,083 --> 00:03:09,065
thirty iterations, for a

93
00:03:10,021 --> 00:03:12,027
different application gradient descent

94
00:03:12,059 --> 00:03:14,015
made the 3,000 iterations.

95
00:03:15,005 --> 00:03:17,055
For another learning algorithm

96
00:03:17,097 --> 00:03:19,009
it may take three million iterations.

97
00:03:19,081 --> 00:03:20,062
It turns out to be

98
00:03:20,072 --> 00:03:22,021
very difficult to tell in

99
00:03:22,030 --> 00:03:24,000
advance how many iterations gradient

100
00:03:24,036 --> 00:03:25,075
descent needs to converge, and

101
00:03:26,015 --> 00:03:27,094
is usually by plotting this sort of plot.

102
00:03:28,093 --> 00:03:32,025
Plotting the cause function as we increase the number of iterations.

103
00:03:32,096 --> 00:03:33,087
It's usually by looking at these

104
00:03:34,034 --> 00:03:35,040
plots that I tried to tell

105
00:03:35,059 --> 00:03:37,006
if gradient descent has converged.

106
00:03:38,059 --> 00:03:39,081
It is also possible to come

107
00:03:40,012 --> 00:03:42,040
up with automatic convergence test; namely

108
00:03:42,074 --> 00:03:44,006
to have an algorithm to try

109
00:03:44,028 --> 00:03:46,027
to tell you if gradient descent

110
00:03:46,059 --> 00:03:48,040
has converged and here's maybe

111
00:03:48,062 --> 00:03:50,015
a pretty typical example of an

112
00:03:50,024 --> 00:03:52,031
automatic convergence test and

113
00:03:52,053 --> 00:03:53,094
so, you test the clear convergence

114
00:03:54,096 --> 00:03:56,024
if your cause function jf theta

115
00:03:57,002 --> 00:03:58,015
decreases by less than

116
00:03:58,037 --> 00:04:01,025
some small value epsilon, some

117
00:04:01,040 --> 00:04:02,031
small value ten to the

118
00:04:02,040 --> 00:04:03,081
minus three in one iteration,

119
00:04:05,025 --> 00:04:06,065
but I find that usually

120
00:04:07,006 --> 00:04:09,053
choosing what this threshold is is pretty difficult.

121
00:04:10,071 --> 00:04:11,087
So, in order to check

122
00:04:12,003 --> 00:04:13,075
your gradient descent has converged, I

123
00:04:14,009 --> 00:04:15,012
actually tend to look at

124
00:04:15,034 --> 00:04:16,072
plots like like this

125
00:04:17,005 --> 00:04:18,012
figure on the left rather than

126
00:04:18,031 --> 00:04:20,063
rely on an automatic convergence test.

127
00:04:21,076 --> 00:04:22,063
Looking at this sort of

128
00:04:22,077 --> 00:04:24,013
figure can also tell you or

129
00:04:24,031 --> 00:04:25,055
give you an advanced warning if maybe

130
00:04:25,081 --> 00:04:27,049
gradient descent is not working correctly.

131
00:04:28,068 --> 00:04:29,076
Concretely, if you plug

132
00:04:30,019 --> 00:04:31,041
jf theta as a function

133
00:04:31,064 --> 00:04:34,044
of number of iterations, then, if

134
00:04:34,085 --> 00:04:35,050
you see a figure like this,

135
00:04:35,081 --> 00:04:36,072
where J of theta is actually

136
00:04:37,012 --> 00:04:38,088
increasing, then that gives

137
00:04:39,011 --> 00:04:41,075
you a clear sign that gradient descent is not working.

138
00:04:42,088 --> 00:04:44,006
And a figure like this

139
00:04:44,051 --> 00:04:46,093
usually means that you should be using a learning rate alpha.

140
00:04:48,026 --> 00:04:49,042
If J of theta is actually

141
00:04:49,062 --> 00:04:51,020
increasing, the most common

142
00:04:51,057 --> 00:04:52,082
cause for that is if

143
00:04:53,018 --> 00:04:54,035
you're trying to minimize

144
00:04:54,086 --> 00:04:57,089
the function that maybe looks like this.

145
00:04:59,033 --> 00:05:00,037
That's if your learning rate is

146
00:05:00,045 --> 00:05:01,045
too big then if you

147
00:05:01,060 --> 00:05:02,093
start off there, gradient descent

148
00:05:03,019 --> 00:05:05,026
may overshoot the minimum, send

149
00:05:05,044 --> 00:05:06,075
you there, then if only there's

150
00:05:07,007 --> 00:05:08,014
too big, you may overshoot again,

151
00:05:08,050 --> 00:05:10,037
it will send you there and

152
00:05:10,050 --> 00:05:11,091
so on so that what

153
00:05:12,025 --> 00:05:13,061
you really wanted was really

154
00:05:13,081 --> 00:05:16,036
start here and for to slowly go downhill.

155
00:05:17,093 --> 00:05:19,025
But if the learning is too

156
00:05:19,044 --> 00:05:20,095
big then gradient descent can

157
00:05:21,025 --> 00:05:22,057
instead keep on over

158
00:05:22,075 --> 00:05:24,030
shooting the minimum so

159
00:05:24,044 --> 00:05:25,067
that you actually end up

160
00:05:26,016 --> 00:05:27,017
getting worse and worse instead

161
00:05:27,020 --> 00:05:28,072
of getting the higher values of

162
00:05:28,077 --> 00:05:29,079
the cost function j of theta

163
00:05:30,070 --> 00:05:31,051
so do you end up with a

164
00:05:31,067 --> 00:05:33,013
plot like and if you

165
00:05:33,022 --> 00:05:34,011
see a plot like this the

166
00:05:34,018 --> 00:05:35,086
fix usually is to just

167
00:05:36,008 --> 00:05:37,068
use a smaller value of alpha.

168
00:05:38,016 --> 00:05:39,063
Oh, and also of course make

169
00:05:39,079 --> 00:05:41,061
sure that your code does not have a bug in it.

170
00:05:41,079 --> 00:05:43,012
But usually to watch it

171
00:05:43,020 --> 00:05:44,054
out of the firms is the

172
00:05:44,060 --> 00:05:46,031
most common, could be a common problem.

173
00:05:49,005 --> 00:05:50,041
Similarly, sometimes, you may

174
00:05:50,056 --> 00:05:51,089
also see j of theta

175
00:05:52,012 --> 00:05:53,007
do something like this and it

176
00:05:53,018 --> 00:05:54,005
go down for a while then

177
00:05:54,016 --> 00:05:56,013
go up then go down for a while then go up.

178
00:05:56,032 --> 00:05:57,012
Go down for a while, it

179
00:05:57,022 --> 00:05:58,091
goes up and so on and

180
00:05:58,093 --> 00:05:59,094
and to fix for something like

181
00:06:00,013 --> 00:06:02,075
this is also to use a smaller value of algorithm.

182
00:06:04,008 --> 00:06:04,095
I'm not going to prove it

183
00:06:05,007 --> 00:06:06,081
here, but undeniable assumptions about

184
00:06:07,010 --> 00:06:09,075
the cost function, which does proof of linear regression.

185
00:06:10,082 --> 00:06:12,047
You can show of mathematicians have

186
00:06:12,057 --> 00:06:13,058
shown that if your learning

187
00:06:13,091 --> 00:06:15,006
rate offer is small enough

188
00:06:15,083 --> 00:06:18,043
then j of theta should decrease on every single iteration.

189
00:06:19,002 --> 00:06:20,085
So, if this doesn't happen, probably

190
00:06:21,033 --> 00:06:22,019
means algorithm is too big then

191
00:06:22,026 --> 00:06:23,089
you should send a smaller, but of

192
00:06:23,097 --> 00:06:24,079
course, you all So you don't

193
00:06:24,088 --> 00:06:25,068
want your learning rate to be

194
00:06:25,073 --> 00:06:26,095
too small because if you

195
00:06:27,006 --> 00:06:27,091
do that, if you were

196
00:06:28,002 --> 00:06:30,042
to do that, then gradient descent can be slow to converge.

197
00:06:31,049 --> 00:06:32,051
And if alpha were too

198
00:06:32,080 --> 00:06:34,005
small, you might end up

199
00:06:34,074 --> 00:06:36,075
starting out here, say, and,

200
00:06:36,095 --> 00:06:37,091
you know, end up taking just

201
00:06:38,022 --> 00:06:39,069
minuscule, minuscule baby steps.

202
00:06:40,074 --> 00:06:40,074
Right?

203
00:06:40,088 --> 00:06:42,022
And just taking a lot

204
00:06:42,098 --> 00:06:46,031
of iterations before you finally get to the minimum.

205
00:06:47,008 --> 00:06:47,098
And so, if alpha is too

206
00:06:48,011 --> 00:06:49,050
small, gradient descent can

207
00:06:49,056 --> 00:06:51,022
make very slow progress and be slow to converge.

208
00:06:53,081 --> 00:06:55,010
To summarize, if the learning

209
00:06:55,037 --> 00:06:57,006
rate is too small, you can

210
00:06:57,026 --> 00:06:59,037
have a slow convergence problem, and

211
00:06:59,062 --> 00:07:00,093
if the learning rate is too

212
00:07:01,010 --> 00:07:02,033
large, j of theta may

213
00:07:02,047 --> 00:07:03,043
not decrease on every iteration

214
00:07:04,039 --> 00:07:05,056
and may not even converge.

215
00:07:07,010 --> 00:07:08,020
In some cases, if the learning

216
00:07:08,052 --> 00:07:10,007
rate is too large, slow convergence

217
00:07:10,099 --> 00:07:14,070
is also possible, but the

218
00:07:14,080 --> 00:07:16,001
more common problem you see

219
00:07:16,027 --> 00:07:17,037
is that just that j of

220
00:07:17,043 --> 00:07:19,026
theta may not decrease on every iteration.

221
00:07:20,054 --> 00:07:21,089
And in order to debug all

222
00:07:22,013 --> 00:07:24,017
of these things, often plotting that

223
00:07:24,043 --> 00:07:25,073
j of theta as a function

224
00:07:26,006 --> 00:07:28,069
of the number of iterations can help you figure out what's going on.

225
00:07:29,026 --> 00:07:30,075
Concretely, what I actually

226
00:07:31,022 --> 00:07:32,013
do when I run gradient

227
00:07:32,051 --> 00:07:34,072
descent is I would try a range of values.

228
00:07:35,000 --> 00:07:36,017
So just try running gradient descent

229
00:07:36,057 --> 00:07:37,068
with a range of values for

230
00:07:37,098 --> 00:07:39,061
alpha, like 0.001, 0.01,

231
00:07:39,086 --> 00:07:41,024
so these are a

232
00:07:41,044 --> 00:07:43,006
factor of 10 differences, and

233
00:07:43,027 --> 00:07:44,027
for these differences of this

234
00:07:44,042 --> 00:07:45,060
of alpha, just plot j of

235
00:07:45,075 --> 00:07:46,080
theta as a function of number

236
00:07:47,002 --> 00:07:48,074
of iterations and then pick

237
00:07:49,017 --> 00:07:50,093
the value of alpha that, you

238
00:07:51,004 --> 00:07:54,022
know, seems to be causing j of theta to decrease rapidly.

239
00:07:55,061 --> 00:07:58,008
In fact, what I do actually isn't these steps of ten.

240
00:07:58,058 --> 00:07:59,051
So, you know, this is

241
00:07:59,088 --> 00:08:01,077
a scale factor of ten if you reach the top.

242
00:08:02,050 --> 00:08:03,045
What I'll actually do is try

243
00:08:03,087 --> 00:08:08,048
this range of values and

244
00:08:08,061 --> 00:08:09,076
so on where this is,

245
00:08:09,097 --> 00:08:12,018
you know, opening 001

246
00:08:12,018 --> 00:08:13,025
then increase the linear rate to

247
00:08:13,050 --> 00:08:15,031
3.4 to get 0.03 and then

248
00:08:15,050 --> 00:08:16,031
to step up this is another

249
00:08:17,032 --> 00:08:20,025
roughly 3 fold increase point

250
00:08:21,070 --> 00:08:22,043
of 0.03 to 0.01s and so these

251
00:08:22,075 --> 00:08:24,081
are roughly, you know,

252
00:08:26,001 --> 00:08:27,075
trying out gradient descents with each

253
00:08:28,001 --> 00:08:29,011
value I try being about

254
00:08:29,037 --> 00:08:30,089
3X bigger than the previous value.

255
00:08:32,012 --> 00:08:33,025
So what I'll do is a range

256
00:08:33,040 --> 00:08:34,058
of values until I've made sure

257
00:08:34,087 --> 00:08:35,096
that I've found one value that

258
00:08:36,011 --> 00:08:36,088
is too small and made sure

259
00:08:37,008 --> 00:08:38,013
I found one value that is

260
00:08:38,025 --> 00:08:39,038
too large, and then I sort

261
00:08:39,063 --> 00:08:40,096
of try to pick the largest

262
00:08:41,040 --> 00:08:42,069
possible value or just something

263
00:08:43,011 --> 00:08:45,008
slightly smaller than the

264
00:08:45,021 --> 00:08:47,039
largest reasonable value that I found.

265
00:08:47,075 --> 00:08:48,078
And when I do that

266
00:08:49,026 --> 00:08:50,035
usually it just gives me

267
00:08:50,052 --> 00:08:52,001
a good learning rate for my problem.

268
00:08:53,023 --> 00:08:53,091
And if you do this

269
00:08:54,008 --> 00:08:55,003
too, hopefully you will be

270
00:08:55,012 --> 00:08:56,019
able to choose a good

271
00:08:56,046 --> 00:08:57,034
learning rate for your implementation

272
00:08:58,050 --> 00:08:58,086
of gradient descent.
