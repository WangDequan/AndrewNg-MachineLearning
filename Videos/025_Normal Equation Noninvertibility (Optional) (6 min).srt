
1
00:00:00,000 --> 00:00:03,016
In this video, I want to talk about the normal equation

2
00:00:03,016 --> 00:00:05,021
and non-invertibility.

3
00:00:05,021 --> 00:00:07,087
This is a somewhat more advanced concept,

4
00:00:07,087 --> 00:00:10,028
but it is something that I've often been asked about.

5
00:00:10,028 --> 00:00:12,071
And so I wanted to talk about it here.

6
00:00:12,071 --> 00:00:14,075
But this is a somewhat more advanced concept,

7
00:00:14,075 --> 00:00:17,098
so feel free to consider this optional material

8
00:00:17,098 --> 00:00:22,041
There's a phenomenon that you may run into

9
00:00:22,041 --> 00:00:24,041
that's maybe for some of you useful to understand.

10
00:00:24,041 --> 00:00:26,061
But even if you don't understand it,

11
00:00:26,061 --> 00:00:28,044
the normal equation and linear regression,

12
00:00:28,044 --> 00:00:30,053
you should really get that to work okay.

13
00:00:30,053 --> 00:00:33,019
Here's the issue:

14
00:00:33,019 --> 00:00:35,069
For those of you that are maybe somewhat

15
00:00:35,069 --> 00:00:37,087
more familar with linear algebra,

16
00:00:37,087 --> 00:00:39,088
what some students have asked me is,

17
00:00:39,088 --> 00:00:42,054
when computing this

18
00:00:42,054 --> 00:00:45,013
theta equals ( X<u>transpose X )<u>inverse X<u>transpose y</u></u></u>

19
00:00:45,013 --> 00:00:49,047
what if the matrix X<u>transpose X is non-invertible?</u>

20
00:00:49,047 --> 00:00:52,033
So, for those of you that know a bit more linear algebra

21
00:00:52,033 --> 00:00:55,017
you may know that only some matrices

22
00:00:55,017 --> 00:00:58,059
are invertible and some matrices do not have an inverse

23
00:00:58,059 --> 00:01:00,054
we call those non-invertible matrices,

24
00:01:00,054 --> 00:01:04,073
singular or degenerate matrices.

25
00:01:04,073 --> 00:01:08,089
The issue or the problem of X<u>tranpose X being non-invertible</u>

26
00:01:08,089 --> 00:01:11,028
should happen pretty rarely.

27
00:01:11,028 --> 00:01:16,074
And in Octave, if you implement this to compute theta,

28
00:01:16,074 --> 00:01:20,063
it turns out that this will actually do the right thing.

29
00:01:20,063 --> 00:01:24,062
I'm getting a little bit technical now and I don't want to go into details,

30
00:01:24,062 --> 00:01:28,020
but Octave has two functions for inverting matrices:

31
00:01:28,020 --> 00:01:32,014
One is called pinv(), and the other is called inv().

32
00:01:32,014 --> 00:01:36,008
The differences between these two are somewhat technical.

33
00:01:36,008 --> 00:01:38,010
One's called the pseudo-inverse, one's called the inverse.

34
00:01:38,010 --> 00:01:42,065
You can show mathemically so as long as you use the pinv() function,

35
00:01:42,065 --> 00:01:47,014
then this will actually compute the value of theta that you want,

36
00:01:47,014 --> 00:01:51,022
even if X<u>transpose X is non-invertible.</u>

37
00:01:51,022 --> 00:01:54,009
The specific details between what is the difference between

38
00:01:54,009 --> 00:01:55,095
pinv() and what is inv()

39
00:01:55,095 --> 00:01:58,056
that is somewhat advanced numerical computing concepts,

40
00:01:58,056 --> 00:02:00,090
that I don't really want to get into.

41
00:02:00,090 --> 00:02:02,099
But I thought in this optional

42
00:02:02,099 --> 00:02:04,067
video I try to give you a little bit of intuition

43
00:02:04,067 --> 00:02:08,082
about what it means that X<u>transpose X to be non-invertible.</u>

44
00:02:08,082 --> 00:02:12,010
For those of you that know a bit more linear algebra

45
00:02:12,010 --> 00:02:13,055
and might be interested.

46
00:02:13,055 --> 00:02:15,094
I'm not going to proove this mathematically,

47
00:02:15,094 --> 00:02:18,068
but if X<u>transpose X is non-invertible,</u>

48
00:02:18,068 --> 00:02:22,059
there are usually two most common causes:

49
00:02:22,059 --> 00:02:26,023
The first cause is if somehow, in your learning problem,

50
00:02:26,023 --> 00:02:28,046
you have redundant features,

51
00:02:28,046 --> 00:02:30,084
concretely, if you try to predict housing prices

52
00:02:30,084 --> 00:02:34,087
and if x<u>1 is the size of a house in square-feet,</u>

53
00:02:34,087 --> 00:02:37,079
and x<u>2 is the size of the house in square-meters,</u>

54
00:02:37,079 --> 00:02:46,007
then, you know, 1 meter is equal to 3.28 feet, rounded to two decimals,

55
00:02:46,007 --> 00:02:48,094
and so your two features will always satisfy the constraint

56
00:02:48,094 --> 00:02:55,037
that x<u>1 equals 3(.28)^2 times x<u>2.</u></u>

57
00:02:55,037 --> 00:02:59,010
And you can show, for those of you - this is somehwat advanced linear algebra now,

58
00:02:59,010 --> 00:03:01,016
but if you're an expert in linear algebra,

59
00:03:01,016 --> 00:03:05,027
you can actually show that if your two features are related via a linear equation like this,

60
00:03:05,027 --> 00:03:09,009
then matrix X<u>transpose X will be non-invertible.</u>

61
00:03:09,009 --> 00:03:13,031
The second thing that can cause X<u>transpose X to be non-invertible</u>

62
00:03:13,031 --> 00:03:17,004
is if you're trying to run a learning algorithm

63
00:03:17,004 --> 00:03:18,085
with a <i>lot</i> of a features.

64
00:03:18,085 --> 00:03:23,003
Concretely, if m is less than or equal to n.

65
00:03:23,003 --> 00:03:27,072
For example, if you imagine that you have m equals 10 training examples

66
00:03:27,072 --> 00:03:31,019
and that you have n equals 100 features, then you're trying

67
00:03:31,019 --> 00:03:36,082
to fit a parameter vector theta, which is (n+1)-dimensional,

68
00:03:36,082 --> 00:03:39,030
so it's a 101-dimensional

69
00:03:39,030 --> 00:03:43,060
you're trying to fit a 101 parameters from just 10 training examples.

70
00:03:43,060 --> 00:03:46,089
And this turns out to sometimes work,

71
00:03:46,089 --> 00:03:49,007
but to not always be a good idea.

72
00:03:49,007 --> 00:03:52,021
Because, as we see later, you might not have enough data

73
00:03:52,021 --> 00:03:58,043
if you only have 10 examples to fit 100 or 101 parameters.

74
00:03:58,043 --> 00:04:01,092
We'll see later in this course, why this might be too little data

75
00:04:01,092 --> 00:04:04,041
to fit this many parameters.

76
00:04:04,041 --> 00:04:07,054
But commonly, what we do then if m is less than n,

77
00:04:07,054 --> 00:04:12,051
is to see if we can either delete some features or to use a technique

78
00:04:12,051 --> 00:04:14,068
called regularization,

79
00:04:14,068 --> 00:04:17,047
which is something that we will talk about a bit later in this course as well,

80
00:04:17,047 --> 00:04:21,090
that will kind of let you fit a <i>lot</i> of parameters using a <i>lot</i> of features

81
00:04:21,090 --> 00:04:24,011
even if you have a relatively small training set.

82
00:04:24,011 --> 00:04:27,069
But this regularization will be a later topic in this course.

83
00:04:27,069 --> 00:04:32,062
But to summarize, if ever you find that X<u>transpose X is singular</u>

84
00:04:32,062 --> 00:04:35,087
or alternatively find is non-invertible,

85
00:04:35,087 --> 00:04:38,038
what I would recommend you do is

86
00:04:38,038 --> 00:04:42,001
first: look at your features and see if you have redundant features

87
00:04:42,001 --> 00:04:45,030
like these x<u>1 and x<u>2 being linearly dependent,</u></u>

88
00:04:45,030 --> 00:04:48,001
or being a linear function of each other, like so

89
00:04:48,001 --> 00:04:49,084
and if you do have redundant features and

90
00:04:49,084 --> 00:04:51,049
if you just delete one of these features -

91
00:04:51,049 --> 00:04:53,072
you really don't need both of these features,

92
00:04:53,072 --> 00:04:55,060
so if you just delete one of these features

93
00:04:55,060 --> 00:04:58,058
that will solve your non-invertibility problem

94
00:04:58,058 --> 00:05:02,065
and, so first think through my features and check if any are redundant

95
00:05:02,065 --> 00:05:05,048
and if so, then, you know, keep deleting the redundant features

96
00:05:05,048 --> 00:05:07,065
until they are no longer redundant.

97
00:05:07,065 --> 00:05:09,079
And if your features are non redundant,

98
00:05:09,079 --> 00:05:11,093
I would check if I might have too many features,

99
00:05:11,093 --> 00:05:13,063
and if that's the case I would either

100
00:05:13,063 --> 00:05:16,014
delete some features if I can bare to use fewer features,

101
00:05:16,014 --> 00:05:20,070
or else I would consider using regularization,

102
00:05:20,070 --> 00:05:22,082
which is this topic that we will talk about later.

103
00:05:22,082 --> 00:05:27,087
So, that's it for the normal equation and what it means

104
00:05:27,087 --> 00:05:31,088
if the matrix X<u>transpose X is non-invertible.</u>

105
00:05:31,088 --> 00:05:35,070
But this is a problem that hopefully you run into pretty rarely.

106
00:05:35,070 --> 00:05:40,055
And if you just implement it in Octave using the pinv() function

107
00:05:40,055 --> 00:05:42,085
which is called the pseudo-inverse function

108
00:05:42,085 --> 00:05:46,069
so you use a different linear algebra library, that is called pseudo-inverse

109
00:05:46,069 --> 00:05:50,007
but that implementation should just do the right thing

110
00:05:50,007 --> 00:05:52,058
even if X<u>transpose X is non-invertible</u>

111
00:05:52,058 --> 00:05:55,019
which should happen pretty rarily anyway

112
00:05:55,019 --> 99:59:59,000
so this should not be a problem for most implementations of linear regression.
