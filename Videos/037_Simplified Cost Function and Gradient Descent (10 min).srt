
1
00:00:00,031 --> 00:00:02,000
In this video, we'll figure out

2
00:00:02,029 --> 00:00:03,075
a slightly simpler way to

3
00:00:03,091 --> 00:00:06,016
write the cost function than we have been using so far.

4
00:00:06,051 --> 00:00:07,096
And we'll also figure out

5
00:00:08,021 --> 00:00:10,060
how to apply gradient descent to fit

6
00:00:10,076 --> 00:00:12,023
the parameters of logistic regression.

7
00:00:13,031 --> 00:00:14,000
So by the end of this

8
00:00:14,015 --> 00:00:15,041
video you know how to

9
00:00:15,051 --> 00:00:18,033
implement a fully working version of logistic regression.

10
00:00:18,080 --> 00:00:24,030
Here's our cost function for logistic regression.

11
00:00:24,078 --> 00:00:26,069
Our overall cost function

12
00:00:27,058 --> 00:00:29,017
is 1 over M times sum

13
00:00:29,046 --> 00:00:30,085
of the training set of the

14
00:00:30,094 --> 00:00:32,049
cost of making different

15
00:00:32,078 --> 00:00:33,097
predictions on the different examples

16
00:00:34,057 --> 00:00:36,027
of labels Y I. And

17
00:00:36,039 --> 00:00:38,079
this is a cost for a single example that we worked out earlier.

18
00:00:39,046 --> 00:00:40,042
And I just want to remind

19
00:00:40,057 --> 00:00:42,053
you that for classification problems

20
00:00:43,042 --> 00:00:45,003
in our training and in fact

21
00:00:45,077 --> 00:00:47,000
even for examples known in

22
00:00:47,004 --> 00:00:48,050
our training set, Y is

23
00:00:48,082 --> 00:00:50,060
always equal to 0 or 1.

24
00:00:50,081 --> 00:00:50,081
Right?

25
00:00:51,000 --> 00:00:51,095
That's sort of part of the

26
00:00:52,010 --> 00:00:53,067
mathematical definition of Y.

27
00:00:55,071 --> 00:00:57,022
Because Y is either 0 or 1.

28
00:00:57,039 --> 00:00:59,013
We'll be able to

29
00:00:59,046 --> 00:01:00,032
come up with a simpler

30
00:01:00,075 --> 00:01:02,034
way to write this cost function.

31
00:01:02,096 --> 00:01:04,054
And in particular, rather than writing

32
00:01:04,092 --> 00:01:06,020
out this cost function on two

33
00:01:06,040 --> 00:01:07,051
separate lines with two separate

34
00:01:07,090 --> 00:01:09,015
cases for Y equals 1 and Y equals

35
00:01:09,048 --> 00:01:10,096
0, I am going to show

36
00:01:11,012 --> 00:01:12,037
you a way take these

37
00:01:12,060 --> 00:01:15,018
two lines and compress them into one equation.

38
00:01:16,021 --> 00:01:17,001
And this will make it more convenient

39
00:01:17,073 --> 00:01:18,062
to write out the cost function

40
00:01:19,021 --> 00:01:20,017
and derive gradient descent.

41
00:01:21,048 --> 00:01:24,000
Concretely, we can write out the cost function as follows.

42
00:01:24,048 --> 00:01:26,009
We'll say the cost of H

43
00:01:27,026 --> 00:01:29,009
of X comma Y. I'm going

44
00:01:29,023 --> 00:01:31,075
to write this as minus Y

45
00:01:31,076 --> 00:01:34,014
times log H of

46
00:01:34,020 --> 00:01:37,073
X minus 1

47
00:01:38,006 --> 00:01:41,000
minus Y times log 1

48
00:01:41,065 --> 00:01:43,043
minus H of X.

49
00:01:44,067 --> 00:01:45,043
And I'll show you in a

50
00:01:45,073 --> 00:01:47,096
second that this expression, or

51
00:01:48,006 --> 00:01:50,029
this equation is an equivalent

52
00:01:51,001 --> 00:01:52,017
way or more compact way

53
00:01:52,034 --> 00:01:53,057
of writing out this definition

54
00:01:54,018 --> 00:01:55,043
of the cost function that we had up here.

55
00:01:56,031 --> 00:01:57,012
Let's see why that's the case.

56
00:02:03,073 --> 00:02:05,056
We know that there are only 2 possible cases.

57
00:02:06,014 --> 00:02:07,001
Y must be 0 or 1.

58
00:02:07,023 --> 00:02:10,015
So let's suppose Y equals 1.

59
00:02:10,084 --> 00:02:11,084
If Y is equal

60
00:02:12,047 --> 00:02:14,012
to 1 then this equation

61
00:02:14,078 --> 00:02:15,065
is saying that the cost

62
00:02:19,002 --> 00:02:19,047
is equal to.

63
00:02:20,015 --> 00:02:23,008
Well if Y is equal to one, then this thing here is equal to one.

64
00:02:23,090 --> 00:02:26,037
And one minus Y is going to be equal to zero, right?

65
00:02:26,059 --> 00:02:27,056
So if Y is equal

66
00:02:27,086 --> 00:02:29,000
to one, then one minus Y

67
00:02:29,037 --> 00:02:31,009
is one minus one, which is therefore zero.

68
00:02:32,033 --> 00:02:33,062
So the second term gets multiplied

69
00:02:34,006 --> 00:02:35,013
by zero and goes away,

70
00:02:36,000 --> 00:02:37,015
and we're left with only this

71
00:02:37,041 --> 00:02:38,037
first term which is Y

72
00:02:38,065 --> 00:02:40,027
times log, minus Y times

73
00:02:40,059 --> 00:02:41,099
log H of X. Y is

74
00:02:42,015 --> 00:02:43,036
1 so that's equal to minus

75
00:02:43,062 --> 00:02:45,038
log H of X.

76
00:02:46,031 --> 00:02:48,009
And this equation is

77
00:02:48,024 --> 00:02:49,081
exactly what we have

78
00:02:50,006 --> 00:02:52,000
up here for if Y is equal to one.

79
00:02:53,027 --> 00:02:54,068
The other case is if

80
00:02:55,053 --> 00:02:56,046
Y is equal to 0.

81
00:02:57,028 --> 00:02:58,058
And if that is

82
00:02:58,071 --> 00:03:01,043
the case then, writing of

83
00:03:01,050 --> 00:03:02,080
the cost function is saying that

84
00:03:03,059 --> 00:03:04,090
if Y is equal to zero,

85
00:03:05,047 --> 00:03:07,052
then this term here, will be equal to zero.

86
00:03:08,037 --> 00:03:09,084
Whereas 1 minus Y, if

87
00:03:10,002 --> 00:03:11,006
Y equals zero, would be

88
00:03:11,028 --> 00:03:12,018
equal to 1, because 1 minus

89
00:03:12,053 --> 00:03:14,028
Y becomes 1 minus 0,

90
00:03:14,053 --> 00:03:16,041
which is just equal to 1.

91
00:03:16,062 --> 00:03:17,071
And so the cost function

92
00:03:18,061 --> 00:03:21,025
simplifies to just this last term here.

93
00:03:22,012 --> 00:03:22,012
Right?

94
00:03:22,056 --> 00:03:23,093
Because the first term

95
00:03:24,068 --> 00:03:27,016
over here gets multiplied by zero, and so it disappears.

96
00:03:27,049 --> 00:03:28,050
So we're just left with this last

97
00:03:28,078 --> 00:03:29,081
term, which is minus

98
00:03:30,050 --> 00:03:32,052
log, 1 minus H of

99
00:03:32,059 --> 00:03:34,009
X. And you can

100
00:03:34,025 --> 00:03:35,068
verify that this term here

101
00:03:35,097 --> 00:03:39,025
is just exactly what we had for when Y is equal to 0.

102
00:03:40,044 --> 00:03:42,003
So this shows that this

103
00:03:42,025 --> 00:03:43,049
definition for the cost is

104
00:03:43,059 --> 00:03:45,030
just a more compact way of

105
00:03:45,040 --> 00:03:46,069
taking both of these expressions,

106
00:03:47,037 --> 00:03:48,065
the cases Y equals 1 and

107
00:03:48,072 --> 00:03:49,090
Y equals 0, and writing

108
00:03:50,024 --> 00:03:51,083
them in one, in a

109
00:03:52,003 --> 00:03:53,097
more convenient form with just one line.

110
00:03:54,059 --> 00:03:56,008
We can, therefore, write

111
00:03:56,037 --> 00:03:58,071
all of our cost function for logistic regression as follows.

112
00:03:59,096 --> 00:04:00,028
It is this

113
00:04:00,056 --> 00:04:01,046
one of m of the sum

114
00:04:01,074 --> 00:04:03,040
of this cost functions, and plugging

115
00:04:03,084 --> 00:04:04,096
in the definition for the

116
00:04:05,005 --> 00:04:07,003
cost that we worked out earlier, we end up with this.

117
00:04:07,025 --> 00:04:08,046
And we just brought the minus sign outside.

118
00:04:09,075 --> 00:04:11,062
And why do we choose this particular cost function?

119
00:04:12,022 --> 00:04:15,002
When it looks like there could be other cost functions that we could have chosen.

120
00:04:16,022 --> 00:04:17,031
Although I won't have time to

121
00:04:17,043 --> 00:04:19,011
go into great detail of this

122
00:04:19,014 --> 00:04:20,052
in this course, this cost function

123
00:04:21,031 --> 00:04:23,012
can be derived from statistics using

124
00:04:23,051 --> 00:04:24,093
the principle of maximum likelihood

125
00:04:25,043 --> 00:04:26,074
estimation, which is an

126
00:04:26,081 --> 00:04:28,056
idea statistics for how

127
00:04:28,076 --> 00:04:31,082
to efficiently find parameters data for different models.

128
00:04:32,098 --> 00:04:35,008
And it also has a nice property that it is convex.

129
00:04:35,086 --> 00:04:36,079
So this is the cost function

130
00:04:37,063 --> 00:04:39,025
that, you know, essentially everyone uses

131
00:04:40,004 --> 00:04:41,045
when putting Logistic Regression models.

132
00:04:42,074 --> 00:04:43,087
If we don't understand the terms

133
00:04:44,022 --> 00:04:45,047
I just say and you don't

134
00:04:45,073 --> 00:04:47,005
know what the principle of maximum

135
00:04:47,025 --> 00:04:49,006
likelihood estimation is, don't worry about.

136
00:04:49,063 --> 00:04:51,004
There's just a deeper

137
00:04:51,025 --> 00:04:53,057
rational and justification behind this

138
00:04:53,079 --> 00:04:55,031
particular cost function then I

139
00:04:55,062 --> 00:04:57,005
have time to go into in this class.

140
00:04:58,018 --> 00:05:00,052
Given this cost function, in

141
00:05:00,066 --> 00:05:01,054
order to fit the parameters,

142
00:05:02,056 --> 00:05:03,068
what we're going to do then is

143
00:05:04,052 --> 00:05:07,042
try to find the parameters theta that minimizes J of theta.

144
00:05:07,091 --> 00:05:10,047
So if we, you know, try to minimize this.

145
00:05:10,070 --> 00:05:13,032
This would give us some set of parameters theta.

146
00:05:15,061 --> 00:05:16,085
Finally, if we're given a new

147
00:05:17,014 --> 00:05:18,037
example with some set

148
00:05:18,054 --> 00:05:19,098
of features X. We can

149
00:05:20,012 --> 00:05:21,043
then take the thetas that we

150
00:05:21,058 --> 00:05:23,035
fit our training set and output

151
00:05:23,094 --> 00:05:25,063
our prediction as this, and

152
00:05:25,080 --> 00:05:26,088
just to remind you the output

153
00:05:27,030 --> 00:05:28,075
of my hypothesis, I am

154
00:05:28,085 --> 00:05:30,012
going to interpret as the

155
00:05:30,022 --> 00:05:32,074
probability that Y is equal to 1.

156
00:05:32,094 --> 00:05:34,061
And this is given the

157
00:05:34,067 --> 00:05:36,030
implement X and parameters by theta.

158
00:05:36,089 --> 00:05:37,083
But think of this

159
00:05:38,006 --> 00:05:40,029
as just my hypothesis is

160
00:05:40,057 --> 00:05:43,012
estimating the probability that Y is equal to 1.

161
00:05:43,087 --> 00:05:45,041
So all that remains to

162
00:05:45,058 --> 00:05:46,079
be done is figure out

163
00:05:47,014 --> 00:05:49,029
how to actually minimize J

164
00:05:49,051 --> 00:05:50,079
of theta as a function

165
00:05:51,000 --> 00:05:51,094
of theta so we can actually

166
00:05:52,045 --> 00:05:53,098
fit the parameters to our training set.

167
00:05:56,038 --> 00:05:57,064
The way we're going to minimize the

168
00:05:57,073 --> 00:05:59,062
cost function is using gradient descent.

169
00:06:00,060 --> 00:06:01,020
Here's our cost function.

170
00:06:02,025 --> 00:06:04,060
And if we want to minimize it as a function of theta.

171
00:06:05,033 --> 00:06:07,026
Here's our usual template for gradient descent.

172
00:06:08,000 --> 00:06:09,064
Where we repeatedly update each

173
00:06:09,086 --> 00:06:11,086
parameter by taking updating

174
00:06:12,035 --> 00:06:13,093
it as itself minus a

175
00:06:14,006 --> 00:06:16,069
learning rate alpha times this derivative term.

176
00:06:17,063 --> 00:06:19,005
If you know some calculus feel

177
00:06:19,018 --> 00:06:20,056
free to take this term and

178
00:06:20,072 --> 00:06:22,005
try to compute a derivative yourself

179
00:06:22,074 --> 00:06:23,089
and see if you can simplify

180
00:06:24,056 --> 00:06:25,087
it to the same answer that I get.

181
00:06:26,066 --> 00:06:29,039
But even if you don't know calculus don't worry about it.

182
00:06:30,052 --> 00:06:31,093
If you actually compute this,

183
00:06:32,037 --> 00:06:34,014
what you get is this equation.

184
00:06:34,077 --> 00:06:37,025
And just write it out here.

185
00:06:37,057 --> 00:06:38,093
The sum from I equals 1

186
00:06:39,002 --> 00:06:41,013
through M of the,

187
00:06:41,036 --> 00:06:42,082
essentially the error, times

188
00:06:43,068 --> 00:06:46,024
X I J. So if

189
00:06:46,038 --> 00:06:47,092
you take this partial derivative

190
00:06:48,048 --> 00:06:49,047
term and plug it back

191
00:06:49,069 --> 00:06:50,098
in here, we can then

192
00:06:51,023 --> 00:06:53,048
write out our gradient descent algorithm as follows.

193
00:06:55,019 --> 00:06:56,026
And all I've done is I

194
00:06:56,037 --> 00:06:57,048
took the derivative term from

195
00:06:57,062 --> 00:06:59,004
the previous line and plugged it in there.

196
00:07:00,017 --> 00:07:01,026
So if you have N

197
00:07:01,044 --> 00:07:03,020
features, you would have, you know, a

198
00:07:03,083 --> 00:07:06,025
parameter vector theta, which parameters

199
00:07:06,083 --> 00:07:08,011
theta zero, theta one, theta

200
00:07:08,041 --> 00:07:09,056
two, down to theta

201
00:07:10,001 --> 00:07:11,013
N and you will

202
00:07:11,033 --> 00:07:12,093
use this update to simultaneously

203
00:07:13,087 --> 00:07:15,012
update all of your values of theta.

204
00:07:15,094 --> 00:07:17,004
Now if you take this

205
00:07:17,036 --> 00:07:19,016
update rule and compare it

206
00:07:19,039 --> 00:07:20,056
to what we were doing

207
00:07:21,018 --> 00:07:23,018
for linear regression, you might

208
00:07:23,037 --> 00:07:25,031
be surprised to realize that,

209
00:07:25,070 --> 00:07:28,022
well, this equation was exactly

210
00:07:28,097 --> 00:07:29,095
what we had for linear regression.

211
00:07:30,055 --> 00:07:31,054
In fact, if you look

212
00:07:31,060 --> 00:07:33,005
at the earlier videos and look

213
00:07:33,024 --> 00:07:34,097
at the update rule, the

214
00:07:35,008 --> 00:07:36,022
gradient descent rule for linear

215
00:07:36,055 --> 00:07:37,068
regression, it looked exactly

216
00:07:38,037 --> 00:07:40,012
like what I drew here inside the blue box.

217
00:07:41,022 --> 00:07:43,007
So are linear regression and

218
00:07:43,023 --> 00:07:44,062
logistic regression different algorithms or not?

219
00:07:45,089 --> 00:07:47,017
Well, this is resolved by

220
00:07:47,037 --> 00:07:49,012
observing that for logistic

221
00:07:49,050 --> 00:07:50,089
regression, what has changed

222
00:07:51,037 --> 00:07:54,037
is that the definition for this hypothesis has changed.

223
00:07:54,072 --> 00:07:55,085
So whereas for linear regression

224
00:07:56,080 --> 00:07:58,031
we had H of X equals

225
00:07:58,062 --> 00:08:00,093
theta transpose X, now the

226
00:08:01,006 --> 00:08:02,048
definition of H of

227
00:08:02,060 --> 00:08:03,086
X has changed and is

228
00:08:04,002 --> 00:08:05,023
instead now 1 over 1

229
00:08:05,042 --> 00:08:07,038
plus e to the negative theta transpose X.

230
00:08:07,091 --> 00:08:09,005
So even though the update

231
00:08:09,033 --> 00:08:11,045
rule looks cosmetically identical, because

232
00:08:12,023 --> 00:08:13,018
the definition of the hypothesis

233
00:08:13,087 --> 00:08:15,018
has changed, this is actually

234
00:08:15,081 --> 00:08:18,023
not the same thing as gradient descent for linear regression.

235
00:08:19,042 --> 00:08:20,091
In an earlier video, when

236
00:08:21,008 --> 00:08:22,051
we were talking about gradient descent

237
00:08:22,089 --> 00:08:24,032
for linear regression, we had

238
00:08:24,050 --> 00:08:25,058
talked about how to monitor

239
00:08:26,016 --> 00:08:28,002
gradient descent to make sure that it is converging.

240
00:08:29,061 --> 00:08:30,091
I usually apply that same

241
00:08:31,041 --> 00:08:32,076
method to logistic regression too

242
00:08:33,029 --> 00:08:36,026
to monitor gradient descent to make sure it's conversion correctly.

243
00:08:37,022 --> 00:08:38,032
And hopefully you can figure

244
00:08:38,058 --> 00:08:39,083
out how to apply that technique

245
00:08:40,027 --> 00:08:41,039
to logistic regression yourself.

246
00:08:43,095 --> 00:08:46,045
When implementing logistic regression with

247
00:08:46,061 --> 00:08:47,086
gradient descent, we have

248
00:08:48,021 --> 00:08:49,062
all of these different parameter

249
00:08:50,037 --> 00:08:51,074
values, you know, theta

250
00:08:52,012 --> 00:08:54,094
0 down to theta N that we need to update using this expression.

251
00:08:56,005 --> 00:08:57,074
And one thing we could do is have a for loop.

252
00:08:58,075 --> 00:09:00,048
So for I equals 0 to

253
00:09:00,089 --> 00:09:03,051
N of i equals 1 to N plus 1.

254
00:09:03,062 --> 00:09:06,050
So update each of these parameter values in turn.

255
00:09:07,021 --> 00:09:08,029
But of course, rather than using

256
00:09:08,063 --> 00:09:10,025
a folder, ideally we would

257
00:09:10,060 --> 00:09:12,003
also use a vectorized implementation.

258
00:09:13,016 --> 00:09:14,058
And so that a vectorized

259
00:09:15,003 --> 00:09:16,073
implementation can update, you

260
00:09:16,085 --> 00:09:17,087
know, all of these N plus

261
00:09:18,029 --> 00:09:20,027
1 parameters all in one fell swoop.

262
00:09:21,009 --> 00:09:22,008
And to check your own

263
00:09:22,020 --> 00:09:23,025
understanding, you might see

264
00:09:23,069 --> 00:09:24,087
if you can figure out how

265
00:09:25,014 --> 00:09:26,080
to do the vectorized implementation

266
00:09:27,075 --> 00:09:28,059
of this algorithm as well.

267
00:09:31,002 --> 00:09:32,007
So now you know how

268
00:09:32,035 --> 00:09:34,050
to implement gradient descents for logistic aggression.

269
00:09:35,005 --> 00:09:36,025
There was one last idea

270
00:09:36,067 --> 00:09:40,002
that we had talked about earlier for which was feature scaling.

271
00:09:40,072 --> 00:09:42,047
We saw how feature scaling can

272
00:09:42,087 --> 00:09:45,040
help gradient descents converge faster for linear regression.

273
00:09:46,049 --> 00:09:47,091
The idea of feature scaling also

274
00:09:48,085 --> 00:09:50,041
applies to gradient descent for logistic regression.

275
00:09:51,073 --> 00:09:54,009
And if you have features that are on very different scales.

276
00:09:54,088 --> 00:09:56,035
Then applying feature scaling can also

277
00:09:56,079 --> 00:09:58,048
make it, gradient descent, run

278
00:09:58,088 --> 00:10:00,012
faster for logistic regression.

279
00:10:01,052 --> 00:10:02,029
So, that's it.

280
00:10:02,062 --> 00:10:03,086
You now know how to implement

281
00:10:04,050 --> 00:10:06,017
logistic regression, and this

282
00:10:06,053 --> 00:10:08,058
is a very powerful and

283
00:10:08,086 --> 00:10:09,098
probably even most widely used

284
00:10:10,036 --> 00:10:11,049
classification algorithm in the world.

285
00:10:11,098 --> 00:10:13,073
And you now know how we get to work with yourself.
