
1
00:00:00,030 --> 00:00:01,068
In the last video, we talked

2
00:00:01,099 --> 00:00:03,092
about gradient descent for minimizing

3
00:00:04,044 --> 00:00:06,070
the cost function J of theta for logistic regression.

4
00:00:07,079 --> 00:00:08,092
In this video, I'd like to

5
00:00:09,001 --> 00:00:10,025
tell you about some advanced

6
00:00:10,084 --> 00:00:12,033
optimization algorithms and some

7
00:00:12,067 --> 00:00:14,006
advanced optimization concepts.

8
00:00:15,017 --> 00:00:16,048
Using some of these ideas, we'll

9
00:00:16,062 --> 00:00:17,092
be able to get logistic regression

10
00:00:19,001 --> 00:00:20,021
to run much more quickly than

11
00:00:20,035 --> 00:00:21,096
it's possible with gradient descent.

12
00:00:22,087 --> 00:00:24,019
And this will also let

13
00:00:24,032 --> 00:00:26,005
the algorithms scale much better

14
00:00:26,067 --> 00:00:28,003
to very large machine learning problems,

15
00:00:28,066 --> 00:00:30,094
such as if we had a very large number of features.

16
00:00:31,085 --> 00:00:33,035
Here's an alternative view of

17
00:00:33,075 --> 00:00:34,090
what gradient descent is doing.

18
00:00:35,059 --> 00:00:38,003
We have some cost function J and we want to minimize it.

19
00:00:38,095 --> 00:00:39,097
So what we need to

20
00:00:40,034 --> 00:00:41,007
is, we need to write

21
00:00:41,032 --> 00:00:42,064
code that can take

22
00:00:42,085 --> 00:00:44,097
as input the parameters theta and

23
00:00:45,020 --> 00:00:46,046
they can compute two things: J

24
00:00:46,070 --> 00:00:48,018
of theta and these partial

25
00:00:48,061 --> 00:00:50,028
derivative terms for, you

26
00:00:50,053 --> 00:00:51,082
know, J equals 0, 1

27
00:00:51,089 --> 00:00:53,070
up to N.  Given code that

28
00:00:53,082 --> 00:00:54,097
can do these two things, what

29
00:00:55,015 --> 00:00:56,071
gradient descent does is it

30
00:00:56,078 --> 00:00:58,061
repeatedly performs the following update.

31
00:00:59,010 --> 00:00:59,010
Right?

32
00:00:59,028 --> 00:01:00,050
So given the code that

33
00:01:00,067 --> 00:01:01,075
we wrote to compute these partial

34
00:01:02,009 --> 00:01:03,079
derivatives, gradient descent plugs

35
00:01:04,048 --> 00:01:07,032
in here and uses that to update our parameters theta.

36
00:01:08,065 --> 00:01:09,059
So another way of thinking

37
00:01:09,090 --> 00:01:11,006
about gradient descent is that

38
00:01:11,034 --> 00:01:12,067
we need to supply code to

39
00:01:12,081 --> 00:01:14,004
compute J of theta and

40
00:01:14,023 --> 00:01:15,070
these derivatives, and then

41
00:01:15,090 --> 00:01:16,093
these get plugged into gradient

42
00:01:17,037 --> 00:01:20,010
descents, which can then try to minimize the function for us.

43
00:01:20,096 --> 00:01:21,096
For gradient descent, I guess

44
00:01:22,048 --> 00:01:23,079
technically you don't actually need code

45
00:01:24,017 --> 00:01:26,051
to compute the cost function J of theta.

46
00:01:26,093 --> 00:01:28,098
You only need code to compute the derivative terms.

47
00:01:29,073 --> 00:01:30,048
But if you think of your

48
00:01:30,059 --> 00:01:32,029
code as also monitoring convergence

49
00:01:33,000 --> 00:01:34,006
of some such,

50
00:01:34,018 --> 00:01:35,043
we'll just think of

51
00:01:35,053 --> 00:01:37,037
ourselves as providing code to

52
00:01:37,051 --> 00:01:38,053
compute both the cost

53
00:01:38,089 --> 00:01:40,025
function and the derivative terms.

54
00:01:42,070 --> 00:01:44,012
So, having written code to

55
00:01:44,028 --> 00:01:45,085
compute these two things, one

56
00:01:46,009 --> 00:01:47,081
algorithm we can use is gradient descent.

57
00:01:48,090 --> 00:01:51,059
But gradient descent isn't the only algorithm we can use.

58
00:01:52,028 --> 00:01:53,068
And there are other algorithms,

59
00:01:54,032 --> 00:01:55,093
more advanced, more sophisticated ones,

60
00:01:56,071 --> 00:01:57,087
that, if we only provide

61
00:01:58,040 --> 00:01:59,051
them a way to compute

62
00:01:59,095 --> 00:02:01,054
these two things, then these

63
00:02:01,076 --> 00:02:03,004
are different approaches to optimize

64
00:02:03,048 --> 00:02:04,079
the cost function for us.

65
00:02:05,010 --> 00:02:07,090
So conjugate gradient BFGS and

66
00:02:08,011 --> 00:02:09,024
L-BFGS are examples of more

67
00:02:09,046 --> 00:02:11,049
sophisticated optimization algorithms that

68
00:02:11,063 --> 00:02:12,061
need a way to compute J

69
00:02:12,081 --> 00:02:13,066
of theta, and need a way

70
00:02:13,075 --> 00:02:15,043
to compute the derivatives, and can

71
00:02:15,066 --> 00:02:16,093
then use more sophisticated

72
00:02:17,062 --> 00:02:19,087
strategies than gradient descent to minimize the cost function.

73
00:02:21,025 --> 00:02:22,056
The details of exactly what

74
00:02:22,078 --> 00:02:25,091
these three algorithms is well beyond the scope of this course.

75
00:02:26,049 --> 00:02:28,019
And in fact you often

76
00:02:28,065 --> 00:02:30,056
end up spending, you know, many days,

77
00:02:31,006 --> 00:02:32,066
or a small number of weeks studying these algorithms.

78
00:02:33,024 --> 00:02:35,084
If you take a class and advance the numerical computing.

79
00:02:36,091 --> 00:02:38,019
But let me just tell you about some of their properties.

80
00:02:40,008 --> 00:02:42,015
These three algorithms have a number of advantages.

81
00:02:42,090 --> 00:02:44,006
One is that, with any

82
00:02:44,028 --> 00:02:45,084
of this algorithms you usually do

83
00:02:46,000 --> 00:02:48,096
not need to manually pick the learning rate alpha.

84
00:02:50,066 --> 00:02:51,044
So one way to think

85
00:02:51,065 --> 00:02:53,062
of these algorithms is that given

86
00:02:54,022 --> 00:02:56,090
is the way to compute the derivative and a cost function.

87
00:02:57,031 --> 00:02:59,074
You can think of these algorithms as having a clever inter-loop.

88
00:03:00,006 --> 00:03:00,068
And, in fact, they have a clever

89
00:03:01,081 --> 00:03:03,078
inter-loop called a line

90
00:03:04,019 --> 00:03:05,084
search algorithm that automatically

91
00:03:06,052 --> 00:03:08,000
tries out different values for

92
00:03:08,008 --> 00:03:09,036
the learning rate alpha and automatically

93
00:03:10,000 --> 00:03:11,009
picks a good learning rate alpha

94
00:03:12,003 --> 00:03:12,090
so that it can even pick

95
00:03:13,012 --> 00:03:14,056
a different learning rate for every iteration.

96
00:03:15,049 --> 00:03:18,022
And so then you don't need to choose it yourself.

97
00:03:21,043 --> 00:03:22,077
These algorithms actually do

98
00:03:22,090 --> 00:03:24,025
more sophisticated things than just

99
00:03:24,046 --> 00:03:25,063
pick a good learning rate, and

100
00:03:25,080 --> 00:03:27,030
so they often end up

101
00:03:27,049 --> 00:03:30,031
converging much faster than gradient descent.

102
00:03:32,046 --> 00:03:33,074
These algorithms actually do more

103
00:03:33,097 --> 00:03:35,015
sophisticated things than just

104
00:03:35,036 --> 00:03:36,074
pick a good learning rate, and

105
00:03:36,087 --> 00:03:38,077
so they often end up converging much

106
00:03:39,002 --> 00:03:40,084
faster than gradient descent, but

107
00:03:41,003 --> 00:03:42,022
detailed discussion of exactly

108
00:03:42,071 --> 00:03:44,041
what they do is beyond the scope of this course.

109
00:03:45,058 --> 00:03:47,006
In fact, I actually used

110
00:03:47,056 --> 00:03:49,002
to have used these algorithms for

111
00:03:49,016 --> 00:03:50,016
a long time, like maybe over

112
00:03:50,046 --> 00:03:53,006
a decade, quite frequently, and it

113
00:03:53,028 --> 00:03:54,040
was only, you know, a

114
00:03:54,050 --> 00:03:55,046
few years ago that I actually

115
00:03:56,015 --> 00:03:57,019
figured out for myself the details

116
00:03:57,078 --> 00:04:00,021
of what conjugate gradient, BFGS and O-BFGS do.

117
00:04:00,097 --> 00:04:02,074
So it is actually entirely possible

118
00:04:03,056 --> 00:04:05,037
to use these algorithms successfully and

119
00:04:05,047 --> 00:04:06,053
apply to lots of different learning

120
00:04:06,078 --> 00:04:08,049
problems without actually understanding

121
00:04:09,046 --> 00:04:11,013
the inter-loop of what these algorithms do.

122
00:04:12,027 --> 00:04:13,062
If these algorithms have a disadvantage,

123
00:04:14,019 --> 00:04:15,034
I'd say that the main

124
00:04:15,061 --> 00:04:16,097
disadvantage is that they're

125
00:04:17,011 --> 00:04:19,038
quite a lot more complex than gradient descent.

126
00:04:20,018 --> 00:04:21,069
And in particular, you probably should

127
00:04:21,097 --> 00:04:23,029
not implement these algorithms

128
00:04:23,085 --> 00:04:26,006
- conjugate gradient, L-BGFS, BFGS -

129
00:04:26,036 --> 00:04:29,051
yourself unless you're an expert in numerical computing.

130
00:04:30,072 --> 00:04:32,031
Instead, just as I

131
00:04:32,042 --> 00:04:33,063
wouldn't recommend that you write

132
00:04:33,085 --> 00:04:35,024
your own code to compute square

133
00:04:35,058 --> 00:04:36,066
roots of numbers or to

134
00:04:36,076 --> 00:04:39,000
compute inverses of matrices, for

135
00:04:39,013 --> 00:04:40,060
these algorithms also what I

136
00:04:40,070 --> 00:04:42,052
would recommend you do is just use a software library.

137
00:04:43,002 --> 00:04:43,076
So, you know, to take a square

138
00:04:44,012 --> 00:04:44,093
root what all of us

139
00:04:45,014 --> 00:04:46,043
do is use some function

140
00:04:47,007 --> 00:04:48,031
that someone else has

141
00:04:48,052 --> 00:04:50,019
written to compute the square roots of our numbers.

142
00:04:51,032 --> 00:04:53,052
And fortunately, Octave and

143
00:04:53,075 --> 00:04:55,006
the closely related language MATLAB

144
00:04:55,043 --> 00:04:57,011
- we'll be using that -

145
00:04:57,013 --> 00:04:58,037
Octave has a very good. Has a pretty

146
00:04:58,052 --> 00:05:02,041
reasonable library implementing some of these advanced optimization algorithms.

147
00:05:03,037 --> 00:05:04,035
And so if you just use

148
00:05:04,060 --> 00:05:06,080
the built-in library, you know, you get pretty good results.

149
00:05:08,000 --> 00:05:08,087
I should say that there is

150
00:05:09,037 --> 00:05:10,087
a difference between good

151
00:05:11,023 --> 00:05:12,074
and bad implementations of these algorithms.

152
00:05:13,068 --> 00:05:15,000
And so, if you're using a

153
00:05:15,012 --> 00:05:16,026
different language for your machine

154
00:05:16,047 --> 00:05:17,056
learning application, if you're using

155
00:05:18,018 --> 00:05:20,008
C, C++, Java, and

156
00:05:20,025 --> 00:05:24,006
so on, you

157
00:05:24,020 --> 00:05:24,070
might want to try out a couple

158
00:05:24,073 --> 00:05:25,066
of different libraries to make sure that you find a

159
00:05:25,074 --> 00:05:27,079
good library for implementing these algorithms.

160
00:05:28,025 --> 00:05:29,041
Because there is a difference in

161
00:05:29,048 --> 00:05:30,074
performance between a good implementation

162
00:05:31,068 --> 00:05:33,014
of, you know, contour gradient or

163
00:05:33,052 --> 00:05:35,014
LPFGS versus less good

164
00:05:35,035 --> 00:05:37,068
implementation of contour gradient or LPFGS.

165
00:05:43,006 --> 00:05:44,031
So now let's explain how

166
00:05:44,057 --> 00:05:47,007
to use these algorithms, I'm going to do so with an example.

167
00:05:48,097 --> 00:05:50,022
Let's say that you have a

168
00:05:50,037 --> 00:05:51,062
problem with two parameters

169
00:05:53,037 --> 00:05:55,057
equals theta zero and theta one.

170
00:05:56,041 --> 00:05:57,044
And let's say your cost function

171
00:05:57,097 --> 00:05:59,020
is J of theta equals theta

172
00:05:59,043 --> 00:06:01,054
one minus five squared, plus theta two minus five squared.

173
00:06:02,062 --> 00:06:04,007
So with this cost function.

174
00:06:04,058 --> 00:06:06,095
You know the value for theta 1 and theta 2.

175
00:06:07,007 --> 00:06:09,058
If you want to minimize J of theta as a function of theta.

176
00:06:09,093 --> 00:06:10,091
The value that minimizes it is

177
00:06:11,002 --> 00:06:12,004
going to be theta 1

178
00:06:12,042 --> 00:06:14,022
equals 5, theta 2 equals equals five.

179
00:06:15,023 --> 00:06:16,062
Now, again, I know some of

180
00:06:16,094 --> 00:06:18,031
you know more calculus than others,

181
00:06:19,000 --> 00:06:20,076
but the derivatives of the

182
00:06:20,085 --> 00:06:23,042
cost function J turn out to be these two expressions.

183
00:06:24,026 --> 00:06:25,006
I've done the calculus.

184
00:06:26,025 --> 00:06:27,025
So if you want to apply

185
00:06:27,048 --> 00:06:29,022
one of the advanced optimization algorithms

186
00:06:29,081 --> 00:06:31,037
to minimize cost function J.

187
00:06:31,066 --> 00:06:32,062
So, you know, if we

188
00:06:32,087 --> 00:06:34,068
didn't know the minimum was at

189
00:06:34,077 --> 00:06:36,013
5, 5, but if you want to have

190
00:06:36,024 --> 00:06:37,055
a cost function 5 the minimum

191
00:06:37,097 --> 00:06:39,083
numerically using something like

192
00:06:40,004 --> 00:06:41,056
gradient descent but preferably more

193
00:06:41,073 --> 00:06:43,043
advanced than gradient descent, what

194
00:06:43,055 --> 00:06:45,000
you would do is implement an octave

195
00:06:45,056 --> 00:06:46,068
function like this, so we

196
00:06:46,086 --> 00:06:48,018
implement a cost function,

197
00:06:49,020 --> 00:06:51,018
cost function theta function like that,

198
00:06:52,018 --> 00:06:53,025
and what this does is that

199
00:06:53,037 --> 00:06:55,066
it returns two arguments, the

200
00:06:55,075 --> 00:06:57,077
first J-val, is how

201
00:06:58,091 --> 00:07:00,001
we would compute the cost function

202
00:07:00,068 --> 00:07:01,077
J. And so this says J-val

203
00:07:02,007 --> 00:07:03,020
equals, you know, theta

204
00:07:03,043 --> 00:07:04,062
one minus five squared plus theta

205
00:07:05,032 --> 00:07:06,023
two minus five squared.

206
00:07:06,054 --> 00:07:09,013
So it's just computing this cost function over here.

207
00:07:10,054 --> 00:07:12,004
And the second argument that

208
00:07:12,025 --> 00:07:14,018
this function returns is gradient.

209
00:07:14,083 --> 00:07:16,002
So gradient is going to

210
00:07:16,016 --> 00:07:17,031
be a two by one vector,

211
00:07:18,087 --> 00:07:20,005
and the two elements of the

212
00:07:20,012 --> 00:07:22,010
gradient vector correspond to

213
00:07:22,080 --> 00:07:24,067
the two partial derivative terms over here.

214
00:07:27,014 --> 00:07:28,056
Having implemented this cost function,

215
00:07:29,057 --> 00:07:30,038
you would, you can then

216
00:07:31,050 --> 00:07:33,000
call the advanced optimization

217
00:07:34,026 --> 00:07:35,072
function called the fminunc

218
00:07:35,094 --> 00:07:36,089
- it stands for function

219
00:07:37,061 --> 00:07:39,036
minimization unconstrained in Octave

220
00:07:40,030 --> 00:07:41,051
-and the way you call this is as follows.

221
00:07:41,079 --> 00:07:42,035
You set a few options.

222
00:07:43,023 --> 00:07:43,057
This is a options

223
00:07:44,032 --> 00:07:43,057


224
00:07:44,032 --> 00:07:46,068
as a data structure that stores the options you want.

225
00:07:47,031 --> 00:07:48,095
So grant up on,

226
00:07:49,016 --> 00:07:52,010
this sets the gradient objective parameter to on.

227
00:07:52,026 --> 00:07:55,018
It just means you are indeed going to provide a gradient to this algorithm.

228
00:07:56,014 --> 00:07:57,055
I'm going to set the maximum number

229
00:07:57,083 --> 00:07:59,027
of iterations to, let's say, one hundred.

230
00:07:59,057 --> 00:08:02,023
We're going give it an initial guess for theta.

231
00:08:02,072 --> 00:08:03,068
There's a 2 by 1 vector.

232
00:08:04,043 --> 00:08:06,086
And then this command calls fminunc.

233
00:08:07,052 --> 00:08:10,029
This at symbol presents a

234
00:08:10,042 --> 00:08:11,081
pointer to the cost function

235
00:08:13,000 --> 00:08:14,031
that we just defined up there.

236
00:08:15,006 --> 00:08:16,001
And if you call this,

237
00:08:16,026 --> 00:08:18,029
this will compute, you know, will use

238
00:08:18,062 --> 00:08:20,049
one of the more advanced optimization algorithms.

239
00:08:21,011 --> 00:08:23,035
And if you want to think it as just like gradient descent.

240
00:08:23,068 --> 00:08:25,017
But automatically choosing the learning

241
00:08:25,050 --> 00:08:27,029
rate alpha for so you don't have to do so yourself.

242
00:08:28,020 --> 00:08:29,087
But it will then attempt to

243
00:08:30,016 --> 00:08:32,000
use the sort of advanced optimization algorithms.

244
00:08:32,063 --> 00:08:33,076
Like gradient descent on steroids.

245
00:08:34,039 --> 00:08:36,049
To try to find the optimal value of theta for you.

246
00:08:37,017 --> 00:08:39,003
Let me actually show you what this looks like in Octave.

247
00:08:40,069 --> 00:08:42,046
So I've written this cost function

248
00:08:42,089 --> 00:08:46,044
of theta function exactly as we had it on the previous line.

249
00:08:46,064 --> 00:08:49,007
It computes J-val which is the cost function.

250
00:08:49,091 --> 00:08:51,080
And it computes the gradient with

251
00:08:52,003 --> 00:08:53,004
the two elements being the partial

252
00:08:53,045 --> 00:08:54,042
derivatives of the cost function

253
00:08:55,022 --> 00:08:56,020
with respect to, you know,

254
00:08:56,036 --> 00:08:57,090
the two parameters, theta one and theta two.

255
00:08:59,003 --> 00:09:00,036
Now let's switch to my Octave window.

256
00:09:00,071 --> 00:09:02,089
I'm gonna type in those commands I had just now.

257
00:09:03,047 --> 00:09:05,085
So, options equals optimset. This is

258
00:09:06,062 --> 00:09:08,050
the notation for setting my

259
00:09:09,066 --> 00:09:11,019
parameters on my options,

260
00:09:11,071 --> 00:09:13,085
for my optimization algorithm. Grant option on, maxIter, 100

261
00:09:14,012 --> 00:09:17,060
so that says 100

262
00:09:18,030 --> 00:09:19,061
iterations, and I am

263
00:09:19,073 --> 00:09:22,009
going to provide the gradient to my algorithm.

264
00:09:23,049 --> 00:09:27,019
Let's say initial theta equals zero's two by one.

265
00:09:27,098 --> 00:09:29,027
So that's my initial guess for theta.

266
00:09:30,050 --> 00:09:31,038
And now I have of theta,

267
00:09:32,062 --> 00:09:35,010
function val exit flag

268
00:09:37,061 --> 00:09:39,042
equals fminunc constraint.

269
00:09:40,057 --> 00:09:41,060
A pointer to the cost function.

270
00:09:43,000 --> 00:09:44,070
and provide my initial guess.

271
00:09:46,009 --> 00:09:49,005
And the options like so.

272
00:09:49,082 --> 00:09:52,075
And if I hit enter this will run the optimization algorithm.

273
00:09:53,094 --> 00:09:54,080
And it returns pretty quickly.

274
00:09:55,078 --> 00:09:57,003
This funny formatting that's because

275
00:09:57,042 --> 00:09:58,042
my line, you know, my

276
00:09:59,070 --> 00:10:00,028
code wrapped around.

277
00:10:00,067 --> 00:10:02,053
So, this funny thing

278
00:10:02,075 --> 00:10:04,088
is just because my command line had wrapped around.

279
00:10:05,049 --> 00:10:06,028
But what this says is that

280
00:10:06,054 --> 00:10:08,050
numerically renders, you know, think

281
00:10:08,066 --> 00:10:10,039
of it as gradient descent

282
00:10:10,044 --> 00:10:11,062
on steroids, they found the optimal value of

283
00:10:11,075 --> 00:10:13,014
a theta is theta 1

284
00:10:13,039 --> 00:10:15,066
equals 5, theta 2 equals 5, exactly as we're hoping for.

285
00:10:16,051 --> 00:10:18,075
The function value at the

286
00:10:18,084 --> 00:10:21,042
optimum is essentially 10 to the minus 30.

287
00:10:21,066 --> 00:10:23,015
So that's essentially zero, which

288
00:10:23,037 --> 00:10:24,075
is also what we're hoping for.

289
00:10:24,084 --> 00:10:27,005
And the exit flag is

290
00:10:27,024 --> 00:10:29,008
1, and this shows

291
00:10:29,073 --> 00:10:31,039
what the convergence status of this.

292
00:10:31,079 --> 00:10:33,000
And if you want you can do

293
00:10:33,014 --> 00:10:35,001
help fminunc to

294
00:10:35,012 --> 00:10:36,048
read the documentation for how

295
00:10:36,067 --> 00:10:38,064
to interpret the exit flag.

296
00:10:38,075 --> 00:10:41,060
But the exit flag let's you verify whether or not this algorithm thing has converged.

297
00:10:43,096 --> 00:10:46,045
So that's how you run these algorithms in Octave.

298
00:10:47,048 --> 00:10:48,091
I should mention, by the way,

299
00:10:48,094 --> 00:10:51,001
that for the Octave implementation, this value

300
00:10:51,063 --> 00:10:53,000
of theta, your parameter vector

301
00:10:53,037 --> 00:10:54,094
of theta, must be in

302
00:10:55,027 --> 00:10:58,021
rd for d greater than or equal to 2.

303
00:10:58,045 --> 00:11:00,033
So if theta is just a real number.

304
00:11:00,076 --> 00:11:02,003
So, if it is not at least

305
00:11:02,015 --> 00:11:03,015
a two-dimensional vector

306
00:11:03,079 --> 00:11:04,086
or some higher than two-dimensional

307
00:11:05,015 --> 00:11:06,084
vector, this fminunc

308
00:11:07,055 --> 00:11:08,075
may not work, so and if

309
00:11:09,013 --> 00:11:10,030
in case you have a

310
00:11:10,059 --> 00:11:11,059
one-dimensional function that you use

311
00:11:11,083 --> 00:11:12,092
to optimize, you can look

312
00:11:13,010 --> 00:11:14,067
in the octave documentation for fminunc

313
00:11:14,095 --> 00:11:16,023
for additional details.

314
00:11:18,023 --> 00:11:19,036
So, that's how we optimize

315
00:11:19,062 --> 00:11:21,063
our trial example of this

316
00:11:22,019 --> 00:11:23,080
simple quick driving cost function.

317
00:11:24,044 --> 00:11:26,051
However, we apply this to let's just say progression.

318
00:11:27,072 --> 00:11:29,026
In logistic regression we have

319
00:11:29,051 --> 00:11:31,028
a parameter vector theta, and

320
00:11:31,042 --> 00:11:32,021
I'm going to use a mix

321
00:11:32,062 --> 00:11:34,087
of octave notation and sort of math notation.

322
00:11:35,029 --> 00:11:36,039
But I hope this explanation

323
00:11:36,087 --> 00:11:38,004
will be clear, but our parameter

324
00:11:38,051 --> 00:11:40,036
vector theta comprises these

325
00:11:40,053 --> 00:11:41,077
parameters theta 0 through theta

326
00:11:42,021 --> 00:11:44,023
n because octave indexes,

327
00:11:46,009 --> 00:11:48,003
vectors using indexing from

328
00:11:48,046 --> 00:11:49,063
1, you know, theta 0 is

329
00:11:49,071 --> 00:11:51,019
actually written theta 1

330
00:11:51,033 --> 00:11:53,028
in octave, theta 1 is gonna be written.

331
00:11:53,092 --> 00:11:54,069
So, if theta 2 in octave

332
00:11:55,027 --> 00:11:56,017
and that's gonna be a written

333
00:11:56,077 --> 00:11:58,042
theta n+1, right?

334
00:11:58,061 --> 00:12:00,064
And that's because Octave indexes

335
00:12:01,032 --> 00:12:03,007
is vectors starting from index

336
00:12:03,042 --> 00:12:05,020
of 1 and so the index of 0.

337
00:12:06,091 --> 00:12:07,095
So what we need

338
00:12:08,015 --> 00:12:09,066
to do then is write a

339
00:12:09,087 --> 00:12:12,007
cost function that captures

340
00:12:12,071 --> 00:12:14,021
the cost function for logistic regression.

341
00:12:15,016 --> 00:12:16,045
Concretely, the cost function

342
00:12:16,087 --> 00:12:18,030
needs to return J-val, which is, you know, J-val

343
00:12:18,094 --> 00:12:20,042
as you need some codes to

344
00:12:20,063 --> 00:12:22,044
compute J of theta and

345
00:12:22,071 --> 00:12:24,000
we also need to give it the gradient.

346
00:12:24,053 --> 00:12:25,046
So, gradient 1 is going

347
00:12:25,091 --> 00:12:27,008
to be some code to compute

348
00:12:27,027 --> 00:12:29,010
the partial derivative in respect to

349
00:12:29,038 --> 00:12:31,025
theta 0, the next partial

350
00:12:31,060 --> 00:12:34,029
derivative respect to theta 1 and so on.

351
00:12:34,076 --> 00:12:36,025
Once again, this is gradient

352
00:12:37,050 --> 00:12:38,038
1, gradient 2 and so

353
00:12:39,002 --> 00:12:40,033
on, rather than gradient 0, gradient

354
00:12:40,050 --> 00:12:42,073
1 because octave indexes

355
00:12:43,046 --> 00:12:46,020
is vectors starting from one rather than from zero.

356
00:12:47,044 --> 00:12:48,046
But the main concept I hope

357
00:12:48,069 --> 00:12:49,053
you take away from this slide

358
00:12:49,089 --> 00:12:50,087
is, that what you need to do,

359
00:12:51,007 --> 00:12:54,037
is write a function that returns

360
00:12:55,050 --> 00:12:56,092
the cost function and returns the gradient.

361
00:12:58,040 --> 00:12:59,075
And so in order to

362
00:12:59,096 --> 00:13:01,040
apply this to logistic regression

363
00:13:02,010 --> 00:13:03,042
or even to linear regression, if

364
00:13:03,055 --> 00:13:06,023
you want to use these optimization algorithms for linear regression.

365
00:13:07,034 --> 00:13:08,035
What you need to do is plug in

366
00:13:08,050 --> 00:13:09,096
the appropriate code to compute

367
00:13:10,082 --> 00:13:12,027
these things over here.

368
00:13:15,010 --> 00:13:17,090
So, now you know how to use these advanced optimization algorithms.

369
00:13:19,002 --> 00:13:21,016
Because, using, because for

370
00:13:21,032 --> 00:13:22,065
these algorithms, you're using a

371
00:13:22,087 --> 00:13:25,019
sophisticated optimization library, it makes

372
00:13:25,069 --> 00:13:26,071
the just a little bit

373
00:13:26,094 --> 00:13:28,050
more opaque and so

374
00:13:28,074 --> 00:13:30,038
just maybe a little bit harder to debug.

375
00:13:31,028 --> 00:13:32,065
But because these algorithms often

376
00:13:33,000 --> 00:13:34,037
run much faster than gradient descent,

377
00:13:35,000 --> 00:13:36,075
often quite typically whenever

378
00:13:37,005 --> 00:13:38,017
I have a large machine learning

379
00:13:38,040 --> 00:13:39,050
problem, I will use

380
00:13:39,075 --> 00:13:42,011
these algorithms instead of using gradient descent.

381
00:13:43,089 --> 00:13:45,007
And with these ideas, hopefully,

382
00:13:45,045 --> 00:13:46,071
you'll be able to get logistic progression

383
00:13:47,035 --> 00:13:48,077
and also linear regression to work

384
00:13:49,010 --> 00:13:51,040
on much larger problems.

385
00:13:51,083 --> 00:13:53,082
So, that's it for advanced optimization concepts.

386
00:13:55,012 --> 00:13:56,016
And in the next and

387
00:13:56,032 --> 00:13:57,072
final video on Logistic Regression,

388
00:13:58,054 --> 00:13:59,047
I want to tell you how to

389
00:13:59,060 --> 00:14:00,099
take the logistic regression algorithm

390
00:14:01,051 --> 00:14:02,078
that you already know about and make

391
00:14:02,099 --> 00:14:05,041
it work also on multi-class classification problems.
