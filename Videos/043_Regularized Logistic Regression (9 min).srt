
1
00:00:00,016 --> 00:00:01,048
For logistic regression, we previously

2
00:00:02,010 --> 00:00:04,073
talked about two types of optimization algorithms.

3
00:00:05,019 --> 00:00:06,019
We talked about how to use

4
00:00:06,055 --> 00:00:09,021
gradient descent to optimize as cost function J of theta.

5
00:00:09,068 --> 00:00:10,076
And we also talked about

6
00:00:11,011 --> 00:00:12,073
advanced optimization methods.

7
00:00:13,051 --> 00:00:14,067
Ones that require that you

8
00:00:14,078 --> 00:00:16,030
provide a way to compute

9
00:00:16,094 --> 00:00:18,016
your cost function J of

10
00:00:18,042 --> 00:00:20,092
theta and that you provide a way to compute the derivatives.

11
00:00:22,044 --> 00:00:23,092
In this video, we'll show how

12
00:00:24,019 --> 00:00:25,042
you can adapt both of

13
00:00:25,050 --> 00:00:27,057
those techniques, both gradient descent and

14
00:00:27,071 --> 00:00:29,035
the more advanced optimization techniques

15
00:00:30,028 --> 00:00:31,076
in order to have them

16
00:00:31,094 --> 00:00:33,054
work for regularized logistic regression.

17
00:00:35,042 --> 00:00:36,067
So, here's the idea.

18
00:00:37,025 --> 00:00:38,077
We saw earlier that Logistic

19
00:00:39,018 --> 00:00:40,049
Regression can also be prone

20
00:00:40,085 --> 00:00:42,053
to overfitting if you fit

21
00:00:42,081 --> 00:00:44,009
it with a very, sort of,

22
00:00:44,028 --> 00:00:45,089
high order polynomial features like this.

23
00:00:46,046 --> 00:00:48,025
Where G is the

24
00:00:48,047 --> 00:00:49,096
sigmoid function and in

25
00:00:50,003 --> 00:00:51,032
particular you end up with

26
00:00:51,053 --> 00:00:53,002
a hypothesis, you know,

27
00:00:53,014 --> 00:00:54,011
whose decision bound to be

28
00:00:54,035 --> 00:00:55,092
just sort of an overly complex

29
00:00:56,061 --> 00:00:58,060
and extremely contortive function that

30
00:00:58,082 --> 00:00:59,067
really isn't such a great

31
00:00:59,078 --> 00:01:01,000
hypothesis for this training

32
00:01:01,035 --> 00:01:02,099
set, and more generally if you have

33
00:01:03,011 --> 00:01:04,089
logistic regression with a lot of features.

34
00:01:05,015 --> 00:01:06,062
Not necessarily polynomial ones, but

35
00:01:06,079 --> 00:01:07,051
just with a lot of

36
00:01:07,067 --> 00:01:09,071
features you can end up with overfitting.

37
00:01:11,062 --> 00:01:14,001
This was our cost function for logistic regression.

38
00:01:14,081 --> 00:01:16,020
And if we want to modify

39
00:01:16,073 --> 00:01:18,081
it to use regularization, all we

40
00:01:18,095 --> 00:01:20,062
need to do is add to

41
00:01:20,081 --> 00:01:22,029
it the following term

42
00:01:22,065 --> 00:01:24,085
plus londer over 2M, sum

43
00:01:25,010 --> 00:01:26,057
from J equals 1, and

44
00:01:26,073 --> 00:01:29,067
as usual sum from J equals 1.

45
00:01:29,079 --> 00:01:31,000
Rather than the sum from J

46
00:01:31,054 --> 00:01:33,067
equals 0, of theta J squared.

47
00:01:34,032 --> 00:01:35,046
And this has to

48
00:01:35,075 --> 00:01:36,095
effect therefore, of penalizing

49
00:01:37,065 --> 00:01:39,014
the parameters theta 1 theta

50
00:01:39,056 --> 00:01:42,059
2 and so on up to theta N from being too large.

51
00:01:43,060 --> 00:01:44,071
And if you do this,

52
00:01:45,071 --> 00:01:46,045
then it will the have the

53
00:01:46,075 --> 00:01:48,087
effect that even though you're fitting

54
00:01:49,025 --> 00:01:51,050
a very high order polynomial with a lot of parameters.

55
00:01:52,020 --> 00:01:53,023
So long as you apply regularization

56
00:01:53,090 --> 00:01:55,009
and keep the parameters small

57
00:01:55,084 --> 00:01:57,057
you're more likely to get a decision boundary.

58
00:01:58,082 --> 00:02:00,004
You know, that maybe looks more like this.

59
00:02:00,031 --> 00:02:01,045
It looks more reasonable for separating

60
00:02:02,050 --> 00:02:03,073
the positive and the negative examples.

61
00:02:05,029 --> 00:02:06,096
So, when using regularization

62
00:02:08,013 --> 00:02:09,008
even when you have a lot

63
00:02:09,021 --> 00:02:11,011
of features, the regularization can

64
00:02:11,062 --> 00:02:13,050
help take care of the overfitting problem.

65
00:02:14,074 --> 00:02:15,078
How do we actually implement this?

66
00:02:16,071 --> 00:02:18,028
Well, for the original gradient descent

67
00:02:18,071 --> 00:02:20,037
algorithm, this was the update we had.

68
00:02:20,066 --> 00:02:22,030
We will repeatedly perform the following

69
00:02:22,075 --> 00:02:24,061
update to theta J. This

70
00:02:24,074 --> 00:02:26,093
slide looks a lot like the previous one for linear regression.

71
00:02:27,050 --> 00:02:28,046
But what I'm going to do is

72
00:02:29,021 --> 00:02:31,038
write the update for theta 0 separately.

73
00:02:31,066 --> 00:02:32,093
So, the first line is

74
00:02:33,006 --> 00:02:34,011
for update for theta 0 and

75
00:02:34,022 --> 00:02:35,046
a second line is now

76
00:02:35,059 --> 00:02:36,072
my update for theta 1

77
00:02:36,087 --> 00:02:38,046
up to theta N.

78
00:02:38,090 --> 00:02:40,074
Because I'm going to treat theta 0 separately.

79
00:02:41,069 --> 00:02:43,013
And in order to

80
00:02:43,069 --> 00:02:45,037
modify this algorithm, to use

81
00:02:46,077 --> 00:02:48,047
a regularized cos function,

82
00:02:49,009 --> 00:02:50,050
all I need to do is

83
00:02:50,094 --> 00:02:51,081
pretty similar to what we

84
00:02:51,093 --> 00:02:53,069
did for linear regression is

85
00:02:53,087 --> 00:02:55,062
actually to just modify this

86
00:02:55,088 --> 00:02:57,047
second update rule as follows.

87
00:02:58,050 --> 00:02:59,080
And, once again, this, you know,

88
00:03:00,037 --> 00:03:02,008
cosmetically looks identical what

89
00:03:02,022 --> 00:03:03,071
we had for linear regression.

90
00:03:04,058 --> 00:03:05,058
But of course is not the

91
00:03:05,065 --> 00:03:06,059
same algorithm as we had,

92
00:03:06,088 --> 00:03:08,037
because now the hypothesis

93
00:03:08,078 --> 00:03:10,041
is defined using this.

94
00:03:10,086 --> 00:03:12,055
So this is not the same algorithm

95
00:03:13,012 --> 00:03:14,038
as regularized linear regression.

96
00:03:14,083 --> 00:03:16,034
Because the hypothesis is different.

97
00:03:16,093 --> 00:03:18,036
Even though this update that I wrote down.

98
00:03:18,062 --> 00:03:20,015
It actually looks cosmetically the

99
00:03:20,034 --> 00:03:22,012
same as what we had earlier.

100
00:03:22,047 --> 00:03:25,031
We're working out gradient descent for regularized linear regression.

101
00:03:26,068 --> 00:03:27,071
And of course, just to wrap

102
00:03:27,083 --> 00:03:29,036
up this discussion, this term

103
00:03:29,056 --> 00:03:30,086
here in the square

104
00:03:31,012 --> 00:03:32,033
brackets, so this term

105
00:03:32,066 --> 00:03:35,012
here, this term is,

106
00:03:35,040 --> 00:03:36,075
of course, the new partial

107
00:03:37,021 --> 00:03:38,059
derivative for respect of

108
00:03:38,065 --> 00:03:41,041
theta J of the new cost function J of theta.

109
00:03:42,030 --> 00:03:43,047
Where J of theta here is

110
00:03:43,069 --> 00:03:44,097
the cost function we defined on

111
00:03:45,018 --> 00:03:48,009
a previous slide that does use regularization.

112
00:03:49,077 --> 00:03:52,006
So, that's gradient descent for regularized linear regression.

113
00:03:55,019 --> 00:03:56,043
Let's talk about how to

114
00:03:56,058 --> 00:03:58,028
get regularized linear regression

115
00:03:58,094 --> 00:04:00,000
to work using the more

116
00:04:00,036 --> 00:04:02,006
advanced optimization methods.

117
00:04:03,018 --> 00:04:05,059
And just to remind you for

118
00:04:05,084 --> 00:04:06,080
those methods what we needed

119
00:04:07,008 --> 00:04:08,038
to do was to define the

120
00:04:08,044 --> 00:04:09,046
function that's called the cost

121
00:04:09,063 --> 00:04:11,015
function, that takes us

122
00:04:11,028 --> 00:04:13,065
input the parameter vector theta and

123
00:04:13,078 --> 00:04:16,018
once again in the equations

124
00:04:16,076 --> 00:04:19,002
we've been writing here we used 0 index vectors.

125
00:04:19,050 --> 00:04:20,068
So we had theta 0 up

126
00:04:21,018 --> 00:04:22,081
to theta N. But

127
00:04:23,001 --> 00:04:25,092
because Octave indexes the vectors starting from 1.

128
00:04:26,081 --> 00:04:28,024
Theta 0 is written

129
00:04:28,056 --> 00:04:29,099
in Octave as theta 1.

130
00:04:30,012 --> 00:04:31,062
Theta 1 is written in

131
00:04:31,086 --> 00:04:32,093
Octave as theta 2, and

132
00:04:33,027 --> 00:04:35,006
so on down to theta

133
00:04:36,026 --> 00:04:36,064
N plus 1.

134
00:04:36,074 --> 00:04:38,044
And what we needed to

135
00:04:38,060 --> 00:04:40,024
do was provide a function.

136
00:04:41,017 --> 00:04:42,037
Let's provide a function called

137
00:04:42,077 --> 00:04:44,013
cost function that we would

138
00:04:44,036 --> 00:04:46,092
then pass in to what we have, what we saw earlier.

139
00:04:47,030 --> 00:04:48,049
We will use the fminunc

140
00:04:49,006 --> 00:04:50,031
and then

141
00:04:50,054 --> 00:04:52,016
you know at cost function,

142
00:04:54,082 --> 00:04:55,043
and so on, right.

143
00:04:55,060 --> 00:04:56,087
But the F min, u

144
00:04:57,002 --> 00:04:58,006
and c was the F min

145
00:04:58,027 --> 00:04:59,031
unconstrained and this will

146
00:04:59,064 --> 00:05:01,023
work with fminunc

147
00:05:01,031 --> 00:05:02,030
was what will take

148
00:05:02,054 --> 00:05:04,033
the cost function and minimize it for us.

149
00:05:05,094 --> 00:05:07,005
So the two main things that

150
00:05:07,017 --> 00:05:08,060
the cost function needed to

151
00:05:08,069 --> 00:05:10,062
return were first J-val.

152
00:05:11,027 --> 00:05:12,039
And for that, we need

153
00:05:12,072 --> 00:05:13,094
to write code to

154
00:05:14,001 --> 00:05:15,070
compute the cost function J of theta.

155
00:05:17,012 --> 00:05:19,002
Now, when we're using regularized logistic

156
00:05:19,044 --> 00:05:20,092
regression, of course the

157
00:05:20,099 --> 00:05:21,095
cost function j of theta

158
00:05:22,027 --> 00:05:23,044
changes and, in particular,

159
00:05:24,048 --> 00:05:25,075
now a cost function needs to

160
00:05:25,087 --> 00:05:29,057
include this additional regularization term at the end as well.

161
00:05:29,085 --> 00:05:30,093
So, when you compute j of

162
00:05:31,002 --> 00:05:33,041
theta be sure to include that term at the end.

163
00:05:34,058 --> 00:05:35,051
And then, the other thing that

164
00:05:36,005 --> 00:05:37,024
this cost function thing

165
00:05:37,068 --> 00:05:39,000
needs to derive with a gradient.

166
00:05:39,052 --> 00:05:41,017
So gradient one needs

167
00:05:41,039 --> 00:05:42,056
to be set to the

168
00:05:42,066 --> 00:05:44,007
partial derivative of J

169
00:05:44,024 --> 00:05:45,051
of theta with respect to theta

170
00:05:45,068 --> 00:05:47,017
zero, gradient two needs

171
00:05:47,057 --> 00:05:49,051
to be set to that, and so on.

172
00:05:49,077 --> 00:05:50,089
Once again, the index is off by one.

173
00:05:51,022 --> 00:05:52,085
Right, because of the indexing from

174
00:05:53,011 --> 00:05:54,044
one Octave users.

175
00:05:55,093 --> 00:05:56,077
And looking at these terms.

176
00:05:57,085 --> 00:05:58,068
This term over here.

177
00:05:59,041 --> 00:06:00,063
We actually worked this out

178
00:06:00,072 --> 00:06:02,083
on a previous slide is actually equal to this.

179
00:06:03,023 --> 00:06:03,063
It doesn't change.

180
00:06:04,012 --> 00:06:07,025
Because the derivative for theta zero doesn't change.

181
00:06:07,064 --> 00:06:09,054
Compared to the version without regularization.

182
00:06:10,095 --> 00:06:13,020
And the other terms do change.

183
00:06:13,083 --> 00:06:16,033
And in particular the derivative respect to theta one.

184
00:06:17,000 --> 00:06:18,082
We worked this out on the previous slide as well.

185
00:06:19,011 --> 00:06:20,067
Is equal to, you know,

186
00:06:20,088 --> 00:06:22,056
the original term and then minus

187
00:06:23,044 --> 00:06:24,087
londer M times theta 1.

188
00:06:25,031 --> 00:06:27,013
Just so we make sure we pass this correctly.

189
00:06:27,080 --> 00:06:29,037
And we can add parentheses here.

190
00:06:29,082 --> 00:06:30,098
Right, so the summation doesn't extend.

191
00:06:31,056 --> 00:06:33,016
And similarly, you know,

192
00:06:33,037 --> 00:06:34,080
this other term here looks

193
00:06:35,012 --> 00:06:36,018
like this, with this additional

194
00:06:37,006 --> 00:06:37,094
term that we had on

195
00:06:38,002 --> 00:06:39,076
the previous slide, that corresponds to

196
00:06:39,094 --> 00:06:41,044
the gradient from their regularization objective.

197
00:06:42,023 --> 00:06:43,064
So if you implement this

198
00:06:43,081 --> 00:06:45,013
cost function and pass

199
00:06:45,072 --> 00:06:47,037
this into fminunc

200
00:06:48,018 --> 00:06:49,016
or to one of those advanced optimization

201
00:06:50,005 --> 00:06:51,093
techniques, that will minimize

202
00:06:52,054 --> 00:06:55,099
the new regularized cost function J of theta.

203
00:06:56,099 --> 00:06:58,022
And the parameters you get out

204
00:06:59,052 --> 00:07:00,074
will be the ones that correspond to

205
00:07:01,044 --> 00:07:02,093
logistic regression with regularization.

206
00:07:04,041 --> 00:07:05,054
So, now you know

207
00:07:05,077 --> 00:07:08,020
how to implement regularized logistic regression.

208
00:07:09,077 --> 00:07:10,092
When I walk around Silicon Valley,

209
00:07:11,037 --> 00:07:12,089
I live here in Silicon Valley, there are

210
00:07:13,010 --> 00:07:14,089
a lot of engineers that are frankly, making

211
00:07:15,042 --> 00:07:16,049
a ton of money for their

212
00:07:16,061 --> 00:07:18,008
companies using machine learning algorithms.

213
00:07:19,018 --> 00:07:20,038
And I know we've

214
00:07:20,060 --> 00:07:22,086
only been, you know, studying this stuff for a little while.

215
00:07:23,062 --> 00:07:25,041
But if you understand linear

216
00:07:26,050 --> 00:07:28,036
regression, the advanced optimization

217
00:07:29,020 --> 00:07:30,070
algorithms and regularization, by

218
00:07:30,094 --> 00:07:32,051
now, frankly, you probably know

219
00:07:32,094 --> 00:07:34,026
quite a lot more machine learning

220
00:07:35,000 --> 00:07:36,029
than many, certainly now,

221
00:07:36,075 --> 00:07:38,005
but you probably know quite a

222
00:07:38,018 --> 00:07:39,057
lot more machine learning right now

223
00:07:40,024 --> 00:07:41,067
than frankly, many of the

224
00:07:41,081 --> 00:07:44,075
Silicon Valley engineers out there having very successful careers.

225
00:07:45,030 --> 00:07:46,042
You know, making tons of money for the companies.

226
00:07:47,005 --> 00:07:49,025
Or building products using machine learning algorithms.

227
00:07:50,037 --> 00:07:50,095
So, congratulations.

228
00:07:52,007 --> 00:07:53,012
You've actually come a long ways.

229
00:07:53,049 --> 00:07:54,055
And you can actually, you

230
00:07:54,077 --> 00:07:55,099
actually know enough to

231
00:07:56,031 --> 00:07:58,020
apply this stuff and get to work for many problems.

232
00:07:59,025 --> 00:08:00,057
So congratulations for that.

233
00:08:00,077 --> 00:08:01,087
But of course, there's

234
00:08:02,035 --> 00:08:03,027
still a lot more that we

235
00:08:03,039 --> 00:08:05,018
want to teach you, and in

236
00:08:05,037 --> 00:08:06,054
the next set of videos after

237
00:08:06,056 --> 00:08:07,085
this, we'll start to talk

238
00:08:08,002 --> 00:08:10,088
about a very powerful cause of non-linear classifier.

239
00:08:11,068 --> 00:08:13,035
So whereas linear regression, logistic

240
00:08:13,068 --> 00:08:14,093
regression, you know, you can

241
00:08:15,007 --> 00:08:17,031
form polynomial terms, but it

242
00:08:17,045 --> 00:08:18,035
turns out that there are much

243
00:08:18,050 --> 00:08:21,014
more powerful nonlinear quantifiers that

244
00:08:21,045 --> 00:08:23,064
can then sort of polynomial regression.

245
00:08:24,063 --> 00:08:25,077
And in the next set

246
00:08:25,081 --> 00:08:28,027
of videos after this one, I'll start telling you about them.

247
00:08:28,050 --> 00:08:29,056
So that you have even more

248
00:08:29,075 --> 00:08:30,043
powerful learning algorithms than you have

249
00:08:31,037 --> 00:08:32,087
now to apply to different problems.
