
1
00:00:00,027 --> 00:00:01,037
Neural Networks are one of

2
00:00:01,044 --> 00:00:03,060
the most powerful learning algorithms that we have today.

3
00:00:04,030 --> 00:00:05,049
In this, and in the

4
00:00:05,058 --> 00:00:06,069
next few videos, I'd like to

5
00:00:06,075 --> 00:00:08,002
start talking about a learning

6
00:00:08,038 --> 00:00:09,092
algorithm for fitting the parameters

7
00:00:10,063 --> 00:00:12,052
of the neural network given the training set.

8
00:00:13,046 --> 00:00:14,083
As for the discussion of most

9
00:00:15,001 --> 00:00:16,030
of the learning algorithms, we're going

10
00:00:16,044 --> 00:00:17,085
to begin by talking about the

11
00:00:17,096 --> 00:00:20,051
cost function for fitting the parameters of the network.

12
00:00:21,017 --> 00:00:22,064
I'm going to focus

13
00:00:23,026 --> 00:00:24,078
on the application of neural

14
00:00:25,005 --> 00:00:26,058
networks to classification problems.

15
00:00:27,066 --> 00:00:28,085
So, suppose we have a

16
00:00:29,012 --> 00:00:31,030
network like that shown on the left.

17
00:00:31,053 --> 00:00:32,050
And suppose we have a training

18
00:00:32,071 --> 00:00:33,085
set like this of this of

19
00:00:33,097 --> 00:00:36,054
Xi, Yi pairs of m training examples.

20
00:00:37,092 --> 00:00:39,003
I'm going to use upper case

21
00:00:39,045 --> 00:00:40,064
L to denote the

22
00:00:40,078 --> 00:00:42,046
total number of layers in this network.

23
00:00:43,032 --> 00:00:44,054
So, for the network shown

24
00:00:44,081 --> 00:00:45,071
on the left, we would have

25
00:00:46,036 --> 00:00:47,092
capital L equals 4.

26
00:00:48,002 --> 00:00:48,090
And, I'm going to use

27
00:00:49,017 --> 00:00:50,074
s subscript L, to denote

28
00:00:51,025 --> 00:00:52,053
the number of units, that is

29
00:00:52,072 --> 00:00:54,049
a number of neurons, not counting

30
00:00:54,077 --> 00:00:57,017
the bias unit in layer L of the network.

31
00:00:57,089 --> 00:00:59,043
So, for example, we would

32
00:00:59,057 --> 00:01:01,028
have a S1 which

33
00:01:01,036 --> 00:01:03,032
is the input layer equals S3 unit,

34
00:01:04,014 --> 00:01:05,096
S2 in my example is five units.

35
00:01:06,090 --> 00:01:08,067
And the output layer S4.

36
00:01:09,093 --> 00:01:12,081
Which is also equals SL, because capital L is equal to four.

37
00:01:12,098 --> 00:01:14,029
The output layer in my

38
00:01:14,045 --> 00:01:16,023
example in the left has four units.

39
00:01:17,062 --> 00:01:19,087
We're going to consider two types of classification problems.

40
00:01:20,043 --> 00:01:21,078
The first is binary classification,

41
00:01:22,096 --> 00:01:25,054
where the labels y are either zero or one.

42
00:01:26,023 --> 00:01:28,054
In this case, we would have one output unit.

43
00:01:29,014 --> 00:01:30,026
So, this neural network on top

44
00:01:30,051 --> 00:01:32,043
has four output units, but if

45
00:01:32,056 --> 00:01:33,095
we had binary classification, we would

46
00:01:34,012 --> 00:01:35,081
have only one output unit

47
00:01:36,071 --> 00:01:38,035
that computes h of x.

48
00:01:40,031 --> 00:01:41,054
And the outputs of the

49
00:01:41,062 --> 00:01:42,095
neural network would be h

50
00:01:43,014 --> 00:01:45,057
of x is going to be a real number.

51
00:01:46,090 --> 00:01:47,098
And in this case the number

52
00:01:48,035 --> 00:01:50,023
of output units, SL, where

53
00:01:50,048 --> 00:01:51,087
L is again the index

54
00:01:52,029 --> 00:01:53,096
of the final layer because that's

55
00:01:54,023 --> 00:01:55,062
the number of layers we have in the network.

56
00:01:56,056 --> 00:01:57,095
So the number of units we

57
00:01:58,010 --> 00:02:00,006
have in the output layer is going to be equal to one.

58
00:02:01,004 --> 00:02:02,043
In this case, to simplify notation

59
00:02:02,095 --> 00:02:05,034
later, I'm also going to set k equals 1.

60
00:02:05,045 --> 00:02:06,056
So, you can think of k as

61
00:02:06,076 --> 00:02:08,024
also denoting the number

62
00:02:08,069 --> 00:02:10,078
of units in the output layer.

63
00:02:11,040 --> 00:02:12,097
The second type of classification problem

64
00:02:13,028 --> 00:02:15,015
we'll consider will be multiclass classification

65
00:02:15,078 --> 00:02:18,002
problem where we may have k distinct classes.

66
00:02:19,015 --> 00:02:20,075
So, our early example, I

67
00:02:21,006 --> 00:02:22,053
had this representation for y

68
00:02:23,008 --> 00:02:24,090
if we have four classes and

69
00:02:25,015 --> 00:02:27,005
in this case, we would have caps lock K

70
00:02:27,034 --> 00:02:29,053
output units and our hypotheses

71
00:02:30,034 --> 00:02:33,071
will output vectors that are K dimensional.

72
00:02:34,097 --> 00:02:36,022
And the number of output units

73
00:02:36,075 --> 00:02:38,038
will be equal to K.

74
00:02:39,000 --> 00:02:40,002
And usually we will have

75
00:02:40,037 --> 00:02:41,062
K greater than or equal

76
00:02:41,081 --> 00:02:42,096
to three in this case, because

77
00:02:43,097 --> 00:02:45,034
if we had two classes then,

78
00:02:45,071 --> 00:02:46,056
you know, we don't need to

79
00:02:46,068 --> 00:02:48,033
use the one versus all method.

80
00:02:48,071 --> 00:02:49,063
We need to use the one versus

81
00:02:49,096 --> 00:02:50,094
all method only if we

82
00:02:51,011 --> 00:02:52,046
have K greater than or

83
00:02:52,074 --> 00:02:54,025
equal to three classes so we

84
00:02:54,046 --> 00:02:56,009
only have two classes we will

85
00:02:56,018 --> 00:02:57,066
need to use only one output unit.

86
00:02:58,025 --> 00:03:00,087
Now, let's define the cost function for our cost function for our neural network.

87
00:03:03,087 --> 00:03:05,012
The cost function we use for

88
00:03:05,024 --> 00:03:06,053
the neural network is going to

89
00:03:06,068 --> 00:03:08,030
be a generalization of the

90
00:03:08,036 --> 00:03:09,034
one that we use for logistic

91
00:03:09,050 --> 00:03:11,050
regression. 
For logistic regression,

92
00:03:12,009 --> 00:03:13,043
we used to minimize the

93
00:03:13,050 --> 00:03:14,049
cost function j of theta

94
00:03:15,027 --> 00:03:16,055
that was minus 1 over

95
00:03:16,077 --> 00:03:17,075
m of this cost function

96
00:03:18,071 --> 00:03:20,056
and then plus this extra regularization

97
00:03:21,030 --> 00:03:22,065
term here, where this was

98
00:03:22,084 --> 00:03:24,002
a sum from j equals

99
00:03:24,069 --> 00:03:26,018
1 through n, because we

100
00:03:26,027 --> 00:03:29,075
did not regularize the bias term theta zero.

101
00:03:31,003 --> 00:03:32,059
For a neural network our cost

102
00:03:32,090 --> 00:03:34,049
function is going to be a generalization of this.

103
00:03:35,065 --> 00:03:37,006
Where instead of having basically

104
00:03:37,053 --> 00:03:39,036
just one logistic regression output

105
00:03:39,065 --> 00:03:41,065
unit, we may instead have K of them.

106
00:03:42,059 --> 00:03:43,052
So here's our cost function.

107
00:03:44,077 --> 00:03:46,030
Neural network now outputs vectors

108
00:03:46,071 --> 00:03:47,091
in RK where K might

109
00:03:48,016 --> 00:03:48,083
be equal to 1 if we

110
00:03:49,019 --> 00:03:50,034
have the binary classification problem.

111
00:03:51,037 --> 00:03:52,024
I'm going to use this notation,

112
00:03:53,030 --> 00:03:56,046
h of x subscript i, to denote the ith output.

113
00:03:57,043 --> 00:03:59,086
That is h of x is a K dimensional vector.

114
00:04:00,084 --> 00:04:02,059
And so this subscript i just

115
00:04:02,096 --> 00:04:04,040
selects out the ith element

116
00:04:05,019 --> 00:04:07,050
of the vector that is output by my neural network.

117
00:04:08,090 --> 00:04:10,005
My cost function, j of

118
00:04:10,018 --> 00:04:11,058
theta is now going

119
00:04:11,075 --> 00:04:13,078
to be the following is minus one

120
00:04:13,093 --> 00:04:14,084
over m of a sum

121
00:04:15,041 --> 00:04:16,077
of a similar term to what

122
00:04:16,095 --> 00:04:18,099
we have in logistic regression.
Except that

123
00:04:19,030 --> 00:04:20,036
we have this sum from K

124
00:04:21,001 --> 00:04:22,049
equals one through K.  The

125
00:04:22,060 --> 00:04:23,064
summation is basically a

126
00:04:23,072 --> 00:04:25,057
sum over my K output unit.

127
00:04:26,006 --> 00:04:28,029
So, if I have four upper units.

128
00:04:29,039 --> 00:04:30,074
That is the final layer of my

129
00:04:30,085 --> 00:04:32,052
neural network has four output

130
00:04:32,086 --> 00:04:34,042
units then this sum

131
00:04:34,069 --> 00:04:35,068
from, this is a sum from

132
00:04:35,089 --> 00:04:37,013
K equals one through four

133
00:04:38,005 --> 00:04:40,055
of basically the logistic regression algorithms

134
00:04:42,006 --> 00:04:43,063
cost function but summing

135
00:04:43,075 --> 00:04:45,056
that cost function over each

136
00:04:45,088 --> 00:04:47,012
of my four output units in turn.

137
00:04:47,080 --> 00:04:48,097
And so, you notice

138
00:04:49,037 --> 00:04:50,069
in particular that this applies

139
00:04:51,039 --> 00:04:53,052
to YK, HK, because

140
00:04:53,074 --> 00:04:55,004
we're basically taking the K

141
00:04:55,050 --> 00:04:57,001
upper unit and comparing that

142
00:04:57,077 --> 00:04:59,058
to the value of YK, which

143
00:04:59,081 --> 00:05:02,001
is you know, which is

144
00:05:02,020 --> 00:05:03,025
that one of those vectors

145
00:05:03,074 --> 00:05:05,011
to say what cause it should be.

146
00:05:06,027 --> 00:05:08,006
And finally, the second term

147
00:05:08,036 --> 00:05:09,049
here is the regularization

148
00:05:10,043 --> 00:05:12,097
term similar to what we had for logistic regression.

149
00:05:14,007 --> 00:05:15,063
This summation terms looks really

150
00:05:15,085 --> 00:05:17,037
complicated and always doing

151
00:05:17,083 --> 00:05:19,045
is a summing over these terms,

152
00:05:19,094 --> 00:05:21,067
theta j i l for

153
00:05:21,086 --> 00:05:23,033
all values of i j

154
00:05:23,041 --> 00:05:24,082
and l. 
Except that we

155
00:05:25,000 --> 00:05:26,033
don't sum over the terms

156
00:05:26,070 --> 00:05:28,020
corresponding to these bias values

157
00:05:28,089 --> 00:05:30,000
like we had for logistic progression.

158
00:05:30,089 --> 00:05:32,007
Concretely, we don't sum

159
00:05:32,024 --> 00:05:33,058
over the terms corresponding

160
00:05:34,030 --> 00:05:36,029
to where i is equal to zero.

161
00:05:36,077 --> 00:05:38,031
So, that is because

162
00:05:38,092 --> 00:05:40,000
when we are computing the activation

163
00:05:40,058 --> 00:05:41,093
of the neuron, we have terms

164
00:05:42,027 --> 00:05:43,062
like these, you know theta, i0

165
00:05:43,081 --> 00:05:47,086
plus theta, i1,

166
00:05:48,016 --> 00:05:50,041
x1 plus, and

167
00:05:50,051 --> 00:05:51,077
so on, where I guess

168
00:05:52,001 --> 00:05:53,031
we could have a 2 there

169
00:05:53,049 --> 00:05:54,042
if this is the first hidden layer,

170
00:05:55,025 --> 00:05:56,080
and so the values with

171
00:05:57,023 --> 00:05:58,073
the 0 there at that corresponds to

172
00:05:58,073 --> 00:06:00,011
something that multiplies into an

173
00:06:00,025 --> 00:06:01,045
x0 or an a0 and

174
00:06:02,020 --> 00:06:02,094
so, this is kind of like

175
00:06:03,012 --> 00:06:04,081
a bias unit and by

176
00:06:04,098 --> 00:06:06,001
analogy to what we were

177
00:06:06,012 --> 00:06:07,068
doing for logistic progression, we won't

178
00:06:07,088 --> 00:06:09,008
sum over those terms in

179
00:06:09,016 --> 00:06:11,005
our regularization term because we

180
00:06:11,016 --> 00:06:13,047
don't want to regularize them and

181
00:06:13,067 --> 00:06:15,013
string the values 0.

182
00:06:15,036 --> 00:06:16,052
But this is just one possible convention

183
00:06:17,067 --> 00:06:18,067
and even if you were

184
00:06:18,083 --> 00:06:20,095
to sum over, you know, i equals 0 up

185
00:06:21,019 --> 00:06:22,081
to SL, it will work

186
00:06:23,016 --> 00:06:24,072
about the same and it doesn't make a big difference.

187
00:06:25,052 --> 00:06:26,075
But maybe this convention

188
00:06:27,050 --> 00:06:28,079
of not regularizing the bias

189
00:06:29,006 --> 00:06:30,031
term is just slightly more common.

190
00:06:32,095 --> 00:06:34,019
So, that's the cost function

191
00:06:34,068 --> 00:06:36,026
we're going to use to fill on your own network.

192
00:06:36,079 --> 00:06:38,012
In the next video, we'll start

193
00:06:38,048 --> 00:06:40,026
to talk about an algorithm for

194
00:06:40,056 --> 00:06:42,052
trying to optimize the cost function.
