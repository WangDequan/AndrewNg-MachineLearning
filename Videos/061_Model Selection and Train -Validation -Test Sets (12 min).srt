
1
00:00:00,009 --> 00:00:01,024
Suppose you like to decide

2
00:00:01,051 --> 00:00:03,035
what degree of polynomial to

3
00:00:03,049 --> 00:00:04,065
fit to a data set, sort of

4
00:00:04,082 --> 00:00:07,083
what features to include to give you a learning algorithm.

5
00:00:08,076 --> 00:00:09,093
Or suppose you'd like to choose

6
00:00:10,023 --> 00:00:13,044
the regularization parameter lambda for the learning algorithm.

7
00:00:14,048 --> 00:00:15,001
How do you do that?

8
00:00:15,077 --> 00:00:17,039
These are called model selection problems.

9
00:00:18,026 --> 00:00:19,050
And in our discussion of

10
00:00:19,060 --> 00:00:20,082
how to do this we'll talk

11
00:00:21,007 --> 00:00:22,005
about not just how to

12
00:00:22,014 --> 00:00:23,050
split your data into a train

13
00:00:23,087 --> 00:00:25,039
and test sets but how to

14
00:00:25,048 --> 00:00:26,096
split your data into what

15
00:00:27,014 --> 00:00:28,039
we'll discover is called the

16
00:00:28,067 --> 00:00:30,014
train validation and test sets.

17
00:00:31,003 --> 00:00:32,007
We'll see in this video

18
00:00:32,050 --> 00:00:33,035
just what these things are and

19
00:00:33,043 --> 00:00:35,053
how to use them to do model selection.

20
00:00:36,084 --> 00:00:37,088
We've already seen a lot

21
00:00:38,006 --> 00:00:39,042
of times the problem of overfitting,

22
00:00:40,011 --> 00:00:41,040
in which just because the

23
00:00:41,064 --> 00:00:43,017
learning algorithm fits a training

24
00:00:43,056 --> 00:00:45,056
set well, that doesn't mean there's a good hypothesis.

25
00:00:47,043 --> 00:00:48,067
More generally, this is why

26
00:00:48,096 --> 00:00:50,035
the training set error is

27
00:00:50,060 --> 00:00:52,021
not a good predictor for how

28
00:00:52,049 --> 00:00:54,068
well the hypothesis will do on new examples.

29
00:00:55,093 --> 00:00:57,043
Concretely, if you fit

30
00:00:57,067 --> 00:00:58,079
some set of parameters - theta

31
00:00:59,007 --> 00:01:00,013
0, theta 1, theta 2

32
00:01:00,031 --> 00:01:01,046
and so on - to your

33
00:01:01,067 --> 00:01:03,072
training set then, the fact

34
00:01:04,026 --> 00:01:05,079
that your hypothesis does well in

35
00:01:05,087 --> 00:01:07,034
the training set, well, this

36
00:01:07,056 --> 00:01:08,070
doesn't mean much in terms

37
00:01:09,000 --> 00:01:10,018
of predicting how well your

38
00:01:10,042 --> 00:01:12,053
hypothesis will generalize to new

39
00:01:12,073 --> 00:01:14,039
examples not seen in

40
00:01:14,050 --> 00:01:15,089
the training set.

41
00:01:16,004 --> 00:01:17,062
And the more general principal is that,

42
00:01:18,031 --> 00:01:19,068
once your parameters were fit

43
00:01:19,093 --> 00:01:21,043
to some set of data--maybe the

44
00:01:21,051 --> 00:01:22,076
training set, maybe something else--then

45
00:01:23,056 --> 00:01:25,006
the error of your hypothesis

46
00:01:25,062 --> 00:01:26,073
as measured on that same

47
00:01:26,095 --> 00:01:28,026
data set, such as the

48
00:01:28,050 --> 00:01:30,009
training error, that's unlikely

49
00:01:31,079 --> 00:01:33,006
to be a good estimate

50
00:01:33,068 --> 00:01:35,054
of your actual generalization error, that

51
00:01:35,073 --> 00:01:37,010
is, of how well the

52
00:01:37,056 --> 00:01:39,025
hypothesis will generalize to new examples.

53
00:01:43,001 --> 00:01:45,001
Now let's consider the model selection problem.

54
00:01:45,090 --> 00:01:46,090
Let's say you try to choose

55
00:01:47,046 --> 00:01:49,054
what degree polynomial to fit to data.

56
00:01:50,001 --> 00:01:51,034
So, you should you choose a linear function, a

57
00:01:51,056 --> 00:01:52,062
quadratic function, a cubic function,

58
00:01:53,045 --> 00:01:57,050
all the way up to a 10th power polynomial?

59
00:01:57,053 --> 00:01:59,051
So it's as if there's one extra parameter in

60
00:01:59,064 --> 00:02:00,082
this algorithm, which I'm going

61
00:02:00,089 --> 00:02:02,045
to denote d, which is

62
00:02:03,000 --> 00:02:06,070
what degree of polynomial do you want to pick?

63
00:02:07,004 --> 00:02:07,085
So it is as if

64
00:02:08,015 --> 00:02:10,056
does this, in addition

65
00:02:11,008 --> 00:02:13,006
to the theta parameters it's

66
00:02:13,027 --> 00:02:14,053
as if there's one more parameter d

67
00:02:14,099 --> 00:02:17,047
that your trying to determine using your data cells.

68
00:02:18,043 --> 00:02:19,069
the first option is d

69
00:02:19,099 --> 00:02:21,005
equals 1, which is for the linear

70
00:02:21,027 --> 00:02:22,084
function we can choose d

71
00:02:23,050 --> 00:02:24,077
equals 2, d equals 3,

72
00:02:24,088 --> 00:02:25,093
all the way up to d

73
00:02:26,012 --> 00:02:26,094
equals 10, so we would like

74
00:02:27,009 --> 00:02:29,012
to fit this extra sort of parameter,

75
00:02:29,081 --> 00:02:31,046
which I am denoting by d,

76
00:02:31,065 --> 00:02:33,025
and concretely, let's say

77
00:02:33,053 --> 00:02:34,099
that you want to choose a

78
00:02:35,003 --> 00:02:36,013
model, that is choose

79
00:02:36,050 --> 00:02:38,028
a degree of polynomial choose one off

80
00:02:38,043 --> 00:02:40,018
these ten models, and fit that model

81
00:02:41,025 --> 00:02:42,056
and also get some estimate

82
00:02:43,062 --> 00:02:45,019
of how well your fitted hypothesis

83
00:02:46,012 --> 00:02:47,056
will generalize to new

84
00:02:48,038 --> 00:02:49,083
examples.
Here's one thing

85
00:02:50,000 --> 00:02:51,031
you could do: you could

86
00:02:52,031 --> 00:02:53,074
take your first model and

87
00:02:54,050 --> 00:02:55,084
minimize the training error

88
00:02:56,066 --> 00:02:57,050
and this would give you some

89
00:02:57,081 --> 00:03:00,011
parameter vector theta, and

90
00:03:01,008 --> 00:03:02,003
you can then take your second model,

91
00:03:02,075 --> 00:03:05,028
the quadratic function and

92
00:03:05,046 --> 00:03:06,084
for that your training set and

93
00:03:06,094 --> 00:03:08,090
this will give you some other parameters vector theta.

94
00:03:09,084 --> 00:03:10,099
In order to distinguish between

95
00:03:11,031 --> 00:03:13,028
these different parameter vectors, I'm

96
00:03:13,043 --> 00:03:15,071
going to use a superscript 1, superscript 2

97
00:03:15,087 --> 00:03:17,053
there where theta superscript

98
00:03:18,009 --> 00:03:19,003
1 just means the parameters

99
00:03:19,065 --> 00:03:21,003
I get by fitting this

100
00:03:21,015 --> 00:03:22,018
model to my training data,

101
00:03:23,002 --> 00:03:24,087
and theta superscript 2 just means

102
00:03:25,019 --> 00:03:26,031
the parameters I get by fitting

103
00:03:26,097 --> 00:03:30,058
this quadratic function to my training ata and so on.

104
00:03:30,077 --> 00:03:33,053
And by fitting a cubic model I get parameters theta 3

105
00:03:34,018 --> 00:03:36,038
up to, you know, say theta 10.

106
00:03:37,000 --> 00:03:38,005
And one thing we could

107
00:03:38,075 --> 00:03:40,049
do is then take these

108
00:03:40,071 --> 00:03:42,071
parameters and look at the test set error.

109
00:03:42,094 --> 00:03:44,018
So I can compute on my

110
00:03:44,034 --> 00:03:45,084
test set, j test

111
00:03:46,055 --> 00:03:49,078
of 1, j test of

112
00:03:50,091 --> 00:03:52,050
theta 2 and so

113
00:03:52,090 --> 00:03:55,065
on, j test of

114
00:03:55,087 --> 00:03:58,000
theta 3 and so on.

115
00:03:58,021 --> 00:04:00,025
So I'm going to

116
00:04:00,040 --> 00:04:02,013
take each of my hypothesis

117
00:04:02,044 --> 00:04:04,078
with the corresponding and just measure

118
00:04:05,018 --> 00:04:06,046
the performance on the test set.

119
00:04:08,087 --> 00:04:10,006
Now one thing I could do

120
00:04:10,034 --> 00:04:11,061
then is, in order to select

121
00:04:12,058 --> 00:04:13,069
one of these models, I could

122
00:04:13,087 --> 00:04:15,043
then see which model

123
00:04:15,094 --> 00:04:17,000
has the lowest test sets

124
00:04:17,035 --> 00:04:18,024
error, and lets just say

125
00:04:18,043 --> 00:04:19,075
for this example, that I

126
00:04:19,097 --> 00:04:21,092
ended up choosing the fifth order polynomial.

127
00:04:23,014 --> 00:04:24,057
So this seems reasonable so far.

128
00:04:25,055 --> 00:04:26,075
By now, lets say, I want to

129
00:04:27,031 --> 00:04:29,067
take my fit hypothesis, this fifth

130
00:04:30,004 --> 00:04:31,093
order model and let's

131
00:04:32,018 --> 00:04:34,018
say I want to ask how well this model generalized.

132
00:04:35,075 --> 00:04:36,074
One thing I could do is

133
00:04:37,012 --> 00:04:38,057
look at how well my fifth

134
00:04:38,086 --> 00:04:42,035
order polynomial hypothesis, had done on my test set.

135
00:04:43,013 --> 00:04:44,080
But the problem is this

136
00:04:45,005 --> 00:04:46,023
will not to be a fair

137
00:04:46,043 --> 00:04:47,058
estimate of how well

138
00:04:48,012 --> 00:04:49,012
my hypothesis generalizes.

139
00:04:51,005 --> 00:04:53,038
And the reason is, what we've

140
00:04:53,057 --> 00:04:54,085
done is, we've fit this

141
00:04:55,001 --> 00:04:56,077
extra the parameter d, that

142
00:04:56,094 --> 00:04:58,025
is this degree of polynomial,

143
00:04:59,011 --> 00:05:00,033
and we'll fit that parameter

144
00:05:01,004 --> 00:05:02,051
d using the test set.

145
00:05:02,089 --> 00:05:03,088
Namely, we chose the value

146
00:05:04,027 --> 00:05:05,050
of d that gave us the

147
00:05:05,070 --> 00:05:07,026
best possible performance on the

148
00:05:07,035 --> 00:05:09,006
test set, and

149
00:05:09,031 --> 00:05:11,025
so, the performance of

150
00:05:11,056 --> 00:05:13,002
my parameter vector theta five

151
00:05:13,068 --> 00:05:15,023
on the test set, that's likely to

152
00:05:15,050 --> 00:05:16,081
be to be an overly optimistic

153
00:05:17,050 --> 00:05:19,076
estimate of generalization error.

154
00:05:19,095 --> 00:05:21,008
Right? So that because I have fit

155
00:05:21,025 --> 00:05:22,025
this parameter d to my

156
00:05:22,036 --> 00:05:23,072
test set, it is no

157
00:05:23,087 --> 00:05:25,031
longer fair to

158
00:05:25,050 --> 00:05:28,012
evaluate my hypothesis on this test set.

159
00:05:28,033 --> 00:05:30,049
That's because I've fit my parameters to the test set.

160
00:05:30,069 --> 00:05:32,027
I've chosen the degree d of

161
00:05:32,041 --> 00:05:34,054
polynomial using the test set. And

162
00:05:34,073 --> 00:05:36,036
so our hypothesis is

163
00:05:36,052 --> 00:05:37,086
likely to do better on

164
00:05:38,006 --> 00:05:39,027
this test set than it

165
00:05:39,044 --> 00:05:41,002
would on new examples that

166
00:05:41,012 --> 00:05:45,045
it hasn't seen before and that's which is what we hear about.

167
00:05:45,058 --> 00:05:45,087
So, just to reiterate

168
00:05:46,025 --> 00:05:47,072
on the previous slide we

169
00:05:47,083 --> 00:05:48,087
saw that if we fit

170
00:05:49,014 --> 00:05:50,080
some set of parameters, say theta

171
00:05:51,035 --> 00:05:52,043
0, theta 1, and so on, to

172
00:05:52,056 --> 00:05:54,002
some training set, then the

173
00:05:54,010 --> 00:05:55,050
performance of the fitted model on

174
00:05:56,000 --> 00:05:57,050
the training set is not

175
00:05:58,001 --> 00:05:59,018
predictive of how well

176
00:05:59,088 --> 00:06:01,032
the hypothesis we generalized the

177
00:06:01,054 --> 00:06:03,017
new examples; is because these

178
00:06:03,087 --> 00:06:04,072
parameters would fit to the training set.

179
00:06:05,043 --> 00:06:06,052
So they are likely to do

180
00:06:06,067 --> 00:06:07,072
well on the training set, even

181
00:06:08,002 --> 00:06:10,038
if the parameters don't do well on other examples.

182
00:06:12,038 --> 00:06:13,064
And in the procedure I've just described

183
00:06:13,079 --> 00:06:14,083
on this slide, we've just done

184
00:06:14,091 --> 00:06:16,026
the same thing and specifically

185
00:06:16,097 --> 00:06:18,060
what we did is we fit this

186
00:06:18,075 --> 00:06:20,044
parameter d to the test set.

187
00:06:21,074 --> 00:06:22,075
And by having fit the parameter

188
00:06:23,018 --> 00:06:24,062
to the test set, this means that

189
00:06:25,041 --> 00:06:26,029
the performance of the hypothesis

190
00:06:27,007 --> 00:06:28,052
on that test set may not

191
00:06:28,074 --> 00:06:30,033
be a fair estimate of how

192
00:06:30,062 --> 00:06:32,062
well the hypothesis is likely to do

193
00:06:33,011 --> 00:06:35,049
on examples we haven't seen before.

194
00:06:35,050 --> 00:06:37,023
To address this problem in a

195
00:06:37,031 --> 00:06:39,017
model selection setting, if

196
00:06:39,047 --> 00:06:41,000
we want to evaluate a hypothesis

197
00:06:41,080 --> 00:06:44,062
this is usually what we do instead.

198
00:06:45,060 --> 00:06:46,085
Given the data set, instead of just splitting it

199
00:06:47,006 --> 00:06:48,031
into a train and test

200
00:06:48,048 --> 00:06:49,030
set, what we are going to

201
00:06:49,043 --> 00:06:51,036
do is instead split it into three pieces.

202
00:06:52,062 --> 00:06:54,049
And the first piece is

203
00:06:54,070 --> 00:06:57,033
going to be called the training set as usual.

204
00:06:58,073 --> 00:06:59,098
So you call this first part,

205
00:07:01,001 --> 00:07:03,057
the training set, and then

206
00:07:03,073 --> 00:07:05,057
were going to coddle the second

207
00:07:06,004 --> 00:07:07,050
piece of data, which is

208
00:07:07,058 --> 00:07:13,051
called the cross-validation set, and

209
00:07:13,068 --> 00:07:15,050
I'm going to abbreviate cross-validation

210
00:07:17,004 --> 00:07:18,043
CV, and the second

211
00:07:19,056 --> 00:07:20,083
piece of this data, I'm going

212
00:07:21,004 --> 00:07:22,068
to call the cross-validation set

213
00:07:26,043 --> 00:07:28,012
cross-validation, and I

214
00:07:28,026 --> 00:07:31,050
am going to abbreviate cross-validation as CV.

215
00:07:32,014 --> 00:07:33,043
Sometimes it's also called the

216
00:07:33,052 --> 00:07:35,076
validation set, instead of cross-validation set.

217
00:07:36,068 --> 00:07:37,076
And then the last part I

218
00:07:37,081 --> 00:07:40,063
am going to call my usual test set.

219
00:07:40,087 --> 00:07:42,081
And the pretty typical ratio

220
00:07:43,037 --> 00:07:45,023
I wish to split these things; would be to

221
00:07:45,050 --> 00:07:46,062
send 60% of your data

222
00:07:47,000 --> 00:07:48,051
to your training set, maybe 20%

223
00:07:48,087 --> 00:07:52,029
to your cross-validation set, and 20% to your test set.

224
00:07:52,062 --> 00:07:53,083
And these numbers can vary a little

225
00:07:53,097 --> 00:07:55,092
bit but this sort of ratio will be pretty typical.

226
00:07:56,072 --> 00:07:58,012
And so our training set

227
00:07:58,056 --> 00:07:59,094
will now be only, maybe

228
00:08:00,018 --> 00:08:01,006
60% of the data,

229
00:08:02,030 --> 00:08:04,061
and our cross-validation set or

230
00:08:04,068 --> 00:08:07,057
our validation set will have some number of examples.

231
00:08:08,017 --> 00:08:09,022
I'm going to denote that M

232
00:08:09,064 --> 00:08:11,008
subscript cv, so that's the

233
00:08:11,016 --> 00:08:13,077
number of cross-validation examples.

234
00:08:15,057 --> 00:08:17,018
And as following our earlier

235
00:08:17,042 --> 00:08:19,087
notational convention, I'm going

236
00:08:19,093 --> 00:08:22,073
to use XiCV, YiCV.

237
00:08:23,068 --> 00:08:25,035
Following our earlier notational convention

238
00:08:25,050 --> 00:08:26,032
I'm going to use

239
00:08:26,073 --> 00:08:30,076
XiCV, YiCV to

240
00:08:30,087 --> 00:08:33,013
denote the i cross-validation example.

241
00:08:34,040 --> 00:08:35,096
And finally we also

242
00:08:36,039 --> 00:08:37,046
have a test set over here;

243
00:08:38,000 --> 00:08:39,099
with M subscript test,

244
00:08:40,070 --> 00:08:41,094
being the number of test examples.

245
00:08:43,009 --> 00:08:43,087
So, now that we have

246
00:08:44,015 --> 00:08:46,013
defined the training validation or

247
00:08:46,016 --> 00:08:47,070
cross validation and test sets,

248
00:08:48,015 --> 00:08:49,047
we can also define the training

249
00:08:49,088 --> 00:08:52,013
error, cross validation error, and test error.

250
00:08:53,007 --> 00:08:54,069
So here's my training error,

251
00:08:55,025 --> 00:08:56,032
and I'm just writing this as

252
00:08:56,057 --> 00:08:58,000
J j subscript train of theta.

253
00:08:58,050 --> 00:09:00,037
This is pretty much the same thing.

254
00:09:00,066 --> 00:09:01,063
It's usually the same thing as

255
00:09:02,016 --> 00:09:03,092
the j of theta that we'll be writing so far,

256
00:09:04,015 --> 00:09:05,012
this is just a training set

257
00:09:05,076 --> 00:09:07,045
error you guys measure on your training set.

258
00:09:08,044 --> 00:09:09,054
And then j subscript cv

259
00:09:09,095 --> 00:09:12,053
is my cause validation error is pretty much what you'd expect.

260
00:09:13,010 --> 00:09:14,048
Just select the training error, except

261
00:09:14,085 --> 00:09:15,085
measure it on the

262
00:09:16,008 --> 00:09:17,092
cross-validation data set, and here's

263
00:09:18,022 --> 00:09:19,090
my test set error, same as before.

264
00:09:21,016 --> 00:09:22,049
So when theta with the model

265
00:09:22,080 --> 00:09:24,078
selection problem like this

266
00:09:25,000 --> 00:09:26,098
is, instead of using

267
00:09:27,019 --> 00:09:28,087
the test set to select

268
00:09:29,022 --> 00:09:30,049
the model, we're instead going

269
00:09:30,070 --> 00:09:32,035
to use validation set or

270
00:09:32,049 --> 00:09:34,099
the cross-validation set to select the model.

271
00:09:36,023 --> 00:09:37,077
Concretely, we're going to

272
00:09:37,088 --> 00:09:40,067
first take our first hypothesis, take

273
00:09:40,086 --> 00:09:42,048
this first model and say,

274
00:09:43,019 --> 00:09:44,028
minimize the cos function,

275
00:09:45,012 --> 00:09:45,083
and this would give me some

276
00:09:46,013 --> 00:09:47,095
parameter vector theta for the linear model

277
00:09:49,025 --> 00:09:50,075
and as before I'm going to put the

278
00:09:50,087 --> 00:09:52,025
superscript 1 just to

279
00:09:52,040 --> 00:09:53,050
denote that this is a parameter

280
00:09:53,072 --> 00:09:55,029
for the linear model. We do

281
00:09:55,045 --> 00:09:58,019
the same thing for the

282
00:09:58,025 --> 00:10:00,037
quadratic model, get some

283
00:10:00,067 --> 00:10:01,067
parameter vector theta 2, get

284
00:10:01,085 --> 00:10:03,048
some parameter vectors there 3, and so on down

285
00:10:03,072 --> 00:10:04,096
to, say, the tenth by the

286
00:10:05,047 --> 00:10:07,028
polynomial, and what

287
00:10:07,047 --> 00:10:09,024
we I'm going to do is, instead of

288
00:10:09,035 --> 00:10:11,002
testing these hypothesis on the

289
00:10:11,020 --> 00:10:12,032
test set, instead I'm

290
00:10:12,059 --> 00:10:13,060
going to test them on the

291
00:10:13,066 --> 00:10:15,017
cross-validation set. I'm going to

292
00:10:15,039 --> 00:10:17,012
measure j subscript cv,

293
00:10:18,026 --> 00:10:19,075
to see how well each of

294
00:10:19,087 --> 00:10:21,078
these hypothesis do on my

295
00:10:22,062 --> 00:10:28,090
cross validation set and then

296
00:10:29,011 --> 00:10:29,085
I'm going to pick the hypothesis

297
00:10:30,069 --> 00:10:32,034
with the lowest cross-validation error.

298
00:10:32,077 --> 00:10:34,025
So for this example, let's say

299
00:10:34,049 --> 00:10:35,070
for the sake of argument that

300
00:10:36,045 --> 00:10:38,037
it was my fourth order polynomial

301
00:10:39,028 --> 00:10:41,016
that had the lowest cross-validation error.

302
00:10:42,009 --> 00:10:43,004
So in that case, I'm going

303
00:10:43,022 --> 00:10:44,063
to pick this fourth order polynomial

304
00:10:45,028 --> 00:10:47,092
model and finally what

305
00:10:48,010 --> 00:10:49,044
this means is that that parameter

306
00:10:50,013 --> 00:10:51,055
d, remember d was the

307
00:10:51,062 --> 00:10:54,071
degree of polynomial, right d equals 2, d equals 3,

308
00:10:54,089 --> 00:10:56,088
up to d equals 10. What we've

309
00:10:57,007 --> 00:10:58,000
done is we fit that parameter

310
00:10:58,046 --> 00:10:59,059
d, and we'll set d equals 4, and

311
00:10:59,089 --> 00:11:01,021
we did so using

312
00:11:01,060 --> 00:11:02,072
the cross-validation set.

313
00:11:02,091 --> 00:11:04,044
And so this degree of

314
00:11:04,053 --> 00:11:05,053
polynomial, so the parameter

315
00:11:06,052 --> 00:11:07,092
is no longer fit to the test set.

316
00:11:08,083 --> 00:11:10,000
And so we've now saved

317
00:11:10,051 --> 00:11:12,032
a way the test set

318
00:11:13,029 --> 00:11:14,017
and we can use the test

319
00:11:14,033 --> 00:11:15,064
set to measure or to

320
00:11:16,010 --> 00:11:17,088
estimate the generalization error of

321
00:11:18,004 --> 00:11:19,026
the model that was selected

322
00:11:20,037 --> 00:11:23,052
by this algorithm.
So, that

323
00:11:23,067 --> 00:11:25,023
was model selection and how

324
00:11:25,054 --> 00:11:26,071
you can take your data and split

325
00:11:26,087 --> 00:11:28,022
it into a train validation

326
00:11:28,094 --> 00:11:30,016
and test set, and use your

327
00:11:30,029 --> 00:11:31,080
cross validation data to select

328
00:11:32,011 --> 00:11:33,075
model and evaluate it on the test set.

329
00:11:35,008 --> 00:11:36,063
One final note: I should

330
00:11:36,080 --> 00:11:38,072
say that in the machine

331
00:11:39,009 --> 00:11:39,096
learning as of this practice

332
00:11:40,049 --> 00:11:41,040
today, there are many

333
00:11:41,073 --> 00:11:43,028
people that will do

334
00:11:43,044 --> 00:11:44,046
that early thing that I

335
00:11:44,057 --> 00:11:45,061
talked about, and said that

336
00:11:46,000 --> 00:11:47,050
isn't such a good idea of

337
00:11:48,011 --> 00:11:49,065
selecting your model using the

338
00:11:49,075 --> 00:11:51,019
test set and they're using

339
00:11:51,058 --> 00:11:52,063
the same test set to report

340
00:11:53,028 --> 00:11:55,012
the error, as though selecting

341
00:11:56,001 --> 00:11:57,041
your degree of polynomial on the

342
00:11:57,050 --> 00:11:58,069
test set, and then reporting

343
00:11:59,022 --> 00:12:00,048
the error on the test set as

344
00:12:00,063 --> 00:12:02,012
though that were good estimate of generalization error.

345
00:12:03,044 --> 00:12:05,050
That sort of practice is unfortunately many

346
00:12:05,073 --> 00:12:06,067
people do do it; and

347
00:12:06,088 --> 00:12:08,007
if you have a massive massive

348
00:12:08,046 --> 00:12:09,048
test set is maybe not

349
00:12:09,070 --> 00:12:11,030
a terrible thing to do, but

350
00:12:13,005 --> 00:12:15,014
most practitioners of machine

351
00:12:15,044 --> 00:12:16,054
learning tend to advise

352
00:12:19,033 --> 00:12:20,011
against that

353
00:12:20,033 --> 00:12:22,036
and is considered better practice to have separate training validations of test sets. I'll just warn you that just

354
00:12:22,055 --> 00:12:23,087
sometimes people do

355
00:12:24,012 --> 00:12:25,026
you know, use the same data

356
00:12:25,054 --> 00:12:27,027
for the purpose of the validation

357
00:12:28,025 --> 00:12:29,025
set and for the purpose

358
00:12:29,049 --> 00:12:30,023
of the test sets. You only have a training set

359
00:12:30,083 --> 00:12:32,002
and the test set and that's because

360
00:12:32,051 --> 00:12:33,059
that's good practice. So, you

361
00:12:33,080 --> 00:12:34,098
will see some people do it

362
00:12:35,053 --> 00:12:36,080
but if possible I will

363
00:12:37,011 --> 00:12:38,071
recommend against doing.
