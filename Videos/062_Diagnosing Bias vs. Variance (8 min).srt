
1
00:00:00,012 --> 00:00:01,021
If you run the learning algorithm

2
00:00:01,071 --> 00:00:02,064
and it doesn't do as well

3
00:00:02,083 --> 00:00:04,051
as you are hoping, almost all

4
00:00:04,074 --> 00:00:05,067
the time it will be because

5
00:00:06,009 --> 00:00:07,065
you have either a high bias

6
00:00:08,000 --> 00:00:09,052
problem or a high variance problem.

7
00:00:09,085 --> 00:00:10,093
In other words they're either an

8
00:00:11,013 --> 00:00:13,014
underfitting problem or an overfitting problem.

9
00:00:14,025 --> 00:00:15,008
And in this case it's very

10
00:00:15,034 --> 00:00:16,057
important to figure out

11
00:00:16,078 --> 00:00:17,096
which of these two problems is

12
00:00:18,028 --> 00:00:19,050
bias or variance or a bit of both that you

13
00:00:20,021 --> 00:00:20,042
actually have.

14
00:00:21,005 --> 00:00:21,098
Because knowing which of these

15
00:00:22,044 --> 00:00:23,089
two things is happening would give

16
00:00:24,005 --> 00:00:25,094
a very strong indicator for whether

17
00:00:26,017 --> 00:00:27,048
the useful and promising ways

18
00:00:27,076 --> 00:00:29,003
to try to improve your algorithm.

19
00:00:30,023 --> 00:00:31,026
In this video, I would like

20
00:00:31,037 --> 00:00:33,003
to delve more deeply into

21
00:00:33,021 --> 00:00:34,085
this bias and various issue and

22
00:00:35,017 --> 00:00:36,053
understand them better as well

23
00:00:36,078 --> 00:00:38,046
as figure out how to look

24
00:00:38,060 --> 00:00:42,090
at and evaluate knows whether or not we might have a bias problem or a variance problem.

25
00:00:43,003 --> 00:00:45,075
Since this would be critical to

26
00:00:45,089 --> 00:00:48,017
figuring out how to improve the performance of learning algorithm that you implement.

27
00:00:48,064 --> 00:00:52,027
So you've already

28
00:00:52,067 --> 00:00:53,068
seen this figure a few times,

29
00:00:54,018 --> 00:00:55,022
where if you fit two simple

30
00:00:55,071 --> 00:00:57,089
hypothesis, like a straight line that that underfits the data.

31
00:00:59,065 --> 00:01:00,071
If you fit a two complex

32
00:01:01,025 --> 00:01:02,086
hypothesis, then that might

33
00:01:03,039 --> 00:01:05,004
fit the training set perfectly but

34
00:01:05,026 --> 00:01:06,081
overfit the data and this

35
00:01:06,093 --> 00:01:09,000
may be hypothesis of some

36
00:01:09,034 --> 00:01:11,000
intermediate level of complexity,

37
00:01:11,081 --> 00:01:13,012
of some, maybe degree two

38
00:01:13,039 --> 00:01:15,076
polynomials are not too low and not too high degree.

39
00:01:16,056 --> 00:01:17,034
That's just right.

40
00:01:17,056 --> 00:01:18,048
And gives you the best

41
00:01:19,009 --> 00:01:20,073
generalization error out of these options.

42
00:01:21,076 --> 00:01:22,095
Now that we're armed with the

43
00:01:23,003 --> 00:01:25,012
notion of training and validation

44
00:01:26,009 --> 00:01:27,054
in test sets, we can understand

45
00:01:28,029 --> 00:01:30,053
the concepts of bias and variance a little bit better.

46
00:01:31,031 --> 00:01:33,014
Concretely, let our

47
00:01:33,037 --> 00:01:34,092
training error and cross

48
00:01:35,004 --> 00:01:36,062
validation error be defined as

49
00:01:36,084 --> 00:01:38,043
in the previous videos, just say,

50
00:01:38,068 --> 00:01:40,010
the squared error, the average

51
00:01:40,045 --> 00:01:41,042
squared error as measured

52
00:01:41,082 --> 00:01:42,081
on the 20 sets or as

53
00:01:42,093 --> 00:01:44,070
measured on the cross validation set.

54
00:01:46,056 --> 00:01:47,068
Now let's plot the following figure.

55
00:01:48,046 --> 00:01:49,093
On the horizontal axis I am

56
00:01:50,001 --> 00:01:52,000
going to plot the degree of polynomial,

57
00:01:52,040 --> 00:01:53,037
so as I go the right

58
00:01:54,081 --> 00:01:57,004
I'm going to be fitting higher and higher order polynomials.

59
00:01:58,059 --> 00:01:59,062
So, we'll do that for this

60
00:01:59,081 --> 00:02:01,009
figure, where maybe d equals 1,

61
00:02:01,071 --> 00:02:02,076
were going to be fitting

62
00:02:03,068 --> 00:02:05,059
very simple functions where as

63
00:02:05,073 --> 00:02:06,068
we are the right of this

64
00:02:07,015 --> 00:02:08,094
this may be

65
00:02:09,074 --> 00:02:11,055
d equals 4 or relatively may

66
00:02:11,065 --> 00:02:13,040
be even larger numbers. I'm going to be fitting

67
00:02:14,012 --> 00:02:17,002
very complex high order polynomials that

68
00:02:17,041 --> 00:02:19,097
might fit the training set with much more complex functions

69
00:02:23,055 --> 00:02:26,043
whereas we're

70
00:02:26,088 --> 00:02:27,097
here on the right of the

71
00:02:28,015 --> 00:02:31,025
horizontal axis, I have much larger values of these

72
00:02:31,072 --> 00:02:34,034
of a much higher degree polynomial, and

73
00:02:34,046 --> 00:02:35,056
so here that is going

74
00:02:35,059 --> 00:02:37,049
to correspond to fitting much

75
00:02:37,075 --> 00:02:39,081
more complex functions to your

76
00:02:40,011 --> 00:02:41,091
training set.
Let's look at

77
00:02:42,000 --> 00:02:44,006
the training error and cause-validation error

78
00:02:44,040 --> 00:02:45,061
and plot them on this figure.

79
00:02:46,056 --> 00:02:49,008
Let's start with the training error.

80
00:02:49,081 --> 00:02:50,056
As we increase the degree of the

81
00:02:50,068 --> 00:02:52,021
polynomial, we're going to

82
00:02:53,025 --> 00:02:55,062
fit our training set better and better and so, if d equals 1

83
00:02:55,080 --> 00:02:57,028


84
00:02:57,031 --> 00:02:58,030
that ever rose to the high training error.

85
00:02:58,043 --> 00:02:59,018
If we have a

86
00:02:59,019 --> 00:03:00,040
very high degree of

87
00:03:00,081 --> 00:03:02,058
polynomial, our training error is going to be really low.

88
00:03:02,084 --> 00:03:05,022
Maybe even zero, because it will fit the training set really well.

89
00:03:05,084 --> 00:03:06,090
And so as we increase

90
00:03:07,038 --> 00:03:08,075
of the greater polynomial we find

91
00:03:09,012 --> 00:03:10,015
typically that the training

92
00:03:10,055 --> 00:03:11,083
error decreases, so I'm

93
00:03:11,096 --> 00:03:15,021
going to write j subscript

94
00:03:15,097 --> 00:03:17,091
train of theta there, because

95
00:03:18,021 --> 00:03:19,062
our training error tends to

96
00:03:19,075 --> 00:03:22,037
decrease with the degree

97
00:03:22,078 --> 00:03:25,018
of the polynomial that we fit to the data.

98
00:03:25,040 --> 00:03:28,024
Next, let's look at the cross validation error. Often that matter, if

99
00:03:28,030 --> 00:03:30,068
we look at the test set error

100
00:03:31,047 --> 00:03:32,093
we'll get a pretty similar result as

101
00:03:33,050 --> 00:03:34,071
if we were to plot the

102
00:03:36,071 --> 00:03:39,078
cross validation error. So, we know that if d equals 1, we're fitting

103
00:03:40,062 --> 00:03:42,015
a very simple function, and

104
00:03:42,034 --> 00:03:44,040
so we may be underfitting the

105
00:03:44,053 --> 00:03:45,062
training set, and so we're

106
00:03:45,071 --> 00:03:47,025
going to go very high cross-validation error.

107
00:03:47,038 --> 00:03:49,062
If we fit, you

108
00:03:49,068 --> 00:03:52,002
know, an intermediate degree polynomial; we

109
00:03:52,011 --> 00:03:53,062
have a d equals 2 in our

110
00:03:54,009 --> 00:03:55,000
example in the previous slide,

111
00:03:55,038 --> 00:03:56,009
we are going to have a

112
00:03:56,025 --> 00:03:57,043
much lower cross-validation error, because

113
00:03:57,056 --> 00:03:59,046
we are just fitting, finding

114
00:03:59,086 --> 00:04:01,005
a much better fit to the data.

115
00:04:02,016 --> 00:04:03,022
And conversely if d were

116
00:04:03,034 --> 00:04:04,031
too high, so if d

117
00:04:04,053 --> 00:04:05,099
took on say a value of

118
00:04:06,028 --> 00:04:07,031
four, then we're again

119
00:04:07,072 --> 00:04:08,080
overfitting and so we

120
00:04:08,094 --> 00:04:11,003
end up with a high value for cross-validation error.

121
00:04:12,028 --> 00:04:13,056
So if you were to vary

122
00:04:13,090 --> 00:04:15,018
this smoothly and plot a

123
00:04:15,038 --> 00:04:16,038
curve you might end up

124
00:04:17,004 --> 00:04:18,057
with a curve like that, where

125
00:04:19,020 --> 00:04:21,022
that's Jcv of theta,

126
00:04:21,068 --> 00:04:23,024
and again if you plot j

127
00:04:23,045 --> 00:04:25,081
test of theta you get something very similar.

128
00:04:27,012 --> 00:04:28,022
And so this sort of

129
00:04:28,052 --> 00:04:30,011
plot also helps us

130
00:04:30,052 --> 00:04:32,000
to better understand the notions

131
00:04:32,056 --> 00:04:34,075
of bias and variance. Concretely, if you

132
00:04:35,067 --> 00:04:37,000
have a learning algorithm that's

133
00:04:37,024 --> 00:04:38,082
not performing as well as

134
00:04:39,006 --> 00:04:40,066
you wanted it to, how

135
00:04:41,006 --> 00:04:43,042
can you figure out if your learning algorithm is suffering.

136
00:04:44,092 --> 00:04:46,055
Concretly, suppose you have applied a

137
00:04:46,077 --> 00:04:48,012
learning algorithm and it is

138
00:04:48,025 --> 00:04:49,063
not performing as well

139
00:04:49,093 --> 00:04:52,000
as your are hoping, so your

140
00:04:52,024 --> 00:04:54,093
cross-validation set error or your test set error is high.

141
00:04:55,095 --> 00:04:56,091
How can we figure out if

142
00:04:56,094 --> 00:04:58,025
the learning algorithm is suffering

143
00:04:58,057 --> 00:05:01,006
from high bias or if it is suffering from high variance.

144
00:05:02,057 --> 00:05:03,025
So the setting of a cross-validation

145
00:05:04,013 --> 00:05:06,032
error being high corresponds to

146
00:05:07,014 --> 00:05:09,012
either this regime or this regime.

147
00:05:10,047 --> 00:05:11,056
So this regime on the

148
00:05:11,070 --> 00:05:13,055
left corresponds to a

149
00:05:13,075 --> 00:05:15,018
high bias problem, that is,

150
00:05:15,068 --> 00:05:17,004
if you are fitting an overly

151
00:05:17,056 --> 00:05:19,020
low order polynomial such as

152
00:05:19,027 --> 00:05:21,000
a plus one, when we

153
00:05:21,017 --> 00:05:23,075
really needed a higher order polynomial to fit the data.

154
00:05:24,070 --> 00:05:26,037
Whereas in contrast, this regime

155
00:05:26,085 --> 00:05:28,094
corresponds to a high variance problem.

156
00:05:29,083 --> 00:05:31,027
That is, if d--the degree of polynomial--was

157
00:05:32,081 --> 00:05:35,006
too large for the data set that we have.

158
00:05:35,099 --> 00:05:37,025
And this figure gives us

159
00:05:37,074 --> 00:05:39,099
a clue for how to distinguish between these two cases.

160
00:05:41,027 --> 00:05:42,073
Concretely, for the high

161
00:05:43,013 --> 00:05:45,056
bias case, that is,

162
00:05:45,097 --> 00:05:47,047
the case of under fitting, what

163
00:05:47,075 --> 00:05:49,017
we find is that both

164
00:05:50,023 --> 00:05:51,083
the cross validation error and

165
00:05:52,020 --> 00:05:54,022
the training error are going to be high.

166
00:05:54,099 --> 00:05:55,075
So, if your algorithm is

167
00:05:56,022 --> 00:05:57,041
suffering from a bias problem,

168
00:05:59,055 --> 00:06:01,044
the training set error

169
00:06:03,007 --> 00:06:05,097
would be high and you

170
00:06:06,006 --> 00:06:07,051
may find that the cross

171
00:06:07,087 --> 00:06:11,014
validation error will also be high.

172
00:06:11,068 --> 00:06:14,045
It might be close, maybe

173
00:06:14,069 --> 00:06:16,025
just slightly higher then a training error.

174
00:06:17,010 --> 00:06:18,000
And so, if you see this combination,

175
00:06:19,024 --> 00:06:20,050
that's a sign that your algorithm

176
00:06:21,000 --> 00:06:22,018
may be suffering from high bias.

177
00:06:23,041 --> 00:06:25,075
In contrast; if

178
00:06:25,085 --> 00:06:26,093
your algorithm is suffering from high

179
00:06:27,020 --> 00:06:29,072
variance; then, if you look here,

180
00:06:30,070 --> 00:06:33,050
we'll notice that, J

181
00:06:33,073 --> 00:06:34,079
train, that is the training

182
00:06:35,031 --> 00:06:37,022
error, is going to be low.

183
00:06:39,048 --> 00:06:41,081
That is, you're fitting the training set very well.

184
00:06:43,020 --> 00:06:47,054
Whereas, your cross validation error, assuming

185
00:06:48,027 --> 00:06:49,054
that this say the squared

186
00:06:50,029 --> 00:06:51,031
error which we're trying to minimize.

187
00:06:51,066 --> 00:06:53,079
Whereas in contrast; your

188
00:06:53,099 --> 00:06:54,093
error on a cross validation

189
00:06:55,063 --> 00:06:56,085
set or your cross function like cross

190
00:06:57,012 --> 00:06:58,060
validation set, will be

191
00:06:58,075 --> 00:07:01,041
much bigger than your training set error.

192
00:07:02,086 --> 00:07:03,091
This double greater than sign,

193
00:07:04,068 --> 00:07:06,083
here, it means much bigger than, all right. So, it's much greater than to multiply great to great.

194
00:07:07,035 --> 00:07:07,088


195
00:07:10,048 --> 00:07:11,082
So this is a double greater

196
00:07:12,011 --> 00:07:13,012
than sign, that is the

197
00:07:13,026 --> 00:07:14,060
map symbol for much greater

198
00:07:14,091 --> 00:07:16,098
than denoted by two greater than signs.

199
00:07:18,050 --> 00:07:19,039
And so if you see this

200
00:07:19,057 --> 00:07:21,039
combination, then what you

201
00:07:21,055 --> 00:07:29,033
find. And so if you see this combination of values, then

202
00:07:29,057 --> 00:07:31,018
that is a clue that

203
00:07:31,039 --> 00:07:32,093
your learning algorithm may be suffering

204
00:07:33,036 --> 00:07:35,018
from high variance and might be overfitting.

205
00:07:36,037 --> 00:07:37,091
And the key that distinguishes these two

206
00:07:38,006 --> 00:07:39,031
cases is if you

207
00:07:39,041 --> 00:07:41,038
have a high bias problem your

208
00:07:41,052 --> 00:07:42,075
training set error will also

209
00:07:42,095 --> 00:07:43,087
be high as your

210
00:07:44,005 --> 00:07:45,081
hypothesis just not fitting the training set well.

211
00:07:46,093 --> 00:07:47,081
And if you have a high

212
00:07:47,093 --> 00:07:49,036
variance problem, your training

213
00:07:49,077 --> 00:07:51,007
set error will usually be low,

214
00:07:51,036 --> 00:07:53,073
that is much lower than the cross validation error.

215
00:07:55,077 --> 00:07:57,000
So, hopefully that gives you

216
00:07:57,010 --> 00:07:58,083
a somewhat better understanding of the

217
00:07:58,091 --> 00:08:00,039
two problems of bias and variance.

218
00:08:01,027 --> 00:08:02,018
I still have a lot more

219
00:08:02,036 --> 00:08:04,062
to say about bias and variance in the next few videos.

220
00:08:05,041 --> 00:08:06,058
But what we will see later; is

221
00:08:06,083 --> 00:08:08,045
that by diagnosing, whether a learning

222
00:08:08,051 --> 00:08:11,000
algorithm may be suffering from high bias or a high variance.

223
00:08:11,089 --> 00:08:14,070
I'll show you even more details on how to do that in later videos.

224
00:08:15,060 --> 00:08:16,087
We'll see that by figuring out

225
00:08:17,016 --> 00:08:18,056
whether a learning algorithm may be

226
00:08:18,074 --> 00:08:20,027
suffering from high bias or

227
00:08:20,075 --> 00:08:22,037
a combination of both that

228
00:08:22,052 --> 00:08:23,033
that would give us much better

229
00:08:23,051 --> 00:08:24,067
guidance for what might be

230
00:08:24,079 --> 00:08:25,093
promising things to try

231
00:08:26,012 --> 00:08:28,018
in order to improve the performance of the learning algorithm.
