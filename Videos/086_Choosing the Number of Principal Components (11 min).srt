
1
00:00:00,009 --> 00:00:01,056
In the PCA algorithm we take

2
00:00:01,098 --> 00:00:03,052
N dimensional features and reduce

3
00:00:03,097 --> 00:00:06,025
them to some K dimensional feature representation.

4
00:00:07,062 --> 00:00:09,008
This number K is a parameter

5
00:00:09,082 --> 00:00:10,080
of the PCA algorithm.

6
00:00:11,081 --> 00:00:13,024
This number K is also called

7
00:00:13,061 --> 00:00:15,008
the number of principle components

8
00:00:15,083 --> 00:00:17,048
or the number of principle components that we've retained.

9
00:00:18,053 --> 00:00:19,064
And in this video I'd like

10
00:00:19,080 --> 00:00:20,085
to give you some guidelines,

11
00:00:21,073 --> 00:00:23,008
tell you about how people

12
00:00:23,042 --> 00:00:24,048
tend to think about how to

13
00:00:24,060 --> 00:00:26,073
choose this parameter K for PCA.

14
00:00:28,064 --> 00:00:29,067
In order to choose k,

15
00:00:30,010 --> 00:00:30,098
that is to choose the number

16
00:00:31,035 --> 00:00:34,010
of principal components, here are a couple of useful concepts.

17
00:00:36,042 --> 00:00:37,052
What PCA tries to do

18
00:00:37,075 --> 00:00:38,075
is it tries to minimize

19
00:00:40,007 --> 00:00:41,050
the average squared projection error.

20
00:00:42,003 --> 00:00:43,020
So it tries to minimize

21
00:00:43,042 --> 00:00:45,047
this quantity, which I'm writing down,

22
00:00:46,040 --> 00:00:47,097
which is the difference between the

23
00:00:48,014 --> 00:00:50,000
original data X and the

24
00:00:50,068 --> 00:00:53,046
projected version, X-approx-i, which

25
00:00:53,061 --> 00:00:54,092
was defined last video, so

26
00:00:55,002 --> 00:00:55,089
it tries to minimize the squared

27
00:00:56,015 --> 00:00:57,035
distance between x and it's projection

28
00:00:58,032 --> 00:00:59,075
onto that lower dimensional surface.

29
00:01:01,021 --> 00:01:02,099
So that's the average square projection error.

30
00:01:03,099 --> 00:01:05,031
Also let me define the

31
00:01:05,043 --> 00:01:07,001
total variation in the

32
00:01:07,009 --> 00:01:08,073
data to be the

33
00:01:09,001 --> 00:01:11,073
average length squared of

34
00:01:12,014 --> 00:01:14,012
these examples Xi

35
00:01:14,045 --> 00:01:16,001
so the total variation in the

36
00:01:16,026 --> 00:01:17,093
data is the average of

37
00:01:18,006 --> 00:01:19,025
my training sets of the

38
00:01:19,037 --> 00:01:21,064
length of each of my training examples.

39
00:01:22,018 --> 00:01:23,068
And this one says, "On average, how

40
00:01:23,093 --> 00:01:24,084
far are my training examples

41
00:01:25,068 --> 00:01:27,095
from the vector, from just being all zeros?"

42
00:01:28,076 --> 00:01:30,045
How far is, how far

43
00:01:30,081 --> 00:01:32,081
on average are my training examples from the origin?

44
00:01:33,051 --> 00:01:34,045
When we're trying to choose k, a

45
00:01:35,087 --> 00:01:37,020
pretty common rule of thumb

46
00:01:37,040 --> 00:01:38,062
for choosing k is to choose

47
00:01:38,079 --> 00:01:40,029
the smaller values so that

48
00:01:40,098 --> 00:01:43,081
the ratio between these is less than 0.01.

49
00:01:44,054 --> 00:01:45,054
So in other words,

50
00:01:46,034 --> 00:01:47,037
a pretty common way to

51
00:01:47,051 --> 00:01:48,045
think about how we choose k

52
00:01:48,079 --> 00:01:51,018
is we want the average squared projection error.

53
00:01:51,057 --> 00:01:54,070
That is the average distance

54
00:01:55,023 --> 00:01:56,034
between x and it's projections

55
00:01:57,056 --> 00:02:00,032
divided by the total variation of the data.

56
00:02:00,079 --> 00:02:01,087
That is how much the data varies.

57
00:02:02,093 --> 00:02:04,006
We want this ratio to be

58
00:02:04,023 --> 00:02:06,076
less than, let's say, 0.01.

59
00:02:06,082 --> 00:02:09,044
Or to be less than 1%, which is another way of thinking about it.

60
00:02:10,086 --> 00:02:11,093
And the way most people think

61
00:02:12,015 --> 00:02:13,063
about choosing K is rather

62
00:02:13,086 --> 00:02:15,065
than choosing K directly the

63
00:02:15,088 --> 00:02:17,011
way most people talk about

64
00:02:17,047 --> 00:02:18,093
it is as what this

65
00:02:19,015 --> 00:02:20,062
number is, whether it is 0.01

66
00:02:20,074 --> 00:02:23,033
or some other number.

67
00:02:23,071 --> 00:02:25,031
And if it is 0.01, another way

68
00:02:25,049 --> 00:02:27,002
to say this to use the

69
00:02:27,027 --> 00:02:30,012
language of PCA is that 99% of the variance is retained.

70
00:02:32,006 --> 00:02:33,047
I don't really want to, don't

71
00:02:33,084 --> 00:02:34,081
worry about what this phrase

72
00:02:35,013 --> 00:02:36,091
really means technically but this

73
00:02:37,083 --> 00:02:39,016
phrase "99% of variance is retained" just means

74
00:02:39,041 --> 00:02:41,071
that this quantity on the left is less than 0.01.

75
00:02:42,034 --> 00:02:43,090
And so, if you

76
00:02:44,093 --> 00:02:46,050
are using PCA and if you want

77
00:02:46,062 --> 00:02:47,072
to tell someone, you know,

78
00:02:48,021 --> 00:02:49,086
how many principle components you've

79
00:02:49,097 --> 00:02:51,008
retained it would be

80
00:02:51,013 --> 00:02:52,036
more common to say well, I

81
00:02:52,044 --> 00:02:55,036
chose k so that 99% of the variance was retained.

82
00:02:55,099 --> 00:02:56,096
And that's kind of a useful thing

83
00:02:57,065 --> 00:02:58,053
to know, it means that you

84
00:02:58,062 --> 00:02:59,081
know, the average squared projection

85
00:03:00,075 --> 00:03:01,071
error divided by the total

86
00:03:01,090 --> 00:03:03,025
variation that was at most 1%.

87
00:03:03,081 --> 00:03:04,077
That's kind of an insightful

88
00:03:05,027 --> 00:03:06,078
thing to think about, whereas if

89
00:03:06,091 --> 00:03:08,041
you tell someone that, "Well I

90
00:03:09,016 --> 00:03:10,071
had to 100 principle

91
00:03:10,088 --> 00:03:12,003
components" or "k was equal

92
00:03:12,071 --> 00:03:13,084
to 100 in a thousand dimensional

93
00:03:14,021 --> 00:03:15,034
data" it's a little

94
00:03:15,041 --> 00:03:16,059
hard for people to interpret

95
00:03:19,009 --> 00:03:19,009
that.

96
00:03:19,031 --> 00:03:22,021
So this number 0.01 is what people often use.

97
00:03:23,006 --> 00:03:25,037
Other common values is 0.05,

98
00:03:26,084 --> 00:03:27,081
and so this would be 5%,

99
00:03:27,099 --> 00:03:28,087
and if you do that then

100
00:03:29,021 --> 00:03:30,038
you go and say well 95%

101
00:03:30,074 --> 00:03:32,031
of the variance is

102
00:03:32,047 --> 00:03:34,028
retained and, you know

103
00:03:34,069 --> 00:03:36,071
other numbers maybe 90% of the variance is

104
00:03:37,097 --> 00:03:40,003
retained, maybe as low as 85%.

105
00:03:40,015 --> 00:03:42,040
So 90% would correspond to say

106
00:03:44,034 --> 00:03:46,094
0.10, kinda 10%.

107
00:03:47,025 --> 00:03:49,015
And so range of values

108
00:03:49,090 --> 00:03:50,077
from, you know, 90, 95,

109
00:03:50,087 --> 00:03:51,046
99, maybe as low as 85% of

110
00:03:51,050 --> 00:03:55,009
the variables contained would be a fairly typical range in values.

111
00:03:55,078 --> 00:03:56,090
Maybe 95 to 99

112
00:03:57,068 --> 00:03:58,081
is really the most

113
00:03:59,002 --> 00:04:02,008
common range of values that people use.

114
00:04:02,012 --> 00:04:02,094
For many data sets you'd be

115
00:04:03,000 --> 00:04:04,031
surprised, in order to retain

116
00:04:04,078 --> 00:04:06,059
99% of the variance, you can

117
00:04:06,078 --> 00:04:08,015
often reduce the dimension of

118
00:04:08,019 --> 00:04:11,081
the data significantly and still retain most of the variance.

119
00:04:12,043 --> 00:04:13,040
Because for most real life

120
00:04:13,056 --> 00:04:15,021
data says many features are

121
00:04:15,028 --> 00:04:17,006
just highly correlated, and so

122
00:04:17,031 --> 00:04:17,093
it turns out to be possible

123
00:04:18,049 --> 00:04:19,054
to compress the data a

124
00:04:19,061 --> 00:04:20,099
lot and still retain you

125
00:04:21,036 --> 00:04:22,031
know 99% of the variance

126
00:04:22,052 --> 00:04:26,025
or 95% of the variance. So how do you implement this?

127
00:04:26,081 --> 00:04:28,061
Well, here's one algorithm that you might use.

128
00:04:28,088 --> 00:04:30,036
You may start off, if you

129
00:04:30,054 --> 00:04:31,036
want to choose the value of

130
00:04:31,047 --> 00:04:33,050
k, we might start off with k equals 1.

131
00:04:33,055 --> 00:04:34,067
And then we run through PCA.

132
00:04:35,035 --> 00:04:36,043
You know, so we compute, you

133
00:04:36,056 --> 00:04:38,087
reduce, compute z1, z2, up to zm.

134
00:04:39,051 --> 00:04:40,079
Compute all of those x1

135
00:04:41,008 --> 00:04:42,054
approx and so on up to xm approx

136
00:04:43,019 --> 00:04:45,011
and then we check if 99% of the variance is retained.

137
00:04:47,013 --> 00:04:48,088
Then we're good and we use k equals 1.

138
00:04:49,001 --> 00:04:51,095
But if it isn't then what we'll do we'll next try K equals 2.

139
00:04:52,062 --> 00:04:53,081
And then we'll again

140
00:04:54,019 --> 00:04:56,006
run through this entire procedure and

141
00:04:56,017 --> 00:04:57,076
check, you know is this expression satisfied.

142
00:04:58,047 --> 00:05:00,098
Is this less than 0.01. And if not then we do this again.

143
00:05:01,022 --> 00:05:03,006
Let's try k equals 3,

144
00:05:03,031 --> 00:05:04,091
then try k equals 4,

145
00:05:04,097 --> 00:05:06,025
and so on until maybe

146
00:05:06,062 --> 00:05:07,073
we get up to k equals

147
00:05:08,006 --> 00:05:09,004
17 and we find 99% of

148
00:05:09,008 --> 00:05:13,006
the data have is retained and then

149
00:05:14,012 --> 00:05:15,011
we use k equals 17, right?

150
00:05:15,056 --> 00:05:17,016
That is one way

151
00:05:17,024 --> 00:05:18,079
to choose the smallest value

152
00:05:19,012 --> 00:05:20,092
of k, so that and 99% of the variance is retained.

153
00:05:22,037 --> 00:05:23,043
But as you can imagine,

154
00:05:23,055 --> 00:05:25,013
this procedure seems horribly inefficient

155
00:05:26,020 --> 00:05:28,012
we're trying k equals one, k equals two, we're doing all these calculations.

156
00:05:29,057 --> 00:05:30,054
Fortunately when you implement

157
00:05:31,012 --> 00:05:33,050
PCA it actually, in

158
00:05:33,095 --> 00:05:35,052
this step, it actually gives us

159
00:05:35,091 --> 00:05:37,007
a quantity that makes it

160
00:05:37,031 --> 00:05:40,016
much easier to compute these things as well.

161
00:05:41,011 --> 00:05:42,016
Specifically when you're calling

162
00:05:42,081 --> 00:05:44,012
SVD to get these

163
00:05:44,033 --> 00:05:45,055
matrices u, s, and d,

164
00:05:45,061 --> 00:05:46,077
when you're calling usvd on the

165
00:05:47,004 --> 00:05:48,056
covariance matrix sigma, it also

166
00:05:48,086 --> 00:05:49,077
gives us back this matrix

167
00:05:50,030 --> 00:05:52,017
S and what

168
00:05:52,036 --> 00:05:53,043
S is, is going to

169
00:05:53,051 --> 00:05:56,079
be a square matrix an N by N matrix in fact,

170
00:05:57,063 --> 00:05:58,008
that is

171
00:05:58,029 --> 00:05:58,029
diagonal.

172
00:05:58,082 --> 00:06:00,037
So is diagonal entries s one

173
00:06:00,054 --> 00:06:01,063
one, s two two, s

174
00:06:01,098 --> 00:06:03,024
three three down to s

175
00:06:03,058 --> 00:06:05,012
n n are going to

176
00:06:05,025 --> 00:06:07,000
be the only non-zero elements of

177
00:06:07,012 --> 00:06:08,087
this matrix, and everything off

178
00:06:09,006 --> 00:06:11,047
the diagonals is going to be zero.

179
00:06:11,058 --> 00:06:11,058
Okay?

180
00:06:11,067 --> 00:06:12,052
So those big O's that I'm drawing,

181
00:06:13,033 --> 00:06:14,025
by that what I mean is

182
00:06:14,074 --> 00:06:16,032
that everything off the diagonal

183
00:06:17,012 --> 00:06:18,022
of this matrix all of those

184
00:06:18,048 --> 00:06:20,031
entries there are going to be zeros.

185
00:06:22,030 --> 00:06:23,079
And so, what is possible

186
00:06:24,018 --> 00:06:25,025
to show, and I won't prove

187
00:06:25,048 --> 00:06:26,037
this here, and it turns out

188
00:06:26,062 --> 00:06:27,087
that for a given value of

189
00:06:27,098 --> 00:06:29,092
k, this quantity

190
00:06:30,058 --> 00:06:37,081
over here can be computed much more simply.

191
00:06:38,080 --> 00:06:40,031
And that quantity can be computed

192
00:06:41,000 --> 00:06:42,089
as one minus sum from

193
00:06:43,012 --> 00:06:44,039
i equals 1 through

194
00:06:44,061 --> 00:06:47,095
K of Sii divided by

195
00:06:48,063 --> 00:06:50,005
sum from I equals 1

196
00:06:50,017 --> 00:06:52,000
through N of Sii.

197
00:06:53,036 --> 00:06:54,081
So just to say that it words, or

198
00:06:55,000 --> 00:06:56,017
just to take another

199
00:06:56,044 --> 00:06:57,032
view of how to explain that,

200
00:06:57,095 --> 00:06:59,037
if K equals 3 let's say.

201
00:07:00,081 --> 00:07:01,097
What we're going to do to

202
00:07:02,007 --> 00:07:03,019
compute the numerator is sum

203
00:07:03,033 --> 00:07:04,068
from one--  I equals 1

204
00:07:04,081 --> 00:07:05,082
through 3 of of Sii, so just

205
00:07:06,024 --> 00:07:08,017
compute the sum of these first three elements.

206
00:07:09,027 --> 00:07:09,070
So that's the numerator.

207
00:07:10,098 --> 00:07:12,087
And then for the denominator, well that's

208
00:07:13,008 --> 00:07:14,097
the sum of all of these diagonal entries.

209
00:07:16,020 --> 00:07:17,047
And one minus the ratio of

210
00:07:17,066 --> 00:07:19,007
that, that gives me this

211
00:07:19,030 --> 00:07:21,032
quantity over here, that I've

212
00:07:21,064 --> 00:07:23,043
circled in blue.

213
00:07:23,064 --> 00:07:24,037
And so, what we can do

214
00:07:24,064 --> 00:07:26,000
is just test if this

215
00:07:26,043 --> 00:07:29,032
is less than or equal to 0.01.

216
00:07:29,037 --> 00:07:30,045
Or equivalently, we can test

217
00:07:30,082 --> 00:07:31,095
if the sum from

218
00:07:32,018 --> 00:07:33,000
i equals 1 through k, s-i-i

219
00:07:33,097 --> 00:07:35,018
divided by sum from i

220
00:07:35,031 --> 00:07:37,008
equals 1 through n, s-i-i

221
00:07:37,064 --> 00:07:38,057
if this is greater than or

222
00:07:38,076 --> 00:07:40,060
equal to 4.99, if you

223
00:07:40,072 --> 00:07:42,092
want to be sure that 99% of the variance is retained.

224
00:07:44,076 --> 00:07:45,064
And so what you can do

225
00:07:45,093 --> 00:07:48,036
is just slowly increase k,

226
00:07:48,076 --> 00:07:49,081
set k equals one, set k equals

227
00:07:50,010 --> 00:07:51,029
two, set k equals three and so

228
00:07:52,013 --> 00:07:53,024
on, and just test this quantity

229
00:07:54,072 --> 00:07:56,012
to see what is the

230
00:07:56,035 --> 00:07:58,081
smallest value of k that ensures that 99% of the variance is retained.

231
00:08:00,060 --> 00:08:01,081
And if you do

232
00:08:02,000 --> 00:08:02,079
this, then you need to call

233
00:08:03,017 --> 00:08:04,066
the SVD function only once.

234
00:08:04,097 --> 00:08:05,082
Because that gives you the

235
00:08:06,000 --> 00:08:07,006
S matrix and once you

236
00:08:07,008 --> 00:08:08,035
have the S matrix, you can

237
00:08:08,049 --> 00:08:09,054
then just keep on doing

238
00:08:09,076 --> 00:08:11,037
this calculation by increasing

239
00:08:11,093 --> 00:08:12,091
the value of K in the

240
00:08:13,006 --> 00:08:14,044
numerator and so you

241
00:08:14,056 --> 00:08:16,029
don't need keep to calling SVD over

242
00:08:16,054 --> 00:08:18,062
and over again to test out the different values of K.

243
00:08:18,091 --> 00:08:20,002
So this procedure is much more

244
00:08:20,014 --> 00:08:21,069
efficient, and this can

245
00:08:21,091 --> 00:08:24,001
allow you to select the

246
00:08:24,008 --> 00:08:25,088
value of K without needing

247
00:08:26,025 --> 00:08:27,062
to run PCA from scratch

248
00:08:28,002 --> 00:08:30,064
over and over. You just run SVD once, this

249
00:08:30,085 --> 00:08:32,035
gives you all of these diagonal numbers,

250
00:08:32,077 --> 00:08:35,009
all of these numbers S11, S22 down to SNN,

251
00:08:35,077 --> 00:08:36,082
and then you can

252
00:08:36,091 --> 00:08:38,044
just you know, vary K

253
00:08:38,073 --> 00:08:40,074
in this expression to find

254
00:08:41,000 --> 00:08:42,025
the smallest value of K, so

255
00:08:43,013 --> 00:08:44,002
that 99% of the variance is retained.

256
00:08:44,085 --> 00:08:45,087
So to summarize, the way

257
00:08:46,004 --> 00:08:47,085
that I often use, the

258
00:08:47,097 --> 00:08:49,004
way that I often choose K

259
00:08:49,041 --> 00:08:50,078
when I am using PCA for compression

260
00:08:51,012 --> 00:08:52,059
is I would call SVD once

261
00:08:52,095 --> 00:08:54,048
in the covariance matrix, and then

262
00:08:54,053 --> 00:08:55,075
I would use this formula and

263
00:08:56,002 --> 00:08:57,092
pick the smallest value of

264
00:08:58,001 --> 00:09:00,038
K for which this expression is satisfied.

265
00:09:01,058 --> 00:09:02,055
And by the way, even if you

266
00:09:02,064 --> 00:09:03,085
were to pick some different value

267
00:09:04,017 --> 00:09:04,096
of K, even if you were

268
00:09:05,000 --> 00:09:05,091
to pick the value of K

269
00:09:06,009 --> 00:09:07,025
manually, you know maybe you

270
00:09:07,029 --> 00:09:08,062
have a thousand dimensional data

271
00:09:09,053 --> 00:09:11,059
and I just want to choose K equals one hundred.

272
00:09:12,042 --> 00:09:13,045
Then, if you want to explain

273
00:09:13,069 --> 00:09:14,075
to others what you just did,

274
00:09:15,023 --> 00:09:17,007
a good way to explain the performance

275
00:09:17,075 --> 00:09:18,090
of your implementation of PCA to

276
00:09:19,022 --> 00:09:20,029
them, is actually to take

277
00:09:20,053 --> 00:09:21,066
this quantity and compute what

278
00:09:21,088 --> 00:09:23,000
this is, and that will

279
00:09:23,011 --> 00:09:25,076
tell you what was the percentage of variance retained.

280
00:09:26,029 --> 00:09:28,007
And if you report that number, then,

281
00:09:28,034 --> 00:09:29,072
you know, people that are familiar

282
00:09:30,010 --> 00:09:31,061
with PCA, and people can

283
00:09:31,087 --> 00:09:33,001
use this to get a

284
00:09:33,008 --> 00:09:34,055
good understanding of how well

285
00:09:34,089 --> 00:09:37,034
your hundred dimensional representation is

286
00:09:37,069 --> 00:09:39,026
approximating your original data

287
00:09:39,058 --> 00:09:41,029
set, because there's 99% of variance retained.

288
00:09:41,099 --> 00:09:44,013
That's really a measure of your

289
00:09:44,036 --> 00:09:45,086
square of construction error, that

290
00:09:46,024 --> 00:09:47,087
ratio being 0.01, just

291
00:09:48,042 --> 00:09:49,094
gives people a good intuitive

292
00:09:50,042 --> 00:09:51,082
sense of whether your implementation

293
00:09:52,058 --> 00:09:53,084
of PCA is finding a

294
00:09:54,000 --> 00:09:56,052
good approximation of your original data set.

295
00:09:58,044 --> 00:09:59,060
So hopefully, that gives you

296
00:09:59,079 --> 00:10:01,025
an efficient procedure for choosing

297
00:10:01,085 --> 00:10:02,079
the number K. For choosing

298
00:10:03,025 --> 00:10:04,094
what dimension to reduce your

299
00:10:05,015 --> 00:10:06,062
data to, and if

300
00:10:06,075 --> 00:10:07,083
you apply PCA to very

301
00:10:07,097 --> 00:10:09,074
high dimensional data sets, you know, to like

302
00:10:09,099 --> 00:10:11,057
a thousand dimensional data, very often,

303
00:10:11,098 --> 00:10:13,034
just because data sets tend

304
00:10:13,052 --> 00:10:14,072
to have highly correlated

305
00:10:15,007 --> 00:10:16,013
features, this is just a

306
00:10:16,027 --> 00:10:17,019
property of most of the data sets you see,

307
00:10:18,044 --> 00:10:19,041
you often find that PCA

308
00:10:20,003 --> 00:10:21,061
will be able to retain ninety nine

309
00:10:21,084 --> 00:10:22,094
per cent of the variance or say,

310
00:10:23,011 --> 00:10:24,044
ninety five ninety nine, some

311
00:10:24,072 --> 00:10:25,090
high fraction of the variance,

312
00:10:26,036 --> 00:10:27,058
even while compressing the data

313
00:10:28,055 --> 00:10:29,072
by a very large factor.
