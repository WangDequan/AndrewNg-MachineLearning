
1
00:00:00,009 --> 00:00:01,027
I've seen over and over that

2
00:00:01,057 --> 00:00:03,016
one of the most reliable ways to

3
00:00:03,029 --> 00:00:04,079
get a high performance machine learning

4
00:00:05,004 --> 00:00:06,016
system is to take

5
00:00:06,054 --> 00:00:07,086
a low bias learning algorithm

6
00:00:08,075 --> 00:00:10,022
and to train it on a massive training set.

7
00:00:11,023 --> 00:00:12,083
But where did you get so much training data from?

8
00:00:13,050 --> 00:00:14,043
Turns out that the machine earnings

9
00:00:14,082 --> 00:00:16,051
there's a fascinating idea called artificial

10
00:00:17,021 --> 00:00:19,000
data synthesis, this doesn't

11
00:00:19,037 --> 00:00:20,073
apply to every single problem, and

12
00:00:20,098 --> 00:00:22,012
to apply to a specific

13
00:00:22,035 --> 00:00:25,005
problem, often takes some thought and innovation and insight.

14
00:00:25,078 --> 00:00:27,017
But if this idea applies

15
00:00:27,057 --> 00:00:29,012
to your machine, only problem, it

16
00:00:29,023 --> 00:00:30,026
can sometimes be a an

17
00:00:30,051 --> 00:00:31,060
easy way to get a

18
00:00:31,067 --> 00:00:33,046
huge training set to give to your learning algorithm.

19
00:00:33,089 --> 00:00:35,052
The idea of artificial

20
00:00:36,022 --> 00:00:38,040
data synthesis comprises of two

21
00:00:38,059 --> 00:00:40,021
variations, main the first

22
00:00:40,064 --> 00:00:41,093
is if we are essentially creating

23
00:00:42,052 --> 00:00:44,093
data from [xx], creating new data from scratch.

24
00:00:45,038 --> 00:00:46,070
And the second is if

25
00:00:46,092 --> 00:00:48,035
we already have it's small

26
00:00:48,059 --> 00:00:49,096
label training set and we

27
00:00:50,021 --> 00:00:51,049
somehow have amplify that training

28
00:00:51,084 --> 00:00:52,067
set or use a small training

29
00:00:52,097 --> 00:00:54,039
set to turn that into

30
00:00:54,065 --> 00:00:56,028
a larger training set and in

31
00:00:56,045 --> 00:00:58,011
this video we'll go over both those ideas.

32
00:01:00,035 --> 00:01:02,021
To talk about the artificial data

33
00:01:02,043 --> 00:01:04,003
synthesis idea, let's use

34
00:01:04,032 --> 00:01:06,093
the character portion of

35
00:01:07,009 --> 00:01:08,046
the photo OCR pipeline, we

36
00:01:08,068 --> 00:01:09,070
want to take it's input image

37
00:01:10,006 --> 00:01:11,037
and recognize what character it is.

38
00:01:13,032 --> 00:01:14,068
If we go out and collect

39
00:01:14,087 --> 00:01:16,026
a large label data set,

40
00:01:16,089 --> 00:01:17,098
here's what it is and what it look like.

41
00:01:18,057 --> 00:01:21,076
For this particular example, I've chosen a square aspect ratio.

42
00:01:22,012 --> 00:01:23,025
So we're taking square image patches.

43
00:01:24,018 --> 00:01:25,010
And the goal is to take

44
00:01:25,076 --> 00:01:27,042
an image patch and recognize the

45
00:01:27,053 --> 00:01:29,026
character in the middle of that image patch.

46
00:01:31,009 --> 00:01:31,098
And for the sake of simplicity,

47
00:01:32,065 --> 00:01:33,073
I'm going to treat these images

48
00:01:34,023 --> 00:01:36,037
as grey scale images, rather than color images.

49
00:01:36,087 --> 00:01:38,054
It turns out that using color

50
00:01:38,093 --> 00:01:41,018
doesn't seem to help that much for this particular problem.

51
00:01:42,018 --> 00:01:43,053
So given this image patch, we'd

52
00:01:43,065 --> 00:01:44,089
like to recognize that that's a

53
00:01:45,001 --> 00:01:46,023
T. Given this image patch,

54
00:01:46,054 --> 00:01:47,092
we'd like to recognize that it's an 'S'.

55
00:01:49,054 --> 00:01:50,073
Given that image patch we

56
00:01:50,084 --> 00:01:52,095
would like to recognize that as an 'I' and so on.

57
00:01:54,010 --> 00:01:55,031
So all of these, our

58
00:01:55,045 --> 00:01:57,023
examples of row images, how

59
00:01:57,037 --> 00:01:59,045
can we come up with a much larger training set?

60
00:02:00,000 --> 00:02:01,057
Modern computers often have a

61
00:02:01,064 --> 00:02:03,070
huge font library and

62
00:02:03,089 --> 00:02:05,032
if you use a word processing

63
00:02:05,095 --> 00:02:07,009
software, depending on what word

64
00:02:07,023 --> 00:02:08,058
processor you use, you might

65
00:02:08,080 --> 00:02:09,097
have all of these fonts and

66
00:02:10,012 --> 00:02:12,049
many, many more Already stored inside.

67
00:02:12,094 --> 00:02:14,034
And, in fact, if you go different websites, there

68
00:02:14,068 --> 00:02:16,028
are, again, huge, free font

69
00:02:16,068 --> 00:02:18,019
libraries on the internet we

70
00:02:18,037 --> 00:02:19,096
can download many, many different

71
00:02:20,025 --> 00:02:22,058
types of fonts, hundreds or perhaps thousands of different fonts.

72
00:02:23,096 --> 00:02:25,018
So if you want more

73
00:02:25,056 --> 00:02:27,002
training examples, one thing you

74
00:02:27,009 --> 00:02:28,034
can do is just take

75
00:02:28,087 --> 00:02:30,021
characters from different fonts

76
00:02:31,024 --> 00:02:32,087
and paste these characters against

77
00:02:33,028 --> 00:02:35,088
different random backgrounds.

78
00:02:36,072 --> 00:02:39,050
So you might take this ----  and paste that c against a random background.

79
00:02:40,068 --> 00:02:41,063
If you do that you now have

80
00:02:42,006 --> 00:02:43,083
a training example of an

81
00:02:44,008 --> 00:02:45,025
image of the character C.

82
00:02:46,036 --> 00:02:47,050
So after some amount of

83
00:02:47,056 --> 00:02:48,091
work, you know this,

84
00:02:48,097 --> 00:02:49,071
and it is a little bit of

85
00:02:49,083 --> 00:02:51,075
work to synthisize realistic looking data.

86
00:02:52,018 --> 00:02:53,008
But after some amount of work,

87
00:02:53,069 --> 00:02:56,012
you can get a synthetic training set like that.

88
00:02:57,018 --> 00:02:59,090
Every image shown on the right was actually a synthesized image.

89
00:03:00,036 --> 00:03:02,008
Where you take a font,

90
00:03:02,081 --> 00:03:04,024
maybe a random font downloaded off

91
00:03:04,040 --> 00:03:05,068
the web and you paste

92
00:03:06,015 --> 00:03:07,031
an image of one character

93
00:03:07,080 --> 00:03:08,087
or a few characters from that font

94
00:03:09,056 --> 00:03:11,043
against this other random background image.

95
00:03:12,013 --> 00:03:12,084
And then apply maybe a little

96
00:03:13,053 --> 00:03:15,015
blurring operators  -----of app

97
00:03:15,068 --> 00:03:17,037
finder, distortions that app

98
00:03:17,062 --> 00:03:18,065
finder, meaning just the sharing

99
00:03:19,034 --> 00:03:20,074
and scaling and little rotation

100
00:03:21,000 --> 00:03:22,025
operations and if you

101
00:03:22,037 --> 00:03:23,033
do that you get a synthetic

102
00:03:23,058 --> 00:03:25,052
training set, on what the one shown here.

103
00:03:26,050 --> 00:03:28,005
And this is work,

104
00:03:28,053 --> 00:03:29,063
grade, it is, it takes

105
00:03:29,096 --> 00:03:31,046
thought at work, in order to

106
00:03:31,069 --> 00:03:33,025
make the synthetic data look realistic,

107
00:03:34,002 --> 00:03:34,071
and if you do a sloppy

108
00:03:35,012 --> 00:03:36,019
job in terms of how

109
00:03:36,025 --> 00:03:38,090
you create the synthetic data then it actually won't work well.

110
00:03:39,062 --> 00:03:40,059
But if you look at

111
00:03:40,078 --> 00:03:43,093
the synthetic data looks remarkably similar to the real data.

112
00:03:45,012 --> 00:03:46,084
And so by using synthetic data

113
00:03:47,034 --> 00:03:48,055
you have essentially an unlimited

114
00:03:48,099 --> 00:03:50,096
supply of training examples for

115
00:03:51,031 --> 00:03:53,006
artificial training synthesis And

116
00:03:53,015 --> 00:03:54,011
so, if you use this

117
00:03:54,033 --> 00:03:55,081
source synthetic data, you have

118
00:03:56,015 --> 00:03:58,009
essentially unlimited supply of

119
00:03:58,056 --> 00:04:00,000
label data to create

120
00:04:00,013 --> 00:04:01,061
a improvised learning algorithm

121
00:04:02,030 --> 00:04:03,099
for the character recognition problem.

122
00:04:05,012 --> 00:04:06,053
So this is an example of

123
00:04:07,000 --> 00:04:08,050
artificial data synthesis where youre

124
00:04:09,003 --> 00:04:10,087
basically creating new data from

125
00:04:11,008 --> 00:04:13,078
scratch, you just generating brand new images from scratch.

126
00:04:14,087 --> 00:04:16,044
The other main approach to artificial data

127
00:04:16,070 --> 00:04:18,020
synthesis is where you

128
00:04:18,037 --> 00:04:19,061
take a examples that you

129
00:04:19,074 --> 00:04:20,077
currently have, that we take

130
00:04:21,001 --> 00:04:22,043
a real example, maybe from

131
00:04:22,069 --> 00:04:24,012
real image, and you create

132
00:04:24,076 --> 00:04:26,012
additional data, so as to

133
00:04:26,037 --> 00:04:27,089
amplify your training set.

134
00:04:28,006 --> 00:04:28,081
So here is an image of a compared

135
00:04:28,091 --> 00:04:30,049
to a from a real image,

136
00:04:31,041 --> 00:04:32,055
not a synthesized image, and

137
00:04:32,062 --> 00:04:33,079
I have overlayed this with

138
00:04:33,087 --> 00:04:35,075
the grid lines just for the purpose of illustration.

139
00:04:36,043 --> 00:04:36,087
Actually have these ----.

140
00:04:36,097 --> 00:04:39,002
So what you

141
00:04:39,010 --> 00:04:40,011
can do is then take this

142
00:04:40,048 --> 00:04:41,050
alphabet here, take this image

143
00:04:42,024 --> 00:04:43,075
and introduce artificial warpings[sp?]

144
00:04:44,029 --> 00:04:45,081
or artificial distortions into the

145
00:04:46,004 --> 00:04:47,002
image so they can

146
00:04:47,022 --> 00:04:48,024
take the image a and turn

147
00:04:48,043 --> 00:04:50,006
that into 16 new examples.

148
00:04:51,011 --> 00:04:52,000
So in this way you can

149
00:04:52,044 --> 00:04:53,074
take a small label training set

150
00:04:54,008 --> 00:04:55,036
and amplify your training set

151
00:04:56,018 --> 00:04:57,018
to suddenly get a lot

152
00:04:57,030 --> 00:05:00,001
more examples, all of it.

153
00:05:01,020 --> 00:05:02,036
Again, in order to do

154
00:05:02,056 --> 00:05:03,093
this for application, it does

155
00:05:04,012 --> 00:05:05,006
take thought and it does

156
00:05:05,013 --> 00:05:06,026
take insight to figure out

157
00:05:06,043 --> 00:05:07,083
what our reasonable sets of

158
00:05:08,042 --> 00:05:09,045
distortions, or whether these

159
00:05:09,072 --> 00:05:11,000
are ways that amplify and multiply

160
00:05:11,047 --> 00:05:12,075
your training set, and for

161
00:05:13,006 --> 00:05:15,012
the specific example of

162
00:05:15,025 --> 00:05:17,017
character recognition, introducing these

163
00:05:17,048 --> 00:05:18,031
warping seems like a natural

164
00:05:18,077 --> 00:05:19,091
choice, but for a

165
00:05:20,008 --> 00:05:21,097
different learning machine application, there may

166
00:05:22,007 --> 00:05:24,018
be different the distortions that might make more sense.

167
00:05:24,086 --> 00:05:25,060
Let me just show one example

168
00:05:26,018 --> 00:05:28,075
from the totally different domain of speech recognition.

169
00:05:30,023 --> 00:05:31,048
So the speech recognition, let's say

170
00:05:31,057 --> 00:05:33,044
you have audio clips and you

171
00:05:33,060 --> 00:05:35,000
want to learn from the audio

172
00:05:35,035 --> 00:05:37,024
clip to recognize what were

173
00:05:37,045 --> 00:05:38,077
the words spoken in that clip.

174
00:05:39,050 --> 00:05:41,033
So let's see how one labeled training example.

175
00:05:42,029 --> 00:05:43,018
So let's say you have one

176
00:05:43,039 --> 00:05:45,000
labeled training example, of someone

177
00:05:45,032 --> 00:05:46,066
saying a few specific words.

178
00:05:46,086 --> 00:05:48,072
So let's play that audio clip here.

179
00:05:49,014 --> 00:05:51,023
0 -1-2-3-4-5.

180
00:05:51,056 --> 00:05:53,081
Alright, so someone

181
00:05:54,022 --> 00:05:55,011
counting from 0 to 5,

182
00:05:55,044 --> 00:05:57,018
and so you want to

183
00:05:57,029 --> 00:05:58,045
try to apply a learning algorithm

184
00:05:59,037 --> 00:06:01,031
to try to recognize the words said in that.

185
00:06:02,004 --> 00:06:04,002
So, how can we amplify the data set?

186
00:06:04,038 --> 00:06:05,033
Well, one thing we do is

187
00:06:06,001 --> 00:06:09,018
introduce additional audio distortions into the data set.

188
00:06:09,097 --> 00:06:10,095
So here I'm going to

189
00:06:11,063 --> 00:06:14,069
add background sounds to simulate a bad cell phone connection.

190
00:06:15,036 --> 00:06:16,080
When you hear beeping sounds, that's

191
00:06:16,098 --> 00:06:17,070
actually part of the audio

192
00:06:17,074 --> 00:06:20,035
track, that's nothing wrong with the speakers, I'm going to play this now.

193
00:06:20,057 --> 00:06:20,057
0-1-2-3-4-5.

194
00:06:21,037 --> 00:06:22,025
Right, so you can listen

195
00:06:22,063 --> 00:06:24,088
to that sort of audio clip and

196
00:06:25,072 --> 00:06:28,060
recognize the sounds,

197
00:06:28,095 --> 00:06:30,080
that seems like another useful training

198
00:06:31,037 --> 00:06:33,023
example to have, here's another example, noisy background.

199
00:06:34,088 --> 00:06:36,087
Zero, one, two, three

200
00:06:37,056 --> 00:06:39,006
four five you know

201
00:06:39,008 --> 00:06:40,027
of cars driving past, people walking

202
00:06:40,057 --> 00:06:42,019
in the background, here's another

203
00:06:42,044 --> 00:06:43,087
one, so taking the original

204
00:06:44,043 --> 00:06:45,098
clean audio clip so

205
00:06:46,008 --> 00:06:47,081
taking the clean audio of

206
00:06:47,099 --> 00:06:48,095
someone saying 0 1 2 3

207
00:06:49,008 --> 00:06:50,049
4 5 we can then automatically

208
00:06:51,079 --> 00:06:54,008
synthesize these additional training

209
00:06:54,047 --> 00:06:55,085
examples and thus amplify

210
00:06:56,041 --> 00:06:57,086
one training example into maybe four different training examples.

211
00:07:00,011 --> 00:07:00,093
So let me play this final

212
00:07:01,030 --> 00:07:03,018
example, as well.

213
00:07:03,033 --> 00:07:07,018
0-1 3-4-5 So by

214
00:07:07,052 --> 00:07:08,050
taking just one labelled example,

215
00:07:09,000 --> 00:07:10,025
we have to go through the effort

216
00:07:10,036 --> 00:07:11,075
to collect just one labelled example

217
00:07:11,094 --> 00:07:13,026
fall of the 01205, and by

218
00:07:14,013 --> 00:07:16,051
synthesizing additional distortions,

219
00:07:17,029 --> 00:07:18,056
by introducing different background sounds,

220
00:07:19,000 --> 00:07:20,024
we've now multiplied this one

221
00:07:20,037 --> 00:07:21,081
example into many more examples.

222
00:07:23,042 --> 00:07:24,048
Much work by just automatically

223
00:07:25,026 --> 00:07:27,008
adding these different background sounds

224
00:07:27,068 --> 00:07:30,050
to the clean audio Just

225
00:07:30,074 --> 00:07:31,098
one word of warning about synthesizing

226
00:07:33,018 --> 00:07:35,022
data by introducing distortions: if

227
00:07:35,031 --> 00:07:36,062
you try to do this

228
00:07:36,081 --> 00:07:38,057
yourself, the distortions you

229
00:07:39,001 --> 00:07:40,030
introduce should be representative the source

230
00:07:40,066 --> 00:07:42,000
of noises, or distortions, that

231
00:07:42,011 --> 00:07:43,068
you might see in the test set.

232
00:07:44,000 --> 00:07:45,035
So, for the character recognition example,

233
00:07:45,093 --> 00:07:47,023
you know, the working things

234
00:07:47,043 --> 00:07:48,062
begin introduced are actually kind

235
00:07:48,076 --> 00:07:49,098
of reasonable, because an image

236
00:07:50,033 --> 00:07:51,050
A that looks like that, that's,

237
00:07:52,000 --> 00:07:53,001
could be an image that

238
00:07:53,020 --> 00:07:55,017
we could actually see in a test set.Reflect

239
00:07:55,037 --> 00:07:57,018
a fact And, you know, that

240
00:07:57,037 --> 00:08:00,019
image on the upper-right, that

241
00:08:00,035 --> 00:08:01,080
could be an image that we could imagine seeing.

242
00:08:03,027 --> 00:08:04,056
And for audio, well, we do

243
00:08:04,074 --> 00:08:06,056
wanna recognize speech, even against

244
00:08:06,097 --> 00:08:07,099
a bad self internal connection, against

245
00:08:08,048 --> 00:08:09,043
different types of background noise, and

246
00:08:09,058 --> 00:08:10,092
so for the audio, we're again

247
00:08:11,023 --> 00:08:12,080
synthesizing examples are actually

248
00:08:13,052 --> 00:08:14,076
representative of the sorts of

249
00:08:14,085 --> 00:08:15,082
examples that we want to

250
00:08:15,099 --> 00:08:17,036
classify, that we want to recognize correctly.

251
00:08:18,076 --> 00:08:20,066
In contrast, usually it does

252
00:08:20,076 --> 00:08:21,093
not help perhaps you actually

253
00:08:22,017 --> 00:08:23,075
a meaning as noise to your data.

254
00:08:24,042 --> 00:08:25,017
I'm not sure you can see

255
00:08:25,043 --> 00:08:26,039
this, but what we've done

256
00:08:26,062 --> 00:08:28,005
here is taken the image, and

257
00:08:28,020 --> 00:08:29,054
for each pixel, in each

258
00:08:29,072 --> 00:08:30,070
of these 4 images, has just

259
00:08:30,099 --> 00:08:32,097
added some random Gaussian noise to each pixel.

260
00:08:33,024 --> 00:08:34,069
To each pixel, is the

261
00:08:35,005 --> 00:08:36,037
pixel brightness, it would

262
00:08:36,050 --> 00:08:38,087
just add some, you know, maybe Gaussian random noise to each pixel.

263
00:08:39,036 --> 00:08:40,094
So it's just a totally meaningless noise, right?

264
00:08:41,064 --> 00:08:43,027
And so, unless you're expecting

265
00:08:43,079 --> 00:08:45,050
to see these sorts of pixel

266
00:08:45,090 --> 00:08:46,083
wise noise in your test

267
00:08:46,090 --> 00:08:48,019
set, this sort of

268
00:08:48,065 --> 00:08:51,053
purely random meaningless noise is less likely to be useful.

269
00:08:52,087 --> 00:08:53,075
But the process of artificial

270
00:08:54,025 --> 00:08:55,057
data synthesis it is you

271
00:08:55,063 --> 00:08:56,065
know a little bit of

272
00:08:56,071 --> 00:08:57,085
an art as well and sometimes

273
00:08:58,013 --> 00:09:00,025
you just have to try it and see if it works.

274
00:09:01,027 --> 00:09:02,005
But if you're trying to

275
00:09:02,013 --> 00:09:03,016
decide what sorts of distortions

276
00:09:03,087 --> 00:09:04,072
to add, you know, do

277
00:09:04,082 --> 00:09:06,025
think about what other meaningful

278
00:09:06,066 --> 00:09:08,017
distortions you might add that

279
00:09:08,065 --> 00:09:09,072
will cause you to generate additional

280
00:09:10,011 --> 00:09:11,037
training examples that are at

281
00:09:11,087 --> 00:09:13,040
least somewhat representative of the

282
00:09:13,048 --> 00:09:15,083
sorts of images you expect to see in your test sets.

283
00:09:18,010 --> 00:09:19,000
Finally, to wrap up this

284
00:09:19,014 --> 00:09:19,091
video, I just wanna say

285
00:09:20,013 --> 00:09:21,041
a couple of words, more about

286
00:09:21,078 --> 00:09:23,036
this idea of getting loss

287
00:09:23,060 --> 00:09:25,061
of data via artificial data synthesis.

288
00:09:26,091 --> 00:09:28,077
As always, before expending a lot

289
00:09:29,016 --> 00:09:30,027
of effort, you know, figuring out

290
00:09:30,045 --> 00:09:32,001
how to create artificial training

291
00:09:33,005 --> 00:09:34,013
examples, it's often a good

292
00:09:34,022 --> 00:09:35,030
practice is to make sure

293
00:09:35,064 --> 00:09:36,053
that you really have a low biased

294
00:09:36,091 --> 00:09:38,035
crossfire, and having a

295
00:09:38,046 --> 00:09:40,032
lot more training data will be of help.

296
00:09:41,000 --> 00:09:41,084
And standard way to do

297
00:09:41,097 --> 00:09:42,080
this is to plot the learning

298
00:09:43,002 --> 00:09:43,097
curves, and make sure that

299
00:09:44,012 --> 00:09:44,091
you only have a low

300
00:09:45,000 --> 00:09:47,047
as well, high variance falsifier.

301
00:09:47,075 --> 00:09:48,064
Or if you don't have a low

302
00:09:48,072 --> 00:09:50,009
bias falsifier, you know,

303
00:09:50,015 --> 00:09:51,003
one other thing that's worth trying

304
00:09:51,045 --> 00:09:53,026
is to keep increasing the number

305
00:09:53,053 --> 00:09:54,044
of features that your classifier

306
00:09:54,060 --> 00:09:55,064
has, increasing the number of

307
00:09:55,074 --> 00:09:56,071
hidden units in your network,

308
00:09:57,017 --> 00:09:58,047
saying, until you actually have a

309
00:09:58,053 --> 00:10:00,000
low bias falsifier, and only

310
00:10:00,030 --> 00:10:01,082
then, should you put

311
00:10:02,003 --> 00:10:04,001
the effort into creating a

312
00:10:04,025 --> 00:10:05,075
large, artificial training set, so

313
00:10:05,086 --> 00:10:06,065
what you really want to avoid

314
00:10:06,087 --> 00:10:07,092
is to, you know, spend

315
00:10:08,011 --> 00:10:08,088
a whole week or spend a few

316
00:10:09,009 --> 00:10:10,037
months figuring out how

317
00:10:10,053 --> 00:10:11,072
to get a great artificially

318
00:10:12,045 --> 00:10:13,025
synthesized data set.

319
00:10:13,082 --> 00:10:15,051
Only to realize afterward, that,

320
00:10:15,075 --> 00:10:17,040
you know, your learning algorithm, performance

321
00:10:18,002 --> 00:10:20,073
doesn't improve that much, even when you're given a huge training set.

322
00:10:22,019 --> 00:10:23,005
So that's about my usual advice

323
00:10:23,041 --> 00:10:24,069
about of a testing that

324
00:10:25,002 --> 00:10:26,028
you really can make use

325
00:10:26,052 --> 00:10:27,075
of a large training set before

326
00:10:28,008 --> 00:10:30,052
spending a lot of effort going out to get that large training set.

327
00:10:31,096 --> 00:10:33,027
Second is, when i'm working

328
00:10:33,059 --> 00:10:35,025
on machine learning problems, one question

329
00:10:35,069 --> 00:10:37,051
I often ask the team

330
00:10:37,087 --> 00:10:39,021
I'm working with, often ask my

331
00:10:39,042 --> 00:10:40,054
students, which is, how much work

332
00:10:40,062 --> 00:10:42,080
would it be to get 10 times as much date as we currently had.

333
00:10:46,072 --> 00:10:47,085
When I face a new machine

334
00:10:48,020 --> 00:10:49,075
learning application very often I

335
00:10:49,098 --> 00:10:50,094
will sit down with a team

336
00:10:51,021 --> 00:10:52,044
and ask exactly this question,

337
00:10:52,091 --> 00:10:53,087
I've asked this question over and

338
00:10:53,097 --> 00:10:55,087
over and over and I've

339
00:10:56,000 --> 00:10:57,053
been very surprised how often

340
00:10:58,038 --> 00:10:59,065
this answer has been that.

341
00:11:00,000 --> 00:11:01,007
You know, it's really not that hard,

342
00:11:01,067 --> 00:11:02,066
maybe a few days of work

343
00:11:02,092 --> 00:11:03,092
at most, to get ten times

344
00:11:04,025 --> 00:11:05,029
as much data as we currently

345
00:11:05,045 --> 00:11:06,064
have for a machine

346
00:11:06,080 --> 00:11:08,082
running application and very

347
00:11:09,008 --> 00:11:09,083
often if you can get

348
00:11:09,095 --> 00:11:11,002
ten times as much data there

349
00:11:11,026 --> 00:11:13,067
will be a way to make your algorithm do much better.

350
00:11:14,005 --> 00:11:15,003
So, you know, if you

351
00:11:15,025 --> 00:11:16,050
ever join the product team

352
00:11:17,082 --> 00:11:18,087
working on some machine learning

353
00:11:19,011 --> 00:11:20,042
application product this is

354
00:11:20,054 --> 00:11:21,071
a very good questions ask yourself

355
00:11:22,028 --> 00:11:23,050
ask the team don't be

356
00:11:23,064 --> 00:11:25,012
too surprised if after a

357
00:11:25,024 --> 00:11:26,052
few minutes of brainstorming if your

358
00:11:26,064 --> 00:11:27,051
team comes up with a

359
00:11:27,065 --> 00:11:28,095
way to get literally ten

360
00:11:29,020 --> 00:11:30,025
times this much data, in

361
00:11:30,037 --> 00:11:31,032
which case, I think you would

362
00:11:31,042 --> 00:11:32,033
be a hero to that team,

363
00:11:32,094 --> 00:11:34,000
because with 10 times as

364
00:11:34,024 --> 00:11:35,036
much data, I think you'll really

365
00:11:35,045 --> 00:11:38,046
get much better performance, just from learning from so much data.

366
00:11:39,064 --> 00:11:44,050
So there are several waysand

367
00:11:47,045 --> 00:11:48,050
that comprised both the ideas

368
00:11:48,097 --> 00:11:50,044
of generating data from

369
00:11:50,063 --> 00:11:53,004
scratch using random fonts and so on.

370
00:11:53,057 --> 00:11:54,042
As well as the second idea

371
00:11:54,084 --> 00:11:56,060
of taking an existing example and

372
00:11:56,066 --> 00:11:58,010
and introducing distortions that amplify

373
00:11:58,027 --> 00:12:00,090
to enlarge the training set A

374
00:12:01,009 --> 00:12:02,014
couple of other examples of

375
00:12:02,027 --> 00:12:03,012
ways to get a lot more

376
00:12:03,026 --> 00:12:04,061
data are to collect the

377
00:12:04,066 --> 00:12:06,060
data or to label them yourself.

378
00:12:07,060 --> 00:12:09,009
So one useful calculation that

379
00:12:09,021 --> 00:12:11,058
I often do is, you know,

380
00:12:11,077 --> 00:12:13,032
how many minutes, how many

381
00:12:13,051 --> 00:12:15,013
hours does it take to

382
00:12:15,035 --> 00:12:16,041
get a certain number of

383
00:12:16,061 --> 00:12:17,077
examples, so actually sit down and

384
00:12:17,089 --> 00:12:19,040
figure out, you know, suppose it

385
00:12:19,054 --> 00:12:21,083
takes me ten seconds to

386
00:12:22,005 --> 00:12:23,099
label one example then

387
00:12:24,012 --> 00:12:25,082
and, suppose that, for

388
00:12:26,019 --> 00:12:29,004
our application, currently we

389
00:12:29,019 --> 00:12:31,050
have 1000 labeled examples examples

390
00:12:31,062 --> 00:12:32,073
so ten times as

391
00:12:32,086 --> 00:12:34,009
much of that would be

392
00:12:34,020 --> 00:12:35,094
if n were equal to ten thousand.

393
00:12:37,044 --> 00:12:40,025
A second way to

394
00:12:40,039 --> 00:12:41,052
get a lot of data is

395
00:12:41,079 --> 00:12:43,053
to just collect the data and you label it yourself.

396
00:12:44,050 --> 00:12:45,037
So what I mean by this is

397
00:12:45,069 --> 00:12:46,097
I will often set down and

398
00:12:47,024 --> 00:12:48,057
do a calculation to figure

399
00:12:48,095 --> 00:12:50,019
out how much time, you

400
00:12:50,035 --> 00:12:51,013
know just like how many hours

401
00:12:52,063 --> 00:12:54,000
will it take, how many

402
00:12:54,020 --> 00:12:55,012
hours or how many days will

403
00:12:55,023 --> 00:12:56,088
it take for me or

404
00:12:57,001 --> 00:12:58,039
for someone else to just sit

405
00:12:58,063 --> 00:12:59,087
down and collect ten times

406
00:13:00,019 --> 00:13:01,049
as much data, as we have

407
00:13:01,079 --> 00:13:03,055
currently, by collecting the data ourselves and labeling them ourselves.

408
00:13:05,025 --> 00:13:06,054
So, for example, that, for

409
00:13:06,062 --> 00:13:08,020
our machine learning application, currently

410
00:13:08,069 --> 00:13:10,017
we have 1,000 examples, so M 1,000.

411
00:13:12,000 --> 00:13:12,075
That what we do is sit

412
00:13:12,087 --> 00:13:14,050
down and ask, how long does

413
00:13:14,072 --> 00:13:16,092
it take me really to collect and label one example.

414
00:13:17,034 --> 00:13:18,048
And sometimes maybe it will

415
00:13:18,060 --> 00:13:19,050
take you, you know ten

416
00:13:19,078 --> 00:13:22,010
seconds to label

417
00:13:23,030 --> 00:13:25,012
one new example, and so

418
00:13:25,051 --> 00:13:27,072
if I want 10 X as many examples, I'd do a calculation.

419
00:13:28,036 --> 00:13:30,039
If it takes me 10 seconds to get one training example.

420
00:13:31,037 --> 00:13:32,034
If I wanted to get 10

421
00:13:32,058 --> 00:13:35,032
times as much data, then I need 10,000 examples.

422
00:13:35,083 --> 00:13:38,047
So I do the calculation, how long

423
00:13:38,076 --> 00:13:40,037
is it gonna take to label,

424
00:13:40,084 --> 00:13:42,063
to manually label 10,000 examples,

425
00:13:43,034 --> 00:13:45,027
if it takes me 10 seconds to label 1 example.

426
00:13:47,007 --> 00:13:47,094
So when you do this calculation,

427
00:13:48,084 --> 00:13:49,091
often I've seen many you

428
00:13:50,038 --> 00:13:51,077
would be surprised, you know,

429
00:13:51,087 --> 00:13:53,013
how little, or sometimes a

430
00:13:53,024 --> 00:13:54,073
few days at work, sometimes a

431
00:13:54,087 --> 00:13:55,055
small number of days of work,

432
00:13:55,077 --> 00:13:57,017
well I've seen many teams be very

433
00:13:57,050 --> 00:13:59,015
surprised that sometimes how

434
00:13:59,034 --> 00:14:00,027
little work it could be,

435
00:14:00,040 --> 00:14:01,020
to just get a lot more

436
00:14:01,037 --> 00:14:02,050
data, and let that be

437
00:14:02,058 --> 00:14:03,047
a way to give your learning

438
00:14:03,058 --> 00:14:04,030
app to give you a huge boost

439
00:14:04,063 --> 00:14:06,035
in performance, and necessarily, you

440
00:14:06,045 --> 00:14:07,054
know, sometimes when you've just

441
00:14:07,078 --> 00:14:08,089
managed to do this, you

442
00:14:09,019 --> 00:14:10,077
will be a hero and whatever product

443
00:14:11,036 --> 00:14:12,051
development, whatever team you're working

444
00:14:12,090 --> 00:14:14,014
on, because this can

445
00:14:14,032 --> 00:14:15,075
be a great way to get much better performance.

446
00:14:17,064 --> 00:14:19,049
Third and finally, one sometimes

447
00:14:20,001 --> 00:14:21,023
good way to get a

448
00:14:21,045 --> 00:14:22,064
lot of data is to use

449
00:14:23,008 --> 00:14:24,035
what's now called crowd sourcing.

450
00:14:25,027 --> 00:14:26,035
So today, there are a

451
00:14:26,051 --> 00:14:27,026
few websites or a few

452
00:14:27,046 --> 00:14:29,051
services that allow you

453
00:14:29,091 --> 00:14:32,021
to hire people on

454
00:14:32,035 --> 00:14:33,040
the web to, you know, fairly

455
00:14:33,073 --> 00:14:36,013
inexpensively label large training sets for you.

456
00:14:36,080 --> 00:14:37,087
So this idea of crowd

457
00:14:38,019 --> 00:14:39,046
sourcing, or crowd sourced

458
00:14:39,095 --> 00:14:41,038
data labeling, is something

459
00:14:41,080 --> 00:14:43,017
that has, is obviously, like

460
00:14:43,034 --> 00:14:45,020
an entire academic literature,

461
00:14:45,065 --> 00:14:47,003
has some of it's own complications and

462
00:14:47,021 --> 00:14:49,038
so on, pertaining to labeler reliability.

463
00:14:50,044 --> 00:14:51,047
Maybe, you know, hundreds of thousands

464
00:14:51,086 --> 00:14:53,041
of labelers, around the

465
00:14:53,058 --> 00:14:55,052
world, working fairly inexpensively to

466
00:14:55,062 --> 00:14:56,080
help label data for you,

467
00:14:57,002 --> 00:14:58,058
and that I've just had mentioned,

468
00:14:58,092 --> 00:15:00,012
there's this one alternative as well.

469
00:15:00,038 --> 00:15:02,016
And probably Amazon Mechanical Turk

470
00:15:02,050 --> 00:15:03,075
systems is probably the most

471
00:15:03,089 --> 00:15:05,086
popular crowd sourcing option right now.

472
00:15:06,086 --> 00:15:08,007
This is often quite a

473
00:15:08,022 --> 00:15:10,003
bit of work to

474
00:15:10,019 --> 00:15:10,094
get to work, if you want

475
00:15:11,014 --> 00:15:12,051
to get very high quality labels,

476
00:15:12,077 --> 00:15:14,015
but is sometimes an

477
00:15:14,024 --> 00:15:15,075
option worth considering as well.

478
00:15:17,033 --> 00:15:18,087
If you want to try to

479
00:15:19,032 --> 00:15:21,000
hire many people, fairly inexpensively

480
00:15:21,080 --> 00:15:24,022
on the web, our labels launch miles of data for you.

481
00:15:26,032 --> 00:15:27,057
So this video, we

482
00:15:27,065 --> 00:15:28,084
talked about the idea of

483
00:15:29,010 --> 00:15:30,087
artificial data synthesis of

484
00:15:31,012 --> 00:15:32,044
either creating new data

485
00:15:32,075 --> 00:15:34,039
from scratch, looking, using

486
00:15:34,063 --> 00:15:35,039
the ramming funds as an example,

487
00:15:35,083 --> 00:15:37,071
or by amplifying an

488
00:15:37,078 --> 00:15:38,098
existing training set, by taking

489
00:15:39,041 --> 00:15:41,034
existing label examples and

490
00:15:41,055 --> 00:15:42,098
introducing distortions to it,

491
00:15:43,024 --> 00:15:44,087
to sort of create extra label examples.

492
00:15:46,000 --> 00:15:47,045
And finally, one thing that

493
00:15:47,062 --> 00:15:48,080
I hope you remember from this

494
00:15:49,012 --> 00:15:49,097
video this idea of if

495
00:15:50,053 --> 00:15:51,053
you are facing a machine learning

496
00:15:51,083 --> 00:15:54,035
problem, it is often worth doing two things.

497
00:15:54,065 --> 00:15:55,083
One just a sanity check,

498
00:15:56,015 --> 00:15:58,060
with learning curves, that having more data would help.

499
00:15:59,051 --> 00:16:00,034
And second, assuming that that's the case,

500
00:16:00,073 --> 00:16:01,077
I will often seat down and

501
00:16:01,085 --> 00:16:03,066
ask yourself seriously: what would

502
00:16:04,004 --> 00:16:05,014
it take to get ten times as

503
00:16:05,025 --> 00:16:06,050
much creative data as you

504
00:16:06,062 --> 00:16:08,045
currently have, and not always,

505
00:16:08,096 --> 00:16:10,044
but sometimes, you may be

506
00:16:10,063 --> 00:16:12,030
surprised by how easy that

507
00:16:12,058 --> 00:16:13,099
turns out to be, maybe

508
00:16:14,005 --> 00:16:15,001
a few days, a few weeks at

509
00:16:15,014 --> 00:16:16,015
work, and that can be

510
00:16:16,025 --> 00:16:18,070
a great way to give your learning algorithm a huge boost in performance
